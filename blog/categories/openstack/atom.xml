<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Openstack | RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/blog/categories/openstack/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2015-12-24T20:46:38+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[深度解读OpenStack Liberty国内代码贡献]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/"/>
    <updated>2015-10-29T18:56:06+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty</id>
    <content type="html"><![CDATA[<p>又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。</p>

<p>OpenStack Liberty完整版本的翻译可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans">https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans</a></p>

<p>本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。</p>

<!-- more -->


<h2>社区贡献分析</h2>

<p>本次统计，并没有采用Review的数量为依据，而直接采用commits的方式，也就是代码实际merge入库的数量。</p>

<p>我们仍然要先看一下模块的贡献情况：</p>

<p><img class="left" src="/images/blogs/contribution-in-liberty-contribution-by-modules.png" width="400"></p>

<p>与之前Release的特点相似，OpenStack早期的核心模块Nova, Keystone代码commits数量出现明显下滑状态，而Neutron, Heat, Trove, Ceilometer, Cinder等模块都保持着稳中有升的态势。值得关注的是，在排名前20名的项目中，出现了两个直接与Docker有关的项目Kolla和Magnum，一个与docker间接有关的项目Murano。可以预见，OpenStack下一步发展的热点就是在与Docker之间的勾勾搭搭。</p>

<p>特别需要注意的是，在stackalytics.com统计的模块中，在Kilo中是259个，而到了Liberty到了389个，当然有一些项目并非完全是OpenStack的项目，但是也从一个侧面反映出OpenStack以及周边项目的蓬勃发展。</p>

<p>从更新日志中我们也能看到，本次Release的正式项目中，变动较大的是Neutron和Heat两个模块。在经历不断锤炼后，Neutron逐渐走向成熟，但是从生产级别角度看，Neutron的确还有很长的路要走。</p>

<h2>国内社区贡献分析</h2>

<p><img class="center" src="/images/blogs/contribution-in-liberty-contributor.png" width="400"></p>

<p>从全球企业的贡献排名来看，排名状况基本变化不大，仍然是HP, Redhat, Mirantis, IBM, Rackspace, Intel, Cisco，但是非常欣喜的，国内的IT的航空母舰华为已经成功杀入前十名，这无疑是振奋人心的事情，希望华为未来能多一些对OpenStack社区的主导力，提高中国在OpenStack社区的地位，当然最好也能扶植一下国内的OpenStack创业公司，实现共同发展、共同进步。华为的主要代码贡献集中在dragonflow，magnum，heat等模块，特别是在dragonflow上，几乎全部是华为贡献的，magnum上也将近有五分之一的代码。</p>

<p><strong><em>华为社区贡献统计</em></strong></p>

<p><img class="center" src="/images/blogs/contribution-in-liberty-huawei.png" width="800"></p>

<p>记得在OpenStack五周年的庆祝活动上，Intel的陈绪博士说过，国内OpenStack贡献企业，就是一朵大云，四朵小云，下面让我们来看看这几朵小云在这个版本的表现。</p>

<p><strong><em> 99cloud社区贡献统计</em></strong></p>

<p><img class="center" src="/images/blogs/contribution-in-liberty-99cloud.png" width="800"></p>

<p>排名第16位的是99cloud，99cloud自上一个版本排名四朵小云之首后，本次继续强劲来袭，排名创造历史新高，第16名。通过对贡献模块的分析，我们能看出99cloud最大的贡献来自于社区文档，而在项目方面的贡献则主要来自murano-dashboard，horizon，neutron等项目上，从中可以看出99cloud对murano这个applicaton catalog的项目关注程度比较高，可能会在将来的产品中有所体现。从贡献中，我们隐约看到了九州云的副总裁李开总的提交，由此可见九州云为社区贡献的积极程度。
更加难能可贵的是，Horizon的全球贡献99cloud是全球前十，Tempest全球前八，Murano项目更是进入全球前三，相当给力。</p>

<p><strong><em> UnitedStack社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-unitedstack.png" width="800"></p>

<p>排在第30位的是UnitedStack，经过了上一个版本的短暂沉寂后，这个版本卷土重来，杀回前30。从代码贡献来看，UnitedStack的主要贡献来自python-openstackclient以及部署用到的puppet相关代码，当然对neutron、trove、kolla、heat等也有一定数量的贡献。</p>

<p><strong><em> Kylin Cloud社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-kylincloud.png" width="800"></p>

<p>排名第38位的是麒麟云，其实麒麟云每次Release中总是有她的身影，但好像总是被忽略的。麒麟云最大的贡献来自Horizon项目，其他模块也有一定数量的贡献。总之，我们想到OpenStack企业的时候，的确应该时常提起麒麟云。</p>

<p><strong><em> Easystack社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-easystack.png" width="800"></p>

<p>排名第70位的是Easystack，Easystack也属于OpenStack早期创业的公司，对于OpenStack的贡献也是持续的。Easystack最大的贡献来自nova，虽然数量不是很多，但是在国内企业里应该算名列前茅的啦。Easystack对Nova的贡献主要来自对libvirt层的bug修复。</p>

<p><strong><em> Awcloud社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-awcloud.png" width="800"></p>

<p>排名第75位的是海云捷迅，海云应该算是在国内发展比较迅猛的一家OpenStack早期创业公司。他们的贡献主要来自Neutron相关的项目，看起来应该是为了解决项目中出现的实际问题所做的努力。海云的马力应该是公司内部贡献排名第一的，尤其是前一段时间发布的两篇关于&#8221;Neutron &amp; OpenStack漫谈&#8221;，非常值得一读。</p>

<p><strong><em> LeTV社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-letv.png" width="800"></p>

<p><strong><em> Netease社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-netease.png" width="800"></p>

<p>排名第94和95位的分别是两家互联网企业，乐视和网易，乐视是最近互联网中使用OpenStack动静最大的一家了，应该能在大规模应用中发现OpenStack很多问题吧。</p>

<p><strong><em> Huron社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-huron.png" width="800"></p>

<p>排名第122位的是我的公司——北京休伦科技有限公司，其实我们公司也算是国内最早一批从事OpenStack创业的公司，z早在2013年的时候就已经开始投入OpenStack私有云产品相关的研发。我们贡献的代码主要来自Nova和Murano两个模块中，都是我们在开发和项目使用中发现的问题，修复后回馈给社区的，我也希望我们能在下一个版本Release中贡献更多的力量。</p>

<p><strong><em> China Mobile社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-chinamobile.png" width="800"></p>

<p>排名第133位的是中国移动，之前并没有在哪一个排名上看到过中国移动在OpenStack贡献，我也是第一次发现。中国移动应该算是国内运营商领域技术实力较强的一家，也是运营商里开始从事OpenStack预研较早的一家。中国移动有大量的IT资源和设备，理应像AT&amp;T一样在OpenStack领域大有所为。纵观中国移动的社区贡献，主要来自Neutron和Ceilometer两个项目，几个Bug修复都是与Volume相关。</p>

<p><strong><em> Lenovo社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-lenovo.png" width="800"></p>

<p>排名第135位的是联想。不评论了。</p>

<p>排名第139位的是清华大学医学院附属医院，这个有点意思。但是stackalytics.com有Bug，他们的具体统计显示不出来。</p>

<p><strong><em> H3C社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-h3c.png" width="800"></p>

<p>排名第143位的是H3C。贡献是Nova中的关于VMware的Bug Fix。</p>

<p>由于stackalytics并没有按照区域统计的功能，所以本次统计完全是全自动统计(全靠我自己手动)，所以难免遗漏了为OpenStack贡献的国内企业，如果发生该情况请及时告知。</p>

<h2>社区贡献内容分析</h2>

<p><img class="center" src="/images/blogs/contribution-in-liberty-complete-blueprints.png" width="800"></p>

<p>从贡献的commits的类型来区分，国内贡献出的代码主要还是以bug为主，这可能也与我们使用的都是OpenStack较成熟的模块有关，本身这些模块成熟程度较高，所以想做blueprint很难。另外一个很重要的原因是和OpenStack管理流程有关的，现在像Nova, Cinder等项目都是需要先Review Specs的，其实就是所谓的设计文档，语言成为国内很多工程师贡献的最大障碍，所以这也导致了Blueprint的贡献度在国内并不高。</p>

<p><strong><em> Huawei社区贡献——完成Blueprint </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-blueprint-huawei.png" width="800"></p>

<p>纵观整个Blueprint的完成统计情况，华为作为国内最有实力的企业，高居全球第五名，完成最多的模块为cinder和mistral。</p>

<p>之后能完成Blueprint的企业还包括UnitedStack、中国移动、麒麟云、海云捷迅和九州云，但是相比来说数量较少，都是个位数字。</p>

<p>OpenStack在国内发展已经超过了四年的时间，但是遗憾的一点，尽管我们拥有世界上最多的开发人员，但是我们对社区仍然没有话语权，国内的用户的需求无法对社区上游形成影响，导致很多本地化定制的需求无法真正的在社区版本代码得到体现。所以如何让中国的声音出现在社区，是我们所有OpenStack人需要思考的问题。欣喜的一点，本土的巨头华为已经身先士卒，投入很大的力量搞OpenStack的社区贡献，我们更希望越来越多的国内传统IT巨头能够意识到这个问题，投身于开源的事业中，否则我们又在起跑线上输给了别人。</p>

<p>以上仅代表个人观点，如有任何异议，欢迎批评指正。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu 14.04 Server开发者安装指南]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/09/09/ubuntu-14-dot-04-installation-guide-for-developer/"/>
    <updated>2015-09-09T13:50:24+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/09/09/ubuntu-14-dot-04-installation-guide-for-developer</id>
    <content type="html"><![CDATA[<h2>为什么会写这篇Blog</h2>

<p>近期，接触了一些OpenStack的入门者，很多人对Linux系统并不是很熟悉，导致安装出来的系统五花八门，间接地影响了后面的开发与调试，所以这里给出我的安装流程，供初学者们参考。我使用的是Ubuntu 14.04 64bit Server版本的ISO进行安装，其他版本方法类似。</p>

<!-- more -->


<h2>注意</h2>

<p>这篇Blog没有提及的地方：</p>

<ul>
<li>网络，需要根据实际情况进行配置，我这里面使用的是DHCP自动获取，所以没有相关步骤</li>
<li>分区，这里面使用的是默认配置，但是生产环境的配置一般需要手动划分</li>
</ul>


<h2>安装步骤</h2>

<ul>
<li>一定要选择English，否则处理中文的时候太麻烦
<img class="center" src="/images/blogs/install-ubuntu/1.png"></li>
<li>正式开始进入安装
<img class="center" src="/images/blogs/install-ubuntu/2.png"></li>
<li>与上面的原则一致，一定要选择English
<img class="center" src="/images/blogs/install-ubuntu/3.png"></li>
<li>Location一定要选择中国，否则默认不会使用中文的Ubuntu源，影响安装速度，这一步很多初学者不会在意
<img class="center" src="/images/blogs/install-ubuntu/4.png">
<img class="center" src="/images/blogs/install-ubuntu/5.png">
<img class="center" src="/images/blogs/install-ubuntu/6.png"></li>
<li>这里面主要是字符集的问题，选择United States
<img class="center" src="/images/blogs/install-ubuntu/7.png"></li>
<li>不需要检查键盘布局
<img class="center" src="/images/blogs/install-ubuntu/8.png"></li>
<li>默认使用English布局就好了
<img class="center" src="/images/blogs/install-ubuntu/9.png"></li>
<li>主机名设置，就是hostname
<img class="center" src="/images/blogs/install-ubuntu/10.png"></li>
<li>用户设置，建议建立一个普通用户
<img class="center" src="/images/blogs/install-ubuntu/11.png">
<img class="center" src="/images/blogs/install-ubuntu/12.png">
<img class="center" src="/images/blogs/install-ubuntu/13.png">
<img class="center" src="/images/blogs/install-ubuntu/15.png">
<img class="center" src="/images/blogs/install-ubuntu/16.png"></li>
<li>不加密Home目录
<img class="center" src="/images/blogs/install-ubuntu/17.png"></li>
<li>设置时区，这一步也很重要，默认情况下会自动检测到，但是如果不对，一定要修改一下，否则你的系统时间与你实际不一致，你程序里的时间跟着不对，跟调试增加难度
<img class="center" src="/images/blogs/install-ubuntu/18.png"></li>
<li>这里面分区用默认的就好啦，当然如果你知道该如何分区，可以采用Manual方式
<img class="center" src="/images/blogs/install-ubuntu/19.png">
<img class="center" src="/images/blogs/install-ubuntu/20.png">
<img class="center" src="/images/blogs/install-ubuntu/21.png">
<img class="center" src="/images/blogs/install-ubuntu/22.png">
<img class="center" src="/images/blogs/install-ubuntu/23.png"></li>
<li>如果访问网络需要使用代理，可以设置一下
<img class="center" src="/images/blogs/install-ubuntu/24.png"></li>
<li>不选择自动更新
<img class="center" src="/images/blogs/install-ubuntu/25.png"></li>
<li>默认只需要选择SSH服务，保证我们在安装后能够SSH登陆服务器即可
<img class="center" src="/images/blogs/install-ubuntu/26.png"></li>
<li>安装grub
<img class="center" src="/images/blogs/install-ubuntu/27.png"></li>
<li>重启完成安装
<img class="center" src="/images/blogs/install-ubuntu/28.png"></li>
</ul>


<h2>后记</h2>

<p>谨记此篇Blog送给我的小徒弟周小球小朋友，希望你能利用利用最后的一年的时间努力学习，找到称心如意的工作。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Kilo)Devstack完全用户手册]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/09/03/devstack-guide/"/>
    <updated>2015-09-03T18:34:20+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/09/03/devstack-guide</id>
    <content type="html"><![CDATA[<p>Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。</p>

<p>本篇Blog主要介绍以下几个实用场景：</p>

<ul>
<li>如何利用Devstack构建一套完美的开发环境</li>
<li>提高Devstack安装成功率的方法</li>
<li>Devstack的实用技巧</li>
<li>各种场景下的配置和注意事项</li>
</ul>


<p>本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。</p>

<!-- more -->


<h2>运行环境的选择</h2>

<p>对于刚刚接触OpenStack的开发者而言，没有太多闲置的资源，所以比较容易的上手方式就是使用虚拟机。对于桌面的虚拟机软件来说，主流的软件无外乎VMWare Workstation和Oracle Virtualbox，对于OpenStack开发而言，二者并无太大差异。以下几点可能会作为选择的主要依据：</p>

<ul>
<li>VMWare Workstation是收费软件，Virtualbox是免费软件</li>
<li>VMWare Workstation支持nested virtualization，就是安装完的devstack virt type是kvm，节省资源，Virtualbox安装以后只能使用qemu，虽然在Virtualbox 5以上版本号称支持，但是实际验证中仍然不能生效，还在研究中</li>
<li>VMWare Workstation使用NAT方式时，内部的IP可以在HOST主机直接访问到，Virtualbox还需要端口转发，所以建议单独增加一块Host-only的Apdaptor便于调试</li>
<li>使用Virtualbox时，为了让虚拟机能够访问外部网络，并且允许Host通过Floating IP对虚拟机进行访问，需要在Host层面设置NAT规则，转换到可以访问的物理网卡上，详情请见下文</li>
</ul>


<h2>Virtualbox网络设置</h2>

<p><img class="center" src="/images/blogs/devstack-guide-network-topology.jpg"></p>

<ul>
<li>Nova Network网卡配置</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet static
address 192.168.56.101
netmask 255.255.255.0

auto eth2
iface eth1 inet static
address 172.16.0.101
netmask 255.255.255.0
</code></pre>

<ul>
<li>Neutron网卡配置</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet static
address 192.168.56.101
netmask 255.255.255.0

auto eth2
iface eth2 inet manual
up ip link set dev $IFACE up
down ip link set dev $IFACE down
</code></pre>

<ul>
<li>MAC网卡NAT映射</li>
</ul>


<p>我们将第三块网卡作为提供外部网络的接口，采用系统层面的NAT方式让该网卡能够访问外部网络。</p>

<pre><code class="plain bash">sudo sysctl net.inet.ip.forwarding=1
</code></pre>

<p>在nat-anchor后面添加</p>

<pre><code class="plain /etc/pf.conf">nat on en0 from 172.16.0.0/24 -&gt; (en0)
</code></pre>

<p>之后加载</p>

<pre><code class="bash bash">sudo pfctl -e -f /etc/pf.conf
</code></pre>

<ul>
<li>Linux网卡NAT映射</li>
</ul>


<pre><code class="plain bash">echo 1 &gt; /proc/sys/net/ipv4/ip_forward
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
</code></pre>

<h2>Devstack快速开始</h2>

<p>其实，Devstack本身并不需要很复杂的配置就可以成功运行，但是仍然有几个需要注意的地方：</p>

<ul>
<li>Ubuntu 14.04 64bit(LTS), 12.04已经逐渐退出历史舞台，所以这里推荐14.04</li>
<li>不能使用root用户，即使你使用root用户执行Devstack，默认也会为你建立一个stack用户，所以不如老老实实的直接使用普通用户运行Devstack，或者提前建立好stack用户，切换后再执行</li>
<li>默认获取Devstack进行安装，安装的是master分支的代码，但是在实际开发中(比如我们做产品的时候)，都是基于某个stable分支进行，所以一般情况在clone devstack的时候需要指定stable分支</li>
</ul>


<p>下面给出一个最简安装步骤：</p>

<pre><code class="plain"># adduser stack
# apt-get install sudo -y
# echo "stack ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoers
# sudo su - stack

(stack)$ git clone https://git.openstack.org/openstack-dev/devstack --branch=stable/kilo
(stack)$ cd devstack &amp;&amp; ./stack.sh
</code></pre>

<h2>提高Devstack安装成功率</h2>

<p>估计在国内使用Devstack的人基本都遇到过安装失败的状况，为了节约大家的时间，先分析一下Devstack为什么会失败，我们先从这张时序图看一下Devstack执行的过程：</p>

<p><img class="center" src="/images/blogs/devstack-guide-flow.png"></p>

<p>从上述流程图中可以很清楚的看到Devstack有以下几个地方需要访问网络：</p>

<ul>
<li>安装依赖时，需要访问Ubuntu的源</li>
<li>执行get_pip.sh时，地址是彻底被墙的，需要访问<a href="https://bootstrap.pypa.io/get-pip.py">https://bootstrap.pypa.io/get-pip.py</a></li>
<li>从github clone源代码，github在国内访问速度并不很快而且间歇性被墙</li>
<li>安装过程中执行pip install requirements，需要访问pip repo</li>
<li>下载镜像，这一步骤取决于你需要安装的模块，如果默认安装只会下载cirros镜像，但是如果是安装类似Trove的模块，可能需要下载的更多</li>
</ul>


<hr />

<p>所以综上所述，为了提高devstack的安装成功率，需要从这几个方面着手优化：</p>

<ul>
<li>使用国内源</li>
</ul>


<pre><code class="plain /etc/apt/sources.list">deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse
</code></pre>

<ul>
<li>从国内源获取get-pip.py，从源代码可以分析出，检测get-pip.py的方式，这里面有两种方式一种是手动下载get-pip.py之后，注释代码，还有一种就是修改PIP_GET_PIP_URL的地址，但是这里只能通过修改install_pip.sh的方式，暂时无法从环境变量里获取</li>
</ul>


<pre><code class="bash devstack/tools/install_pip.sh">FILES=$TOP_DIR/files

PIP_GET_PIP_URL=https://bootstrap.pypa.io/get-pip.py
LOCAL_PIP="$FILES/$(basename $PIP_GET_PIP_URL)"

function install_get_pip {
    # The OpenStack gate and others put a cached version of get-pip.py
    # for this to find, explicitly to avoid download issues.
    #
    # However, if DevStack *did* download the file, we want to check
    # for updates; people can leave their stacks around for a long
    # time and in the mean-time pip might get upgraded.
    #
    # Thus we use curl's "-z" feature to always check the modified
    # since and only download if a new version is out -- but only if
    # it seems we downloaded the file originally.
    if [[ ! -r $LOCAL_PIP || -r $LOCAL_PIP.downloaded ]]; then
        curl --retry 6 --retry-delay 5 \
            -z $LOCAL_PIP -o $LOCAL_PIP $PIP_GET_PIP_URL || \
            die $LINENO "Download of get-pip.py failed"
        touch $LOCAL_PIP.downloaded
    fi
    sudo -H -E python $LOCAL_PIP
}
</code></pre>

<p>修改为我在coding.net上缓存的get-pip脚本</p>

<pre><code class="bash devstack/tools/install_pip.sh">PIP_GET_PIP_URL=https://coding.net/u/xiaoquqi/p/pip/git/raw/master/contrib/get-pip.py
</code></pre>

<ul>
<li>国内的代码托管服务器有从github上定期同步源代码的，但是经过实际测试都不是很理想，所以可能这是最不稳定的一部分，但是可以提前使用脚本，人工的下载所有代码，之后我会尝试在我自己的源中定时同步OpenStack源代码，敬请关注</li>
<li>现在pip的安装速度明显提升，原来还需要使用国内源，例如豆瓣，现在即使不修改也能很快的进行安装</li>
<li>镜像下载建议使用一些下载工具，然后放到指定的目录中，这样最有效</li>
</ul>


<h2>无网络状况下安装Devstack</h2>

<p>因为我们是做OpenStack的产品的公司，所以就要求我们的Devstack要能够满足无网络状况下的安装，之前也写过一篇详细介绍无网络安装Devstack博客,由于时间关系，可能一些内容已经过时了，这里面再进行一下更新，思路还是上面的思路，这里给出一些使用的工具，如果不清楚如何使用的话，可以参考我之前的博客。</p>

<ul>
<li>本地源的缓存使用apt-mirror，这是一个需要时间的工作，第一次同步的时间会非常长，准备好大约100G左右的空间吧</li>
<li>缓存get-pip.py，这个比较容易，搭建一个Apache服务器，但是需要把端口修改为10000，否则在安装好OpenStack后，会占用80端口，重新执行Devstack时候会出现错误</li>
<li>建立本地的Gerrit，并且上传所有代码</li>
<li>从requirements项目中，下载所有的pip，建立本地的pip缓存源，如果是搭建研发环境，可能还需要下载test-requirements的内容和tox</li>
<li>将镜像下载到刚刚创建的Apache服务器</li>
</ul>


<p>完成以上步骤，你可以尽情断掉外网，愉快的进行Devstack的安装了，稍后我会将以上步骤进行进一步完善。</p>

<h2>OFFLINE模式下安装Devstack</h2>

<p>在Devstack中提供了一种OFFLINE的方式，这种方式的含义就是，当你第一次完成安装后，所有需要的内容已经下载到本地，再次运行就没有必要访问网络了(前提是你不想升级)，所以可以将安装模式设置为OFFLINE，避免网络的访问，方法为：</p>

<pre><code class="bash devstack/localrc">OFFLINE=True
</code></pre>

<h2>虚拟机重启后，如何利用rejoin-stack.sh，免重新安装</h2>

<p>其实使用OFFLINE模式，可以在离线状态下无数次重新运行devstack，但是如果不是为了重新配置，我们并没有需要每次重新运行stack.sh。在Devstack中提供了另外一个脚本叫做rejoin-stack.sh，原理很简单就是把所有的进程重新组合进screen，所以我们借助这个脚本完全可以不重新执行stack.sh，快速恢复环境。但是当虚拟机重启后，cinder使用的卷组并不会自动重建，所以在运行rejoin之前，需要将恢复卷组的工作，放入开机启动的脚本中。</p>

<pre><code class="bash /etc/init.d/cinder-setup-backing-file">losetup /dev/loop1 /opt/stack/data/stack-volumes-default-backing-file
losetup /dev/loop2 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file
exit 0
</code></pre>

<pre><code class="bash Run as root">chmod 755 /etc/init.d/cinder-setup-backing-file
ln -s /etc/init.d/cinder-setup-backing-file /etc/rc2.d/S10cinder-setup-backing-file
</code></pre>

<pre><code class="bash Run as normal user">cd $HOME/devstack
./rejoin-stack.sh
</code></pre>

<h2>Scenario 0: 公共部分</h2>

<pre><code class="bash devstack/localrc"># Misc
ADMIN_PASSWORD=sysadmin
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=$ADMIN_PASSWORD

# Target Path
DEST=/opt/stack.kilo

# Enable Logging
LOGFILE=$DEST/logs/stack.sh.log
VERBOSE=True
LOG_COLOR=True
SCREEN_LOGDIR=$DEST/logs
</code></pre>

<h2>Scenario 1: 单节点Nova-Network的安装</h2>

<p>这应该就是Devstack默认的模式，有以下几点需要注意：</p>

<ul>
<li>根据上面的网卡配置</li>
</ul>


<blockquote><p>第一块网卡为NAT方式，用于访问外部网络</p>

<p>第二块为Host-only Adaptor，用于访问云平台</p>

<p>第三块为Host-only Adaptor，用于虚拟机桥接网路</p>

<p>需要注意的是：这种方式下并不能让虚拟机正常访问外部网络，可以通过将eth2设置为Bridge模式，但是这样会造成DHCP冲突(如果外部网络有DHCP)，所以暂时没有完美的解决方案</p></blockquote>

<ul>
<li>打开novnc和consoleauth，否则无法访问VNC</li>
</ul>


<p>这里给出的配置方案是第一种网络配置，即虚拟机无法网络外部网络的情况</p>

<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth

FLAT_INTERFACE=eth1
# eth1 address
HOST_IP=192.168.56.101
FIXED_RANGE=172.24.17.0/24
FIXED_NETWORK_SIZE=254
FLOATING_RANGE=172.16.0.128/25
</code></pre>

<h2>Scenario 2: 双节点Nova-Network的安装</h2>

<ul>
<li>控制节点</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth
disable_service n-cpu n-net n-api-meta c-vol

# current host ip
HOST_IP=192.168.56.101
FLAT_INTERFACE=eth1
MULTI_HOST=1
</code></pre>

<ul>
<li>计算节点</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth
ENABLED_SERVICES=n-cpu,n-net,n-api-meta,c-vol

# current host ip
HOST_IP=192.168.56.101
FLAT_INTERFACE=eth1
# needed by cinder-volume service
DATABASE_TYPE=mysql

# controller ip
SERVICE_HOST=192.168.56.101
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
NOVA_VNC_ENABLED=True
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html"
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</code></pre>

<h2>Scenario 3: 单节点Neutron的安装</h2>

<ul>
<li>基本配置</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth

# Neutron
disable_service n-net
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron
ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas

HOST_IP=192.168.56.101
FIXED_RANGE=20.0.0.0/24
NETWORK_GATEWAY=20.0.0.1
FLOATING_RANGE=172.16.0.0/24
PUBLIC_NETWORK_GATEWAY=172.16.0.1
Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200
</code></pre>

<ul>
<li>OVS设置</li>
</ul>


<p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p>

<pre><code class="bash bash">sudo ip addr flush dev br-ex
</code></pre>

<p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p>

<pre><code class="bash bash">sudo ovs-vsctl add-port br-ex eth2
</code></pre>

<h2>Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)</h2>

<ul>
<li>控制/网络节点
<figure class='code'><figcaption><span>devstack/localrc</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&lt;h1&gt;Nova&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;enable_service n-novnc n-cauth
</span><span class='line'>HOST_IP=192.168.56.101
</span><span class='line'>disable_service n-cpu n-net n-api-meta c-vol&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Neutron&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;disable_service n-net
</span><span class='line'>ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta
</span><span class='line'>FIXED_RANGE=20.0.0.0/24
</span><span class='line'>NETWORK_GATEWAY=20.0.0.1
</span><span class='line'>FLOATING_RANGE=172.16.0.0/24
</span><span class='line'>PUBLIC_NETWORK_GATEWAY=172.16.0.1
</span><span class='line'>Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200</span></code></pre></td></tr></table></div></figure></p>

<ul>
<li>计算节点
<figure class='code'><figcaption><span>devstack/localrc</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&lt;h1&gt;Nova&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;disable_all_services
</span><span class='line'>ENABLED_SERVICES=n-cpu,rabbit,neutron,q-agt,c-vol&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;current host ip&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;HOST_IP=192.168.56.103&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;needed by cinder-volume service&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;DATABASE_TYPE=mysql&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;controller ip&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SERVICE_HOST=192.168.56.101
</span><span class='line'>MYSQL_HOST=$SERVICE_HOST
</span><span class='line'>RABBIT_HOST=$SERVICE_HOST
</span><span class='line'>GLANCE_HOSTPORT=$SERVICE_HOST:9292
</span><span class='line'>NOVA_VNC_ENABLED=True
</span><span class='line'>NOVNCPROXY_URL=&ldquo;&lt;a href="http://$SERVICE_HOST:6080/vnc_auto.html"&gt;http://$SERVICE_HOST:6080/vnc_auto.html&lt;/a&gt;&rdquo;
</span><span class='line'>VNCSERVER_LISTEN=$HOST_IP
</span><span class='line'>VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</span><span class='line'>Q_HOST=$SERVICE_HOST</span></code></pre></td></tr></table></div></figure></p>

<ul>
<li>OVS设置</li>
</ul>


<p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p>

<pre><code class="bash bash">sudo ip addr flush dev br-ex
</code></pre>

<p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p>

<pre><code class="bash bash">sudo ovs-vsctl add-port br-ex eth2
</code></pre>

<h2>Scenario 5: 从源代码安装客户端</h2>

<p>新的Devstack里面默认不再提供client的源代码的安装方式，需要使用localrc中的环境变量进行开启，否则将直接从master获取的client代码进行安装，当然这样会造成系统无法正常使用。那么如何才能确定client在当前Devstack可用的版本呢？最简单的方法可以先从pip中安装包，之后通过pip list | grep client的方式获取client的源代码。这里面提供我在Kilo中使用的版本依赖。</p>

<pre><code class="bash devstack/localrc">KEYSTONECLIENT_BRANCH=1.3.1
CINDERCLIENT_BRANCH=1.1.1
GLANCECLIENT_BRANCH=0.17.1
HEATCLIENT_BRANCH=0.4.0
NEUTRONCLIENT_BRANCH=2.4.0
NOVACLIENT_BRANCH=2.23.0
SWIFTCLIENT_BRANCH=2.4.0

# client code
LIBS_FROM_GIT=python-keystoneclient,python-glanceclient,python-novaclient,python-neutronclient,python-swiftclient,python-cinderclient
</code></pre>

<h2>Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift</h2>

<pre><code class="bash devstack/localrc"># Ceilometer
enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api
enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator

# Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Trove
enable_service trove tr-api tr-tmgr tr-cond

# Sahara
enable_service sahara

# Swift
enable_service s-proxy s-object s-container s-account
SWIFT_REPLICAS=1
SWIFT_HASH=011688b44136573e209e
</code></pre>

<h2>Scenario 7: 安装Ceph</h2>

<pre><code class="bash devstack/localrc"># Ceph
ENABLED_SERVICES+=,ceph
CEPH_LOOPBACK_DISK_SIZE=200G
CEPH_CONF=/etc/ceph/ceph.conf
CEPH_REPLICAS=1

# Glance - Image Service
GLANCE_CEPH_USER=glance
GLANCE_CEPH_POOL=glance-pool

# Cinder - Block Device Service
CINDER_DRIVER=ceph
CINDER_CEPH_USER=cinder
CINDER_CEPH_POOL=cinder-pool
CINDER_CEPH_UUID=1b1519e4-5ecd-11e5-8559-080027f18a73
CINDER_BAK_CEPH_POOL=cinder-backups
CINDER_BAK_CEPH_USER=cinder-backups
CINDER_ENABLED_BACKENDS=ceph
CINDER_ENABLED_BACKENDS=ceph

# Nova - Compute Service
NOVA_CEPH_POOL=nova-pool
</code></pre>

<h2>Scenario 8: 安装Murano</h2>

<p>想通过这个例子演示，对于一个新的OpenStack项目，如何使用Devstack尝鲜。</p>

<pre><code class="bash bash">cd /opt/stack.kilo
git clone https://github.com/openstack/murano --branch=stable/kilo
cd murano/contrib/devstack
cp lib/murano ${DEVSTACK_DIR}/lib
cp lib/murano-dashboard ${DEVSTACK_DIR}/lib
cp extras.d/70-murano.sh ${DEVSTACK_DIR}/extras.d
</code></pre>

<pre><code class="plain devstack/localrc"># Enable Neutron
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron

# Enable Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Enable Murano
enable_service murano murano-api murano-engine
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph性能优化总结(v0.94)]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/06/28/ceph-performance-optimization-summary/"/>
    <updated>2015-06-28T14:30:22+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/06/28/ceph-performance-optimization-summary</id>
    <content type="html"><![CDATA[<p>最近一直在忙着搞Ceph存储的优化和测试，看了各种资料，但是好像没有一篇文章把其中的方法论交代清楚，所以呢想在这里进行一下总结，很多内容并不是我原创，只是做一个总结。如果其中有任何的问题，欢迎各位喷我，以便我提高。</p>

<h2>优化方法论</h2>

<p>做任何事情还是要有个方法论的，“授人以鱼不如授人以渔”的道理吧，方法通了，所有的问题就有了解决的途径。通过对公开资料的分析进行总结，对分布式存储系统的优化离不开以下几点：</p>

<h3>1. 硬件层面</h3>

<ul>
<li>硬件规划</li>
<li>SSD选择</li>
<li>BIOS设置</li>
</ul>


<h3>2. 软件层面</h3>

<ul>
<li>Linux OS</li>
<li>Ceph Configurations</li>
<li>PG Number调整</li>
<li>CRUSH Map</li>
<li>其他因素</li>
</ul>


<!-- more -->


<h2>硬件优化</h2>

<h3>1. 硬件规划</h3>

<ul>
<li>Processor</li>
</ul>


<p>ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。当然如果你使用EC方式，可能需要更多的CPU资源。</p>

<p>ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。</p>

<p>ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。</p>

<ul>
<li>内存</li>
</ul>


<p>ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存，当然2G更好。</p>

<ul>
<li>网络规划</li>
</ul>


<p>万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。</p>

<h3>2. SSD选择</h3>

<p>硬件的选择也直接决定了Ceph集群的性能，从成本考虑，一般选择SATA SSD作为Journal，<a href="http://www.intel.com/content/www/us/en/solid-state-drives/solid-state-drives-dc-s3500-series.html">Intel® SSD DC S3500 Series</a>基本是目前看到的方案中的首选。400G的规格4K随机写可以达到11000 IOPS。如果在预算足够的情况下，推荐使用PCIE SSD，性能会得到进一步提升，但是由于Journal在向数据盘写入数据时Block后续请求，所以Journal的加入并未呈现出想象中的性能提升，但是的确会对Latency有很大的改善。</p>

<p>如何确定你的SSD是否适合作为SSD Journal，可以参考SÉBASTIEN HAN的<a href="http://www.sebastien-han.fr/blog/2014/10/10/ceph-how-to-test-if-your-ssd-is-suitable-as-a-journal-device/">Ceph: How to Test if Your SSD Is Suitable as a Journal Device?</a>，这里面他也列出了常见的SSD的测试结果，从结果来看SATA SSD中，Intel S3500性能表现最好。</p>

<h3>3. BIOS设置</h3>

<ul>
<li>Hyper-Threading(HT)</li>
</ul>


<p>基本做云平台的，VT和HT打开都是必须的，超线程技术(HT)就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高的CPU的运行效率。</p>

<ul>
<li>关闭节能</li>
</ul>


<p>关闭节能后，对性能还是有所提升的，所以坚决调整成性能型(Performance)。当然也可以在操作系统级别进行调整，详细的调整过程请参考<a href="http://www.servernoobs.com/avoiding-cpu-speed-scaling-in-modern-linux-distributions-running-cpu-at-full-speed-tips/">链接</a>，但是不知道是不是由于BIOS已经调整的缘故，所以在CentOS 6.6上并没有发现相关的设置。</p>

<pre><code>for CPUFREQ in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do [ -f $CPUFREQ ] || continue; echo -n performance &gt; $CPUFREQ; done
</code></pre>

<ul>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-numa/">NUMA</a></li>
</ul>


<p>简单来说，NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存，<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2013-December/036211.html">NUMA可能会在某些情况下影响ceph-osd</a>。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。</p>

<pre><code>kernel /vmlinuz-2.6.32-504.12.2.el6.x86_64 ro root=UUID=870d47f8-0357-4a32-909f-74173a9f0633 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM   biosdevname=0 numa=off
</code></pre>

<h2>软件优化</h2>

<h3>1. Linux OS</h3>

<ul>
<li>Kernel pid max</li>
</ul>


<pre><code>echo 4194303 &gt; /proc/sys/kernel/pid_max
</code></pre>

<ul>
<li>Jumbo frames, 交换机端需要支持该功能，系统网卡设置才有效果</li>
</ul>


<pre><code>ifconfig eth0 mtu 9000
</code></pre>

<p>永久设置</p>

<pre><code>echo "MTU=9000" | tee -a /etc/sysconfig/network-script/ifcfg-eth0
/etc/init.d/networking restart
</code></pre>

<ul>
<li>read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，查看默认值</li>
</ul>


<pre><code>cat /sys/block/sda/queue/read_ahead_kb
</code></pre>

<p>根据一些Ceph的公开分享，8192是比较理想的值</p>

<pre><code>echo "8192" &gt; /sys/block/sda/queue/read_ahead_kb
</code></pre>

<ul>
<li>swappiness, 主要控制系统对swap的使用，这个参数的调整最先见于UnitedStack公开的文档中，猜测调整的原因主要是使用swap会影响系统的性能。</li>
</ul>


<pre><code>echo "vm.swappiness = 0" | tee -a /etc/sysctl.conf
</code></pre>

<ul>
<li>I/O Scheduler，关于I/O Scheculder的调整网上已经有很多资料，这里不再赘述，简单说SSD要用noop，SATA/SAS使用deadline。</li>
</ul>


<pre><code>echo "deadline" &gt; /sys/block/sd[x]/queue/scheduler
echo "noop" &gt; /sys/block/sd[x]/queue/scheduler
</code></pre>

<ul>
<li>cgroup</li>
</ul>


<p>这方面的文章好像比较少，昨天在和Ceph社区交流过程中，Jan Schermer说准备把生产环境中的一些脚本贡献出来，但是暂时还没有，他同时也列举了一些使用cgroup进行隔离的<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg21111.html">原因</a>。</p>

<blockquote><ul>
<li>不在process和thread在不同的core上移动(更好的缓存利用)</li>
<li>减少NUMA的影响</li>
<li>网络和存储控制器影响 - 较小</li>
<li>通过限制cpuset来限制Linux调度域(不确定是不是重要但是是最佳实践)</li>
<li>如果开启了HT，可能会造成OSD在thread1上，KVM在thread2上，并且是同一个core。Core的延迟和性能取决于其他一个线程做什么。</li>
</ul>
</blockquote>

<p>这一点具体实现待补充！！！</p>

<h3>2. Ceph Configurations</h3>

<h4>[global]</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> public network </td>
<td> 客户端访问网络 </td>
<td> </td>
<td> 192.168.100.0/24 </td>
</tr>
<tr>
<td> cluster network </td>
<td> 集群网络 </td>
<td> </td>
<td> 192.168.1.0/24 </td>
</tr>
<tr>
<td> max open files </td>
<td> 如果设置了该选项，Ceph会设置系统的max open fds </td>
<td> 0 </td>
<td> 131072 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>查看系统最大文件打开数可以使用命令</li>
</ul>


<pre><code>    cat /proc/sys/fs/file-max
</code></pre>

<hr />

<h4>[osd] - filestore</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> filestore xattr use omap </td>
<td> 为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用 </td>
<td> false </td>
<td> true </td>
</tr>
<tr>
<td> filestore max sync interval </td>
<td> 从日志到数据盘最大同步间隔(seconds) </td>
<td> 5 </td>
<td> 15 </td>
</tr>
<tr>
<td> filestore min sync interval </td>
<td> 从日志到数据盘最小同步间隔(seconds) </td>
<td> 0.1 </td>
<td> 10 </td>
</tr>
<tr>
<td> filestore queue max ops </td>
<td> 数据盘最大接受的操作数 </td>
<td> 500 </td>
<td> 25000 </td>
</tr>
<tr>
<td> filestore queue max bytes </td>
<td> 数据盘一次操作最大字节数(bytes) </td>
<td> 100 &lt;&lt; 20 </td>
<td> 10485760 </td>
</tr>
<tr>
<td> filestore queue committing max ops </td>
<td> 数据盘能够commit的操作数 </td>
<td> 500 </td>
<td> 5000 </td>
</tr>
<tr>
<td> filestore queue committing max bytes </td>
<td> 数据盘能够commit的最大字节数(bytes) </td>
<td> 100 &lt;&lt; 20 </td>
<td> 10485760000 </td>
</tr>
<tr>
<td> filestore op threads </td>
<td> 并发文件系统操作数 </td>
<td> 2 </td>
<td> 32 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>调整omap的原因主要是EXT4文件系统默认仅有4K</li>
<li>filestore queue相关的参数对于性能影响很小，参数调整不会对性能优化有本质上提升</li>
</ul>


<hr />

<h4>[osd] - journal</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd journal size </td>
<td> OSD日志大小(MB) </td>
<td> 5120 </td>
<td> 20000 </td>
</tr>
<tr>
<td> journal max write bytes </td>
<td> journal一次性写入的最大字节数(bytes) </td>
<td> 10 &lt;&lt; 20 </td>
<td> 1073714824 </td>
</tr>
<tr>
<td> journal max write entries </td>
<td> journal一次性写入的最大记录数 </td>
<td> 100 </td>
<td> 10000 </td>
</tr>
<tr>
<td> journal queue max ops </td>
<td> journal一次性最大在队列中的操作数 </td>
<td> 500 </td>
<td> 50000 </td>
</tr>
<tr>
<td> journal queue max bytes </td>
<td> journal一次性最大在队列中的字节数(bytes) </td>
<td> 10 &lt;&lt; 20 </td>
<td> 10485760000 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space.</li>
<li>上面这段话的意思就是，Ceph OSD进程在往数据盘上刷数据的过程中，是停止写操作的。</li>
</ul>


<hr />

<h4>[osd] - osd config tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd max write size </td>
<td> OSD一次可写入的最大值(MB) </td>
<td> 90 </td>
<td> 512 </td>
</tr>
<tr>
<td> osd client message size cap </td>
<td> 客户端允许在内存中的最大数据(bytes) </td>
<td> 524288000 </td>
<td> 2147483648 </td>
</tr>
<tr>
<td> osd deep scrub stride </td>
<td> 在Deep Scrub时候允许读取的字节数(bytes) </td>
<td> 524288 </td>
<td> 131072 </td>
</tr>
<tr>
<td> osd op threads </td>
<td> OSD进程操作的线程数 </td>
<td> 2 </td>
<td> 8 </td>
</tr>
<tr>
<td> osd disk threads </td>
<td> OSD密集型操作例如恢复和Scrubbing时的线程 </td>
<td> 1 </td>
<td> 4 </td>
</tr>
<tr>
<td> osd map cache size </td>
<td> 保留OSD Map的缓存(MB) </td>
<td> 500 </td>
<td> 1024 </td>
</tr>
<tr>
<td> osd map cache bl size </td>
<td> OSD进程在内存中的OSD Map缓存(MB) </td>
<td> 50 </td>
<td> 128 </td>
</tr>
<tr>
<td> osd mount options xfs </td>
<td> Ceph OSD xfs Mount选项 </td>
<td> rw,noatime,inode64 </td>
<td> rw,noexec,nodev,noatime,nodiratime,nobarrier </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>增加osd op threads和disk threads会带来额外的CPU开销</li>
</ul>


<hr />

<h4>[osd] - recovery tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd recovery op priority </td>
<td> 恢复操作优先级，取值1-63，值越高占用资源越高 </td>
<td> 10 </td>
<td> 4 </td>
</tr>
<tr>
<td> osd recovery max active </td>
<td> 同一时间内活跃的恢复请求数 </td>
<td> 15 </td>
<td> 10 </td>
</tr>
<tr>
<td> osd max backfills </td>
<td> 一个OSD允许的最大backfills数 </td>
<td> 10 </td>
<td> 4 </td>
</tr>
</tbody>
</table>


<h4>[osd] - client tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> rbd cache </td>
<td> RBD缓存 </td>
<td> true </td>
<td> true </td>
</tr>
<tr>
<td> rbd cache size </td>
<td> RBD缓存大小(bytes) </td>
<td> 33554432 </td>
<td> 268435456 </td>
</tr>
<tr>
<td> rbd cache max dirty </td>
<td> 缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through </td>
<td> 25165824 </td>
<td> 134217728 </td>
</tr>
<tr>
<td> rbd cache max dirty age </td>
<td> 在被刷新到存储盘前dirty数据存在缓存的时间(seconds) </td>
<td> 1 </td>
<td> 5 </td>
</tr>
</tbody>
</table>


<h4>关闭Debug</h4>

<h3>3. PG Number</h3>

<p>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。</p>

<pre><code>Total PGs = (Total_number_of_OSD * 100) / max_replication_count
</code></pre>

<p>例如15个OSD，副本数为3的情况下，根据公式计算的结果应该为500，最接近512，所以需要设定该pool(volumes)的pg_num和pgp_num都为512.</p>

<pre><code>ceph osd pool set volumes pg_num 512
ceph osd pool set volumes pgp_num 512
</code></pre>

<h3>4. CRUSH Map</h3>

<p>CRUSH是一个非常灵活的方式，CRUSH MAP的调整取决于部署的具体环境，这个可能需要根据具体情况进行分析，这里面就不再赘述了。</p>

<h3>5. 其他因素的影响</h3>

<p>在今年的(2015年)的Ceph Day上，海云捷迅在调优过程中分享过一个由于在集群中存在一个性能不好的磁盘，导致整个集群性能下降的case。通过osd perf可以提供磁盘latency的状况，同时在运维过程中也可以作为监控的一个重要指标，很明显在下面的例子中，OSD 8的磁盘延时较长，所以需要考虑将该OSD剔除出集群：</p>

<pre><code>ceph osd perf
</code></pre>

<pre><code>osd fs_commit_latency(ms) fs_apply_latency(ms)
  0                    14                   17
  1                    14                   16
  2                    10                   11
  3                     4                    5
  4                    13                   15
  5                    17                   20
  6                    15                   18
  7                    14                   16
  8                   299                  329
</code></pre>

<h2>ceph.conf</h2>

<pre><code>[global]
fsid = 059f27e8-a23f-4587-9033-3e3679d03b31
mon_host = 10.10.20.102, 10.10.20.101, 10.10.20.100
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd pool default size = 3
osd pool default min size = 1

public network = 10.10.20.0/24
cluster network = 10.10.20.0/24

max open files = 131072

[mon]
mon data = /var/lib/ceph/mon/ceph-$id

[osd]
osd data = /var/lib/ceph/osd/ceph-$id
osd journal size = 20000
osd mkfs type = xfs
osd mkfs options xfs = -f

filestore xattr use omap = true
filestore min sync interval = 10
filestore max sync interval = 15
filestore queue max ops = 25000
filestore queue max bytes = 10485760
filestore queue committing max ops = 5000
filestore queue committing max bytes = 10485760000

journal max write bytes = 1073714824
journal max write entries = 10000
journal queue max ops = 50000
journal queue max bytes = 10485760000

osd max write size = 512
osd client message size cap = 2147483648
osd deep scrub stride = 131072
osd op threads = 8
osd disk threads = 4
osd map cache size = 1024
osd map cache bl size = 128
osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier"
osd recovery op priority = 4
osd recovery max active = 10
osd max backfills = 4

[client]
rbd cache = true
rbd cache size = 268435456
rbd cache max dirty = 134217728
rbd cache max dirty age = 5
</code></pre>

<h2>总结</h2>

<p>优化是一个长期迭代的过程，所有的方法都是别人的，只有在实践过程中才能发现自己的，本篇文章仅仅是一个开始，欢迎各位积极补充，共同完成一篇具有指导性的文章。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph集群磁盘没有剩余空间的解决方法]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full/"/>
    <updated>2015-05-12T09:21:42+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full</id>
    <content type="html"><![CDATA[<h2>故障描述</h2>

<p>OpenStack + Ceph集群在使用过程中，由于虚拟机拷入大量新的数据，导致集群的磁盘迅速消耗，没有空余空间，虚拟机无法操作，Ceph集群所有操作都无法执行。</p>

<!-- more -->


<h2>故障现象</h2>

<ul>
<li>尝试使用OpenStack重启虚拟机无效</li>
<li>尝试直接用rbd命令直接删除块失败</li>
</ul>


<pre><code class="plain">[root@controller ~]# rbd -p volumes rm volume-c55fd052-212d-4107-a2ac-cf53bfc049be
2015-04-29 05:31:31.719478 7f5fb82f7760  0 client.4781741.objecter  FULL, paused modify 0xe9a9e0 tid 6
</code></pre>

<ul>
<li>查看ceph健康状态</li>
</ul>


<pre><code class="plain ceph -s">cluster 059f27e8-a23f-4587-9033-3e3679d03b31
 health HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482/129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)
 monmap e6: 4 mons at {node-5e40.cloud.com=10.10.20.40:6789/0,node-6670.cloud.com=10.10.20.31:6789/0,node-66c4.cloud.com=10.10.20.36:6789/0,node-fb27.cloud.com=10.10.20.41:6789/0}, election epoch 886, quorum 0,1,2,3 node-6670.cloud.com,node-66c4.cloud.com,node-5e40.cloud.com,node-fb27.cloud.com
 osdmap e2743: 3 osds: 3 up, 3 in
        flags full
  pgmap v6564199: 320 pgs, 4 pools, 262 GB data, 43027 objects
        786 GB used, 47785 MB / 833 GB avail
        7482/129081 objects degraded (5.796%)
             300 active+clean
              20 active+degraded+remapped+backfill_toofull
</code></pre>

<pre><code class="plain ceph health detail">HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482/129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)
pg 3.8 is stuck unclean for 7067109.597691, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.7d is stuck unclean for 1852078.505139, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.21 is stuck unclean for 7072842.637848, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.22 is stuck unclean for 7070880.213397, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.a is stuck unclean for 7067057.863562, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.7f is stuck unclean for 7067122.493746, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.5 is stuck unclean for 7067088.369629, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1e is stuck unclean for 7073386.246281, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.19 is stuck unclean for 7068035.310269, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.5d is stuck unclean for 1852078.505949, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1a is stuck unclean for 7067088.429544, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1b is stuck unclean for 7072773.771385, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.3 is stuck unclean for 7067057.864514, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.15 is stuck unclean for 7067088.825483, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.11 is stuck unclean for 7067057.862408, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.6d is stuck unclean for 7067083.634454, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.6e is stuck unclean for 7067098.452576, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.c is stuck unclean for 5658116.678331, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.e is stuck unclean for 7067078.646953, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.20 is stuck unclean for 7067140.530849, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.7d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.7f is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.6d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.6e is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.5d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.20 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.21 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.22 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.1e is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.19 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.1a is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.1b is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.15 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.11 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.c is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.e is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.8 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.a is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.5 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.3 is active+degraded+remapped+backfill_toofull, acting [2,0]
recovery 7482/129081 objects degraded (5.796%)
osd.0 is full at 95%
osd.2 is full at 95%
osd.1 is near full at 93%
</code></pre>

<h2>解决方案一(已验证)</h2>

<p>增加OSD节点，这也是官方文档中推荐的做法，增加新的节点后，Ceph开始重新平衡数据，OSD使用空间开始下降</p>

<pre><code class="plain">2015-04-29 06:51:58.623262 osd.1 [WRN] OSD near full (91%)
2015-04-29 06:52:01.500813 osd.2 [WRN] OSD near full (92%)
</code></pre>

<h2>解决方案二(理论上，没有进行验证)</h2>

<p>如果在没有新的硬盘的情况下，只能采用另外一种方式。在当前状态下，Ceph不允许任何的读写操作，所以此时任何的Ceph命令都不好使，解决的方案就是尝试降低Ceph对于full的比例定义，我们从上面的日志中可以看到Ceph的full的比例为95%，我们需要做的就是提高full的比例，之后尽快尝试删除数据，将比例下降。</p>

<ul>
<li>尝试直接用命令设置，但是失败了，Ceph集群并没有重新同步数据，怀疑可能仍然需要重启服务本身</li>
</ul>


<pre><code class="plain">ceph mon tell \* injectargs '--mon-osd-full-ratio 0.98'
</code></pre>

<ul>
<li>修改配置文件，之后重启monitor服务，但是担心出问题，所以没有敢尝试该方法，后续经过在邮件列表确认，该方法应该不会对数据产生影响，但是前提是在恢复期间，所有的虚拟机不要向Ceph再写入任何数据。</li>
</ul>


<p>默认情况下full的比例是95%，而near full的比例是85%，所以需要根据实际情况对该配置进行调整。</p>

<pre><code class="plain /etc/ceph/ceph.conf">[global]
    mon osd full ratio = .98
    mon osd nearfull ratio = .80
</code></pre>

<h2>分析总结</h2>

<h3>原因</h3>

<p>根据Ceph官方文档中的描述，当一个OSD full比例达到95%时，集群将不接受任何Ceph Client端的读写数据的请求。所以导致虚拟机在重启时，无法启动的情况。</p>

<h3>解决方法</h3>

<p>从官方的推荐来看，应该比较支持添加新的OSD的方式，当然临时的提高比例是一个解决方案，但是并不推荐，因为需要手动的删除数据去解决，而且一旦再有一个新的节点出现故障，仍然会出现比例变满的状况，所以解决之道最好是扩容。</p>

<h3>思考</h3>

<p>在这次故障过程中，有两点是值得思考的：</p>

<ul>
<li>监控：由于当时服务器在配置过程中DNS配置错误，导致监控邮件无法正常发出，从而没有收到Ceph WARN的提示信息</li>
<li>云平台本身： 由于Ceph的机制，在OpenStack平台中分配中，大多时候是超分的，从用户角度看，拷贝大量数据的行为并没有不妥之处，但是由于云平台并没有相应的预警机制，导致了该问题的发生</li>
</ul>


<h2>参考文档</h2>

<ul>
<li><a href="http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity">http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
