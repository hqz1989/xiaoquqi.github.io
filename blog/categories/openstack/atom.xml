<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Openstack | RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/blog/categories/openstack/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2015-09-03T18:44:24+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ceph性能优化总结(v0.94)]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/06/28/ceph-performance-optimization-summary/"/>
    <updated>2015-06-28T14:30:22+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/06/28/ceph-performance-optimization-summary</id>
    <content type="html"><![CDATA[<p>最近一直在忙着搞Ceph存储的优化和测试，看了各种资料，但是好像没有一篇文章把其中的方法论交代清楚，所以呢想在这里进行一下总结，很多内容并不是我原创，只是做一个总结。如果其中有任何的问题，欢迎各位喷我，以便我提高。</p>

<h2>优化方法论</h2>

<p>做任何事情还是要有个方法论的，“授人以鱼不如授人以渔”的道理吧，方法通了，所有的问题就有了解决的途径。通过对公开资料的分析进行总结，对分布式存储系统的优化离不开以下几点：</p>

<h3>1. 硬件层面</h3>

<ul>
<li>硬件规划</li>
<li>SSD选择</li>
<li>BIOS设置</li>
</ul>


<h3>2. 软件层面</h3>

<ul>
<li>Linux OS</li>
<li>Ceph Configurations</li>
<li>PG Number调整</li>
<li>CRUSH Map</li>
<li>其他因素</li>
</ul>


<!-- more -->


<h2>硬件优化</h2>

<h3>1. 硬件规划</h3>

<ul>
<li>Processor</li>
</ul>


<p>ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。当然如果你使用EC方式，可能需要更多的CPU资源。</p>

<p>ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。</p>

<p>ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。</p>

<ul>
<li>内存</li>
</ul>


<p>ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存，当然2G更好。</p>

<ul>
<li>网络规划</li>
</ul>


<p>万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。</p>

<h3>2. SSD选择</h3>

<p>硬件的选择也直接决定了Ceph集群的性能，从成本考虑，一般选择SATA SSD作为Journal，<a href="http://www.intel.com/content/www/us/en/solid-state-drives/solid-state-drives-dc-s3500-series.html">Intel® SSD DC S3500 Series</a>基本是目前看到的方案中的首选。400G的规格4K随机写可以达到11000 IOPS。如果在预算足够的情况下，推荐使用PCIE SSD，性能会得到进一步提升，但是由于Journal在向数据盘写入数据时Block后续请求，所以Journal的加入并未呈现出想象中的性能提升，但是的确会对Latency有很大的改善。</p>

<p>如何确定你的SSD是否适合作为SSD Journal，可以参考SÉBASTIEN HAN的<a href="http://www.sebastien-han.fr/blog/2014/10/10/ceph-how-to-test-if-your-ssd-is-suitable-as-a-journal-device/">Ceph: How to Test if Your SSD Is Suitable as a Journal Device?</a>，这里面他也列出了常见的SSD的测试结果，从结果来看SATA SSD中，Intel S3500性能表现最好。</p>

<h3>3. BIOS设置</h3>

<ul>
<li>Hyper-Threading(HT)</li>
</ul>


<p>基本做云平台的，VT和HT打开都是必须的，超线程技术(HT)就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高的CPU的运行效率。</p>

<ul>
<li>关闭节能</li>
</ul>


<p>关闭节能后，对性能还是有所提升的，所以坚决调整成性能型(Performance)。当然也可以在操作系统级别进行调整，详细的调整过程请参考<a href="http://www.servernoobs.com/avoiding-cpu-speed-scaling-in-modern-linux-distributions-running-cpu-at-full-speed-tips/">链接</a>，但是不知道是不是由于BIOS已经调整的缘故，所以在CentOS 6.6上并没有发现相关的设置。</p>

<pre><code>for CPUFREQ in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do [ -f $CPUFREQ ] || continue; echo -n performance &gt; $CPUFREQ; done
</code></pre>

<ul>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-numa/">NUMA</a></li>
</ul>


<p>简单来说，NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存，<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2013-December/036211.html">NUMA可能会在某些情况下影响ceph-osd</a>。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。</p>

<pre><code>kernel /vmlinuz-2.6.32-504.12.2.el6.x86_64 ro root=UUID=870d47f8-0357-4a32-909f-74173a9f0633 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM   biosdevname=0 numa=off
</code></pre>

<h2>软件优化</h2>

<h3>1. Linux OS</h3>

<ul>
<li>Kernel pid max</li>
</ul>


<pre><code>echo 4194303 &gt; /proc/sys/kernel/pid_max
</code></pre>

<ul>
<li>Jumbo frames, 交换机端需要支持该功能，系统网卡设置才有效果</li>
</ul>


<pre><code>ifconfig eth0 mtu 9000
</code></pre>

<p>永久设置</p>

<pre><code>echo "MTU=9000" | tee -a /etc/sysconfig/network-script/ifcfg-eth0
/etc/init.d/networking restart
</code></pre>

<ul>
<li>read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，查看默认值</li>
</ul>


<pre><code>cat /sys/block/sda/queue/read_ahead_kb
</code></pre>

<p>根据一些Ceph的公开分享，8192是比较理想的值</p>

<pre><code>echo "8192" &gt; /sys/block/sda/queue/read_ahead_kb
</code></pre>

<ul>
<li>swappiness, 主要控制系统对swap的使用，这个参数的调整最先见于UnitedStack公开的文档中，猜测调整的原因主要是使用swap会影响系统的性能。</li>
</ul>


<pre><code>echo "vm.swappiness = 0" | tee -a /etc/sysctl.conf
</code></pre>

<ul>
<li>I/O Scheduler，关于I/O Scheculder的调整网上已经有很多资料，这里不再赘述，简单说SSD要用noop，SATA/SAS使用deadline。</li>
</ul>


<pre><code>echo "deadline" &gt; /sys/block/sd[x]/queue/scheduler
echo "noop" &gt; /sys/block/sd[x]/queue/scheduler
</code></pre>

<ul>
<li>cgroup</li>
</ul>


<p>这方面的文章好像比较少，昨天在和Ceph社区交流过程中，Jan Schermer说准备把生产环境中的一些脚本贡献出来，但是暂时还没有，他同时也列举了一些使用cgroup进行隔离的<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg21111.html">原因</a>。</p>

<blockquote><ul>
<li>不在process和thread在不同的core上移动(更好的缓存利用)</li>
<li>减少NUMA的影响</li>
<li>网络和存储控制器影响 - 较小</li>
<li>通过限制cpuset来限制Linux调度域(不确定是不是重要但是是最佳实践)</li>
<li>如果开启了HT，可能会造成OSD在thread1上，KVM在thread2上，并且是同一个core。Core的延迟和性能取决于其他一个线程做什么。</li>
</ul>
</blockquote>

<p>这一点具体实现待补充！！！</p>

<h3>2. Ceph Configurations</h3>

<h4>[global]</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> public network </td>
<td> 客户端访问网络 </td>
<td> </td>
<td> 192.168.100.0/24 </td>
</tr>
<tr>
<td> cluster network </td>
<td> 集群网络 </td>
<td> </td>
<td> 192.168.1.0/24 </td>
</tr>
<tr>
<td> max open files </td>
<td> 如果设置了该选项，Ceph会设置系统的max open fds </td>
<td> 0 </td>
<td> 131072 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>查看系统最大文件打开数可以使用命令</li>
</ul>


<pre><code>    cat /proc/sys/fs/file-max
</code></pre>

<hr />

<h4>[osd] - filestore</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> filestore xattr use omap </td>
<td> 为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用 </td>
<td> false </td>
<td> true </td>
</tr>
<tr>
<td> filestore max sync interval </td>
<td> 从日志到数据盘最大同步间隔(seconds) </td>
<td> 5 </td>
<td> 15 </td>
</tr>
<tr>
<td> filestore min sync interval </td>
<td> 从日志到数据盘最小同步间隔(seconds) </td>
<td> 0.1 </td>
<td> 10 </td>
</tr>
<tr>
<td> filestore queue max ops </td>
<td> 数据盘最大接受的操作数 </td>
<td> 500 </td>
<td> 25000 </td>
</tr>
<tr>
<td> filestore queue max bytes </td>
<td> 数据盘一次操作最大字节数(bytes) </td>
<td> 100 &lt;&lt; 20 </td>
<td> 10485760 </td>
</tr>
<tr>
<td> filestore queue committing max ops </td>
<td> 数据盘能够commit的操作数 </td>
<td> 500 </td>
<td> 5000 </td>
</tr>
<tr>
<td> filestore queue committing max bytes </td>
<td> 数据盘能够commit的最大字节数(bytes) </td>
<td> 100 &lt;&lt; 20 </td>
<td> 10485760000 </td>
</tr>
<tr>
<td> filestore op threads </td>
<td> 并发文件系统操作数 </td>
<td> 2 </td>
<td> 32 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>调整omap的原因主要是EXT4文件系统默认仅有4K</li>
<li>filestore queue相关的参数对于性能影响很小，参数调整不会对性能优化有本质上提升</li>
</ul>


<hr />

<h4>[osd] - journal</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd journal size </td>
<td> OSD日志大小(MB) </td>
<td> 5120 </td>
<td> 20000 </td>
</tr>
<tr>
<td> journal max write bytes </td>
<td> journal一次性写入的最大字节数(bytes) </td>
<td> 10 &lt;&lt; 20 </td>
<td> 1073714824 </td>
</tr>
<tr>
<td> journal max write entries </td>
<td> journal一次性写入的最大记录数 </td>
<td> 100 </td>
<td> 10000 </td>
</tr>
<tr>
<td> journal queue max ops </td>
<td> journal一次性最大在队列中的操作数 </td>
<td> 500 </td>
<td> 50000 </td>
</tr>
<tr>
<td> journal queue max bytes </td>
<td> journal一次性最大在队列中的字节数(bytes) </td>
<td> 10 &lt;&lt; 20 </td>
<td> 10485760000 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space.</li>
<li>上面这段话的意思就是，Ceph OSD进程在往数据盘上刷数据的过程中，是停止写操作的。</li>
</ul>


<hr />

<h4>[osd] - osd config tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd max write size </td>
<td> OSD一次可写入的最大值(MB) </td>
<td> 90 </td>
<td> 512 </td>
</tr>
<tr>
<td> osd client message size cap </td>
<td> 客户端允许在内存中的最大数据(bytes) </td>
<td> 524288000 </td>
<td> 2147483648 </td>
</tr>
<tr>
<td> osd deep scrub stride </td>
<td> 在Deep Scrub时候允许读取的字节数(bytes) </td>
<td> 524288 </td>
<td> 131072 </td>
</tr>
<tr>
<td> osd op threads </td>
<td> OSD进程操作的线程数 </td>
<td> 2 </td>
<td> 8 </td>
</tr>
<tr>
<td> osd disk threads </td>
<td> OSD密集型操作例如恢复和Scrubbing时的线程 </td>
<td> 1 </td>
<td> 4 </td>
</tr>
<tr>
<td> osd map cache size </td>
<td> 保留OSD Map的缓存(MB) </td>
<td> 500 </td>
<td> 1024 </td>
</tr>
<tr>
<td> osd map cache bl size </td>
<td> OSD进程在内存中的OSD Map缓存(MB) </td>
<td> 50 </td>
<td> 128 </td>
</tr>
<tr>
<td> osd mount options xfs </td>
<td> Ceph OSD xfs Mount选项 </td>
<td> rw,noatime,inode64 </td>
<td> rw,noexec,nodev,noatime,nodiratime,nobarrier </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>增加osd op threads和disk threads会带来额外的CPU开销</li>
</ul>


<hr />

<h4>[osd] - recovery tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd recovery op priority </td>
<td> 恢复操作优先级，取值1-63，值越高占用资源越高 </td>
<td> 10 </td>
<td> 4 </td>
</tr>
<tr>
<td> osd recovery max active </td>
<td> 同一时间内活跃的恢复请求数 </td>
<td> 15 </td>
<td> 10 </td>
</tr>
<tr>
<td> osd max backfills </td>
<td> 一个OSD允许的最大backfills数 </td>
<td> 10 </td>
<td> 4 </td>
</tr>
</tbody>
</table>


<h4>[osd] - client tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> rbd cache </td>
<td> RBD缓存 </td>
<td> true </td>
<td> true </td>
</tr>
<tr>
<td> rbd cache size </td>
<td> RBD缓存大小(bytes) </td>
<td> 33554432 </td>
<td> 268435456 </td>
</tr>
<tr>
<td> rbd cache max dirty </td>
<td> 缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through </td>
<td> 25165824 </td>
<td> 134217728 </td>
</tr>
<tr>
<td> rbd cache max dirty age </td>
<td> 在被刷新到存储盘前dirty数据存在缓存的时间(seconds) </td>
<td> 1 </td>
<td> 5 </td>
</tr>
</tbody>
</table>


<h4>关闭Debug</h4>

<h3>3. PG Number</h3>

<p>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。</p>

<pre><code>Total PGs = (Total_number_of_OSD * 100) / max_replication_count
</code></pre>

<p>例如15个OSD，副本数为3的情况下，根据公式计算的结果应该为500，最接近512，所以需要设定该pool(volumes)的pg_num和pgp_num都为512.</p>

<pre><code>ceph osd pool set volumes pg_num 512
ceph osd pool set volumes pgp_num 512
</code></pre>

<h3>4. CRUSH Map</h3>

<p>CRUSH是一个非常灵活的方式，CRUSH MAP的调整取决于部署的具体环境，这个可能需要根据具体情况进行分析，这里面就不再赘述了。</p>

<h3>5. 其他因素的影响</h3>

<p>在今年的(2015年)的Ceph Day上，海云捷迅在调优过程中分享过一个由于在集群中存在一个性能不好的磁盘，导致整个集群性能下降的case。通过osd perf可以提供磁盘latency的状况，同时在运维过程中也可以作为监控的一个重要指标，很明显在下面的例子中，OSD 8的磁盘延时较长，所以需要考虑将该OSD剔除出集群：</p>

<pre><code>ceph osd perf
</code></pre>

<pre><code>osd fs_commit_latency(ms) fs_apply_latency(ms)
  0                    14                   17
  1                    14                   16
  2                    10                   11
  3                     4                    5
  4                    13                   15
  5                    17                   20
  6                    15                   18
  7                    14                   16
  8                   299                  329
</code></pre>

<h2>ceph.conf</h2>

<pre><code>[global]
fsid = 059f27e8-a23f-4587-9033-3e3679d03b31
mon_host = 10.10.20.102, 10.10.20.101, 10.10.20.100
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd pool default size = 3
osd pool default min size = 1

public network = 10.10.20.0/24
cluster network = 10.10.20.0/24

max open files = 131072

[mon]
mon data = /var/lib/ceph/mon/ceph-$id

[osd]
osd data = /var/lib/ceph/osd/ceph-$id
osd journal size = 20000
osd mkfs type = xfs
osd mkfs options xfs = -f

filestore xattr use omap = true
filestore min sync interval = 10
filestore max sync interval = 15
filestore queue max ops = 25000
filestore queue max bytes = 10485760
filestore queue committing max ops = 5000
filestore queue committing max bytes = 10485760000

journal max write bytes = 1073714824
journal max write entries = 10000
journal queue max ops = 50000
journal queue max bytes = 10485760000

osd max write size = 512
osd client message size cap = 2147483648
osd deep scrub stride = 131072
osd op threads = 8
osd disk threads = 4
osd map cache size = 1024
osd map cache bl size = 128
osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier"
osd recovery op priority = 4
osd recovery max active = 10
osd max backfills = 4

[client]
rbd cache = true
rbd cache size = 268435456
rbd cache max dirty = 134217728
rbd cache max dirty age = 5
</code></pre>

<h2>总结</h2>

<p>优化是一个长期迭代的过程，所有的方法都是别人的，只有在实践过程中才能发现自己的，本篇文章仅仅是一个开始，欢迎各位积极补充，共同完成一篇具有指导性的文章。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph集群磁盘没有剩余空间的解决方法]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full/"/>
    <updated>2015-05-12T09:21:42+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full</id>
    <content type="html"><![CDATA[<h2>故障描述</h2>

<p>OpenStack + Ceph集群在使用过程中，由于虚拟机拷入大量新的数据，导致集群的磁盘迅速消耗，没有空余空间，虚拟机无法操作，Ceph集群所有操作都无法执行。</p>

<!-- more -->


<h2>故障现象</h2>

<ul>
<li>尝试使用OpenStack重启虚拟机无效</li>
<li>尝试直接用rbd命令直接删除块失败</li>
</ul>


<pre><code class="plain">[root@controller ~]# rbd -p volumes rm volume-c55fd052-212d-4107-a2ac-cf53bfc049be
2015-04-29 05:31:31.719478 7f5fb82f7760  0 client.4781741.objecter  FULL, paused modify 0xe9a9e0 tid 6
</code></pre>

<ul>
<li>查看ceph健康状态</li>
</ul>


<pre><code class="plain ceph -s">cluster 059f27e8-a23f-4587-9033-3e3679d03b31
 health HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482/129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)
 monmap e6: 4 mons at {node-5e40.cloud.com=10.10.20.40:6789/0,node-6670.cloud.com=10.10.20.31:6789/0,node-66c4.cloud.com=10.10.20.36:6789/0,node-fb27.cloud.com=10.10.20.41:6789/0}, election epoch 886, quorum 0,1,2,3 node-6670.cloud.com,node-66c4.cloud.com,node-5e40.cloud.com,node-fb27.cloud.com
 osdmap e2743: 3 osds: 3 up, 3 in
        flags full
  pgmap v6564199: 320 pgs, 4 pools, 262 GB data, 43027 objects
        786 GB used, 47785 MB / 833 GB avail
        7482/129081 objects degraded (5.796%)
             300 active+clean
              20 active+degraded+remapped+backfill_toofull
</code></pre>

<pre><code class="plain ceph health detail">HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482/129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)
pg 3.8 is stuck unclean for 7067109.597691, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.7d is stuck unclean for 1852078.505139, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.21 is stuck unclean for 7072842.637848, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.22 is stuck unclean for 7070880.213397, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.a is stuck unclean for 7067057.863562, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.7f is stuck unclean for 7067122.493746, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.5 is stuck unclean for 7067088.369629, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1e is stuck unclean for 7073386.246281, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.19 is stuck unclean for 7068035.310269, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.5d is stuck unclean for 1852078.505949, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1a is stuck unclean for 7067088.429544, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1b is stuck unclean for 7072773.771385, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.3 is stuck unclean for 7067057.864514, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.15 is stuck unclean for 7067088.825483, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.11 is stuck unclean for 7067057.862408, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.6d is stuck unclean for 7067083.634454, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.6e is stuck unclean for 7067098.452576, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.c is stuck unclean for 5658116.678331, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.e is stuck unclean for 7067078.646953, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.20 is stuck unclean for 7067140.530849, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.7d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.7f is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.6d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.6e is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.5d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.20 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.21 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.22 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.1e is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.19 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.1a is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.1b is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.15 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.11 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.c is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.e is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.8 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.a is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.5 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.3 is active+degraded+remapped+backfill_toofull, acting [2,0]
recovery 7482/129081 objects degraded (5.796%)
osd.0 is full at 95%
osd.2 is full at 95%
osd.1 is near full at 93%
</code></pre>

<h2>解决方案一(已验证)</h2>

<p>增加OSD节点，这也是官方文档中推荐的做法，增加新的节点后，Ceph开始重新平衡数据，OSD使用空间开始下降</p>

<pre><code class="plain">2015-04-29 06:51:58.623262 osd.1 [WRN] OSD near full (91%)
2015-04-29 06:52:01.500813 osd.2 [WRN] OSD near full (92%)
</code></pre>

<h2>解决方案二(理论上，没有进行验证)</h2>

<p>如果在没有新的硬盘的情况下，只能采用另外一种方式。在当前状态下，Ceph不允许任何的读写操作，所以此时任何的Ceph命令都不好使，解决的方案就是尝试降低Ceph对于full的比例定义，我们从上面的日志中可以看到Ceph的full的比例为95%，我们需要做的就是提高full的比例，之后尽快尝试删除数据，将比例下降。</p>

<ul>
<li>尝试直接用命令设置，但是失败了，Ceph集群并没有重新同步数据，怀疑可能仍然需要重启服务本身</li>
</ul>


<pre><code class="plain">ceph mon tell \* injectargs '--mon-osd-full-ratio 0.98'
</code></pre>

<ul>
<li>修改配置文件，之后重启monitor服务，但是担心出问题，所以没有敢尝试该方法，后续经过在邮件列表确认，该方法应该不会对数据产生影响，但是前提是在恢复期间，所有的虚拟机不要向Ceph再写入任何数据。</li>
</ul>


<p>默认情况下full的比例是95%，而near full的比例是85%，所以需要根据实际情况对该配置进行调整。</p>

<pre><code class="plain /etc/ceph/ceph.conf">[global]
    mon osd full ratio = .98
    mon osd nearfull ratio = .80
</code></pre>

<h2>分析总结</h2>

<h3>原因</h3>

<p>根据Ceph官方文档中的描述，当一个OSD full比例达到95%时，集群将不接受任何Ceph Client端的读写数据的请求。所以导致虚拟机在重启时，无法启动的情况。</p>

<h3>解决方法</h3>

<p>从官方的推荐来看，应该比较支持添加新的OSD的方式，当然临时的提高比例是一个解决方案，但是并不推荐，因为需要手动的删除数据去解决，而且一旦再有一个新的节点出现故障，仍然会出现比例变满的状况，所以解决之道最好是扩容。</p>

<h3>思考</h3>

<p>在这次故障过程中，有两点是值得思考的：</p>

<ul>
<li>监控：由于当时服务器在配置过程中DNS配置错误，导致监控邮件无法正常发出，从而没有收到Ceph WARN的提示信息</li>
<li>云平台本身： 由于Ceph的机制，在OpenStack平台中分配中，大多时候是超分的，从用户角度看，拷贝大量数据的行为并没有不妥之处，但是由于云平台并没有相应的预警机制，导致了该问题的发生</li>
</ul>


<h2>参考文档</h2>

<ul>
<li><a href="http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity">http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Kilo)Devstack Kilo版本localrc推荐]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/11/best-localrc-for-devstack-kilo/"/>
    <updated>2015-05-11T11:33:44+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/11/best-localrc-for-devstack-kilo</id>
    <content type="html"><![CDATA[<p>Devstack在Kilo版本中发生了一些变化，其中一个commit(279cfe75198c723519f1fb361b2bff3c641c6cef)的就是优化默认启动的程序，尽量减小对硬件的要求。如果不修改默认的配置进行安装，会产生一些问题，例如VNC无法打开，Heat模块没有加载等。这里给出一个个人比较常用的localrc，供大家参考。该配置在Ubuntu 14.04 Server LTS进行了测试。</p>

<!-- more -->


<p>该配置文件中开启了所有的OpenStack的核心模块，以下几点需要注意：</p>

<ul>
<li>为了运行Neutron，服务器必须是双网卡，否则外网不会通</li>
<li>我的实验网段为200.21.0.0/16，eth0的IP为200.21.1.61，eth1与eth0为同一网段</li>
<li>eth1为公网访问网络，floating网络范围200.21.50.1/24，配置的GATEWAY为200.21.50.2</li>
<li>保证eth1所处的网段能够连接外网，但是配置为manual模式，配置如下：</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth1
iface eth1 inet manual
up ifconfig $IFACE 0.0.0.0 up
down ifconfig $IFACE 0.0.0.0 down
</code></pre>

<ul>
<li>localrc的配置</li>
</ul>


<pre><code class="plain localrc"># Misc
ADMIN_PASSWORD=sysadmin
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=$ADMIN_PASSWORD

# Target Path
DEST=/opt/stack.kilo

# Enable Logging
LOGFILE=$DEST/logs/stack.sh.log
VERBOSE=True
LOG_COLOR=True
SCREEN_LOGDIR=$DEST/logs

# Nova
enable_service n-novnc n-cauth

# Neutron
disable_service n-net
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron
ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas

# Ceilometer
enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api
enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator

# Enable Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Trove
enable_service trove tr-api tr-tmgr tr-cond

# Sahara
enable_service sahara

#FIXED_RANGE=10.0.0.0/24
HOST_IP=200.21.1.61
FLOATING_RANGE=200.21.50.1/24
PUBLIC_NETWORK_GATEWAY=200.21.50.2
Q_FLOATING_ALLOCATION_POOL=start=200.21.50.100,end=200.21.50.150
</code></pre>

<ul>
<li><p>确认br-ex是否存在
<code>plain sudo ovs-vsctl show
Bridge br-ex
  Port br-ex
      Interface br-ex
          type: internal
  Port "qg-7ec5be02-69"
      Interface "qg-7ec5be02-69"
          type: internal
ovs_version: "2.0.2"
</code></p></li>
<li><p>将eth1作为br-ex的接口
<code>bash
sudo ovs-vsctl add-port br-ex eth1
</code></p></li>
</ul>


<pre><code class="plain sudo ovs-vsctl show">Bridge br-ex
    Port br-ex
        Interface br-ex
            type: internal
    Port "qg-7ec5be02-69"
        Interface "qg-7ec5be02-69"
            type: internal
    Port "eth1"
        Interface "eth1"
ovs_version: "2.0.2"
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack Kilo版本新功能分析]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/04/what-is-new-in-kilo/"/>
    <updated>2015-05-04T10:37:22+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/04/what-is-new-in-kilo</id>
    <content type="html"><![CDATA[<p>OpenStack Kilo版本已经于2015年4月30日正式Release，这是OpenStack第11个版本，距离OpenStack项目推出已经整整过去了5年多的时间。在这个阶段OpenStack得到不断的增强，同时OpenStack社区也成为即Linux之后的第二大开源社区，参与的人数、厂商众多，也成就了OpenStack今天盛世的局面。虽然OpenStack在今年经历了Nebula的倒闭，但是随着国内的传统行业用户对OpenStack越来越重视，我们坚信OpenStack明天会更好。</p>

<p>OpenStack Kilo版本的完整翻译版本可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans">https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans</a></p>

<p>OpenStack Kilo版本的翻译工作由我和我的同事裴莹莹(Wendy)共同完成，翻译校对工作由裴莹莹完成。如果翻译有任何问题，请各位多多指正。</p>

<!-- more -->


<h2>社区贡献分析</h2>

<p>我们先来看一下OpenStack在最近的4个稳定版本发布中，每一个项目的贡献情况：</p>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-contribution-by-modules.jpg"></p>

<p>我们能够很明显的发现，OpenStack最早的几大核心模块(Nova, Cinder, Glance, Keystone, Horizon, Swift)的代码贡献所占比例呈明显下降趋势，这里强调一下，是比例而不是数量，从数量上来看，版本之间相差并不大，以Nova为例，从Havana版本的24%下降到如今的10%。这从一个侧面反映了OpenStack的核心模块日趋稳定，更多的关注集中到更高层次或者功能优化上。</p>

<p>Neutron模块则一直处于稳中有升的状态,从Havana版本的7%上升到10%，说明Neutron仍然处于需要进一步完善的状态。</p>

<p>对于Ceilometer，Heat，Sahara，Ironic, Trove等新晋的核心模块，都处于稳步增长的阶段。贡献的比例在四个版本中基本保持持平的态势。在Kilo版本中，Sahara和Heat进入了前十名。</p>

<p>从Kilo版本的比例来看，Others的比例过半，Others主要包括了OpenStack测试相关项目，例如Rally；开发相关项目，例如Devstack;以及一些新的模块，例如：Manila，Magnum等众多进入孵化器的项目;还包括所有的Client以及Spec等。可以预见，OpenStack的开发重心逐步从底层的核心模块，逐步向更高层次、提供更丰富功能的方向发展。</p>

<h2>国内社区贡献分析</h2>

<p><img class="center" src="/images/blogs/what-is-new-in-kilo-contributor.png"></p>

<p>从企业贡献排名来看，几大巨头企业牢牢占据贡献榜的前几名，OpenStack最成功的公司-Mirantis排名紧追Redhat成为第二贡献大户。排名前几位的公司还包括：IBM, Rackspace, Cisco, Suse, VMware, Intel等。</p>

<p>国内方面，华为继续稳定在第13名，但Review的数量从Juno版本的1353提升到2548个，贡献的项目几乎涵盖所有的项目，主要贡献来自Heat，Ceilometer, Horizon，Neutron, Nova等项目。</p>

<p>国内排名第2的贡献企业是九州云，排名达到了21位，看来龚永生的到来为九州云添加了无限活力。九州云的贡献主要来自Horizon和Neutron两个项目，龚永生不愧为Neutron的Core，在网络方面的贡献，九州云的确很给力。</p>

<p>排名第3的企业是海云捷迅，排名为44位，海云是国内比较早的一批OpenStack创业企业，贡献方面以Sahara，Neutron，Nova，oslo.messaging以及Cinder为主，从之前了解的情况来看，海云的项目不少，可能提交的修改是与在实际项目中遇到的问题有关。</p>

<p>排名之后的企业还有Kylin Cloud，UnitedStack，EasyStack等。由于是手工统计，在统计过程中如有遗漏，希望大家多多指正。</p>

<h2>Horizon新功能</h2>

<p>Horizon在K版本除了增强了对新增模块的支持，从UE的角度也为我们带来了很多新功能</p>

<ul>
<li>支持向导式的创建虚拟机，现在还处于beta版本，如果想在Horizon里激活，可以通过设置local_setting.py的配置实现：</li>
</ul>


<pre><code class="plain local_setting.py">LAUNCH_INSTANCE_NG_ENABLED = True
</code></pre>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-instance-guide1.png"></p>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-instance-guide2.png"></p>

<ul>
<li>支持简单的主题，主要通过修改<em>variables.scss和</em>style.scss完成对主题颜色和简单样式的修改，但是格局不能改变，修改local_settings.py</li>
</ul>


<pre><code class="plain local_setting.py">CUSTOM_THEME_PATH = 'static/themes/blue'
</code></pre>

<pre><code class="plain static/themes/blue/_variables.scss">$gray:                   #2751DB !default;
$gray-darker:            #94A5F2 !default;
$gray-dark:              #0C0CED !default;
$gray-light:             #C7CFF2 !default;
$gray-lighter:           #DCE1F5 !default;

$brand-primary:         #375A7F !default;
$brand-success:         #00bc8c !default;
$brand-info:            #34DB98 !default;
$brand-warning:         #F39C12 !default;
$brand-danger:          #E74C3C !default;
</code></pre>

<pre><code class="plain static/themes/blue/_style.scss">// Blue
// ----

@mixin btn-shadow($color) {
  @include gradient-vertical-three-colors(lighten($color, 3%), $color, 6%, darken($color, 3%));
  filter: none;
  border: 1px solid darken($color, 10%);
}

// Buttons ====================================================================

.btn-default,
.btn-default:hover {
  @include btn-shadow($btn-default-bg);
}

.btn-primary,
.btn-primary:hover {
  @include btn-shadow($btn-primary-bg);
}
</code></pre>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-horizon-theme1.png"></p>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-horizon-theme2.png"></p>

<h2>Nova新功能</h2>

<h3>Nova Scheduler</h3>

<ul>
<li>标准化了conductor，compute与scheduler的接口，为之后的接口分离做好准备，对于部分直接访问nova数据库的filters进行了优化，不再允许直接访问，参考链接：<a href="https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst">https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst</a></li>
<li>对Scheduler做了一些优化，例如：Scheduler对于每一个请求都会重新进行Filters/Weighers，为了优化这个问题，将filter/weighter的初始化从handler移到scheduler，这样每次请求的时候都可以重新使用了。</li>
</ul>


<h3>Libvirt NFV相关功能</h3>

<ul>
<li>NUMA(Non Uniform Memory Architecture)，在这个架构下，每个处理器都会访问“本地”的内存池，从而在CPU和存储之间有更小的延迟和更大的带宽。</li>
<li>在Kilo版本中针对此功能的实现包括：基于NUMA的调度的实现；可以将vCPU绑定在物理CPU上；超大页的支持。以上提到的三点都是通过Flavor的Extra Spces完成定义的。</li>
</ul>


<h3>EC2 API</h3>

<ul>
<li>EC2 API被从Nova中踢出去了</li>
<li>取而代之的是在stackforge的EC2 API转换服务</li>
</ul>


<h3>API Microversioning</h3>

<p>先来解释一下为什么需要API的微版本：主要原因在于现在这种API扩展方式，对于API实现的代码的增加或减少管理非常不方便，容易导致不一致性。引入微版本主要目的就是让开发人员在修改API代码时能够向前兼容，而不是加入一个新的API扩展；用户通过指定API的版本，在请求时也能决定是使用的具体的动作。</p>

<p>包含版本的返回:
<code>plain Result
GET /
{
     "versions": [
        {
            "id": "v2.1",
            "links": [
                  {
                    "href": "http://localhost:8774/v2/",
                    "rel": "self"
                }
            ],
            "status": "CURRENT",
            "version": "5.2"
            "min_version": "2.1"
        },
   ]
}
</code></p>

<p>客户端的Header信息：
<code>plain Header
X-OpenStack-Nova-API-Version: 2.114
</code></p>

<h3>一个已知的问题：Evacuate</h3>

<p>这个问题的产生主要是因为Evacuate的清理机制，主机名的变化会导致nova-compute重启过程中误删所有虚拟机，所以一个变通的方法是设置</p>

<pre><code class="plain nova.conf">destroy_after_evacuate=False
</code></pre>

<p>这个问题会在Liberty中得到修复，相关的Spec：<a href="https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst">https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst</a></p>

<h2>Glance新功能</h2>

<ul>
<li>自动进行镜像格式转化，例如，Ceph是使用RAW格式的，假如我们上传的是QCOW2，创建虚拟机时，就会经历一番上传下载的过程，速度异常缓慢。而且RAW格式通常都是原始大小，上传时候非常慢，完全可以通过上传小镜像自动转换为指定格式。</li>
<li>Glance支持多字段排序
<code>plain API
/images?sort_key=status&amp;sort_dir=asc&amp;sort_key=name&amp;sort_dir=asc&amp;sort_key=created_at&amp;sort_dir=desc
</code></li>
<li>临时将镜像设置为非活跃状态，假如一个镜像里有病毒，管理员就会将该镜像设置为非活跃状态，在清理后重新发布该镜像，在这个过程中，所有非管理员用户都无法使用或者下载这个镜像</li>
<li>免重启动态加载配置文件，配置文件改动后重启服务，现在可以给glance服务发送SIGHUP触发，这样升级就可以零当机时间。</li>
<li>使用多个Swift容器存储镜像，减少大规模部署时的请求瓶颈</li>
</ul>


<h2>Cinder新功能</h2>

<ul>
<li>实现服务逻辑代码与数据库结构之间的解耦，支持Rolling更新</li>
<li>一致性组是指具备公共操作的卷，逻辑上化为一组。在K版本中对增强一致性组的功能：可以添加、删除卷，从已经存在的快照创建新的组，关于一致性组的详细操作可以参考：<a href="http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html">http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html</a></li>
</ul>


<pre><code class="plain cinder">cinder consisgroup-update
[--name NAME]
[--description DESCRIPTION]
[--add-volumes UUID1,UUID2,......]
[--remove-volumes UUID3,UUID4,......]
CG
</code></pre>

<pre><code class="plain cinder">cinder consisgroup-create-from-src
[--cgsnapshot CGSNAPSHOT]
[--name NAME]
[--description DESCRIPTION]
</code></pre>

<ul>
<li>卷类型的增强功能主要包含两个：为某一项目创建私有的卷类型和为卷类型增加描述信息</li>
</ul>


<pre><code class="plain cinder">cinder type-create &lt;name&gt; --is-public
cinder type-create &lt;name&gt; &lt;description&gt;
</code></pre>

<h2>Neutron新功能</h2>

<ul>
<li>DVR支持OVS中的VLANs</li>
<li>新的V2版本的LBaas的API</li>
<li>新的插件的更新，详情请见更新日志中</li>
<li>一些高级服务的分离，例如：L3, ML2, VPNaaS, LBaaS</li>
</ul>


<p>网络方面我不是权威，希望有高人能出来讲讲Kilo中的Neutron新功能。</p>

<h2>Keystone新功能</h2>

<ul>
<li>项目嵌套，创建一个新的Project时候，可以指定parent的Project</li>
</ul>


<pre><code class="plain keystone">POST /projects

{
    "project": {
        "description": "Project space for Test Group",
        "domain_id": "1789d1",
        "enabled": true,
        "name": "Test Group",
        "parent_id": "7fa612"
    }
}
</code></pre>

<ul>
<li>Keystone与Keystone的联盟，有了这个功能两个或者更多的云服务提供者就可以共享资源，这个功能在J版本引入，在K版本中主要针对该功能的进一步增强，具体的使用方法可参考这篇博文：<a href="http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/">http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/</a></li>
<li>针对新人授权的一些增强功能</li>
<li>keystone的配置中有部分配置发生了变化，例如：keystone.token.backends.memcache被keystone.token.persistence.backends.memcache取代，更多详细内容请参考更新日志</li>
</ul>


<h2>Swift新功能</h2>

<ul>
<li>纠删码的加入应该是这个版本最大的亮点，但是纠删码作为beta版本发布，并不推荐应用于生产环境，关于纠删码的详细介绍可以参考：<a href="http://docs.openstack.org/developer/swift/overview_erasure_code.html">http://docs.openstack.org/developer/swift/overview_erasure_code.html</a></li>
<li>复合型令牌，简而言之就是需要用户加上服务的Token才能对Swfit存放的内容进行操作，如下图所示：</li>
</ul>


<pre><code class="plain swift">client
   \
    \   &lt;request&gt;: &lt;path-specific-to-the-service&gt;
     \  x-auth-token: &lt;user-token&gt;
      \
    SERVICE
       \
        \    PUT: /v1/SERVICE_1234/&lt;container&gt;/&lt;object&gt;
         \   x-auth-token: &lt;user-token&gt;
          \  x-service-token: &lt;service-token&gt;
           \
          Swift
</code></pre>

<p>具体的设计文档：<a href="http://docs.openstack.org/developer/swift/overview_backing_store.html">http://docs.openstack.org/developer/swift/overview_backing_store.html</a></p>

<ul>
<li>全局性的集群复制优化，大幅提高效率，避免经过广域网传播的数据</li>
</ul>


<h2>Ceilometer新功能</h2>

<ul>
<li>支持Ceph对象存储监控，当对象存储为Ceph而不是Swfit的时候，使用Polling机制，使用Ceph的Rados Gateway的API接口获取数据，具体的设计文档：<a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst</a></li>
<li>Ceilometer API RBAC - 更细粒度的权限控制: <a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst</a>
<code>plain Ceilometer
{
  "context_is_admin": [["role:admin"]]
}
</code>
更细粒度的控制
<code>plain Ceilometer
{
   "context_is_admin": [["role:admin"]],
   "admin_or_cloud_admin": [["rule:context_is_admin"],
            ["rule:admin_and_matching_project_domain_id"]],
   "telemetry:alarm_delete": [["rule:admin_or_cloud_admin"]]
}
</code></li>
<li>接口中的模糊查询，增加了一个新的查询符号=~</li>
<li>支持更多的测量，包括Hyper-V，IPMI相关的</li>
</ul>


<h2>Ironic新功能</h2>

<ul>
<li>iLO的优化</li>
<li>使用Config Drive替代Metadata服务</li>
<li>全盘镜像支持，可以跳过raddisk和kernel，这样就可以部署Windows的镜像了</li>
<li>使用本地盘启动，替代PXE方式，可以通过设置flavor的capabilities:boot_option实现</li>
</ul>


<h2>Oslo</h2>

<p>解决了很多之前遗留的技术债，还有一些命名规范的问题。olso.messaging实现了心跳，olso.log在所有项目中使用，优化了oslo.db的代码。</p>

<h2>OpenStack文档</h2>

<p>优化了docs.openstack.org页面，也可以从中选择相应的语言。有专门的团队负责安装、网络和高可靠的文档。</p>

<h2>其他模块</h2>

<p>对于Sahara, Heat, Trove等模块的更新没有在这里Highlight出来，大家可以参考更新日志里的内容，或者查看specs中的具体描述。</p>

<h2>总结</h2>

<p>通过Kilo的一些更新可以看到，Kilo版本在不断优化代码结构的基础上，增加了一些新功能，也偿还了一些技术债，总体来说是一种稳中有升的态势，但是总体感觉并没有太多的惊喜和出人意料。相信随着更多的孵化项目进入正式版本中，OpenStack一定会向更多元化的方向发展。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Havana)将OpenStack Havana源代码编译为DEB包]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/05/build-openstack-source-code-to-deb-package/"/>
    <updated>2015-03-05T21:57:16+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/05/build-openstack-source-code-to-deb-package</id>
    <content type="html"><![CDATA[<h2>Why</h2>

<p>我想有以下有几个原因促使我写这篇Blog：</p>

<ul>
<li>很多人开始OpenStack之旅是从Ubuntu开始，但是却没有一篇文章系统的介绍如何将修改后的代码重新编译回DEB包。</li>
<li>如果我们采用源代码直接安装的方式对OpenStack模块进行管理，一致性很难保证，难以维护。</li>
<li>Debian类系统的打包看起来比RPM包复杂很多。</li>
</ul>


<h2>Who</h2>

<p>谁需要看这篇文章呢？</p>

<ul>
<li>不了解如何编译DEB包</li>
<li>想把修改过的OpenStack源代码重新发布，供内部使用</li>
<li>希望改变直接维护源代码</li>
</ul>


<p>当然，如果您已经是这方面的高手，欢迎给我指正我Blog中的不足，十分感谢。</p>

<!-- more -->


<h2>Quick Start</h2>

<p>我已经将整个的编译过程集成在Vagrant脚本中，你可以直接安装Vagrant后，下载我的源代码，启动后就能看到整个的编译过程。</p>

<p>Vagrant 版本要求为1.3.5，Virtualbox版本要求为4.1或者4.2均可。</p>

<h2>Let&rsquo;s play some magic</h2>

<pre><code>git clone https://github.com/xiaoquqi/vagrant-build-openstack-deb
cd vagrant-build-openstack-deb
vagrant up
</code></pre>

<p>虚拟机启动后，将会自动从github(这里使用的是csdn code的镜像代码)同步最新代码，然后使用编译脚本，执行打包操作。如果不考虑下载的时间，整个过程大概持续5分钟左右的时间，编译好的Deb包将会存放在/root/build目录下。</p>

<pre><code>vagrant ssh
</code></pre>

<p>即可登陆到虚拟机，切换到root目录就可以查看到所有打包好的DEB的情况了，当然你也可以直接使用dpkg -i命令进行安装。</p>

<pre><code>sudo -s
cd /root/build
ls -lrt *.deb
dpkg -i python-glance_2013.2.2.dev1.g5cd7a22~precise-0ubuntu1_all.deb
</code></pre>

<h2>Step by Step</h2>

<p>看过了整个的编译过程，下面来介绍一点点细节。</p>

<p>全部的编译部分代码都在这个文件中：<a href="https://github.com/xiaoquqi/vagrant-build-openstack-deb/blob/master/scripts/build.sh">https://github.com/xiaoquqi/vagrant-build-openstack-deb/blob/master/scripts/build.sh</a></p>

<p>下面让我们来仔细分析一下整个编译过程。</p>

<ul>
<li>添加必要的源</li>
</ul>


<p>这里面我们用的源包含sohu的Ubuntu 12.04源以及Ubuntu的Havana源</p>

<pre><code>deb http://mirrors.sohu.com/ubuntu/ precise main restricted
deb http://mirrors.sohu.com/ubuntu/ precise-updates main restricted
deb http://mirrors.sohu.com/ubuntu/ precise universe
deb http://mirrors.sohu.com/ubuntu/ precise-updates universe
deb http://mirrors.sohu.com/ubuntu/ precise multiverse
deb http://mirrors.sohu.com/ubuntu/ precise-updates multiverse
deb http://mirrors.sohu.com/ubuntu/ precise-backports main restricted universe multiverse
deb http://mirrors.sohu.com/ubuntu/ precise-security main restricted
deb http://mirrors.sohu.com/ubuntu/ precise-security universe
deb http://mirrors.sohu.com/ubuntu/ precise-security multiverse
deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/havana main
</code></pre>

<ul>
<li>安装必要的编译软件</li>
</ul>


<pre><code>apt-get install -y debootstrap equivs schroot
apt-get install -y devscripts
apt-get install -y build-essential checkinstall sbuild
apt-get install -y dh-make
apt-get install -y bzr bzr-builddeb
apt-get install -y git
apt-get install -y python-setuptools
</code></pre>

<ul>
<li>编译脚本的源代码仓库</li>
</ul>


<p>Ubuntu维护源代码编译脚本是使用叫做bzr的工具，常使用Launchpad的朋友应该比较熟悉，这是一套类似于git的分布式管理工具，不同的是这是一套完全用python语言实现的管理工具，不仅具有代码版本控制功能并且与Launchpad高度整合，作为Ubuntu维护不可缺少的重要工具之一。例如，这里面用到的glance编译脚本就可以在这里找到：</p>

<p><a href="https://code.launchpad.net/~ubuntu-server-dev/glance/havana">https://code.launchpad.net/~ubuntu-server-dev/glance/havana</a></p>

<p>页面上方有下载代码的方式：</p>

<pre><code>bzr branch lp:~ubuntu-server-dev/glance/havana
git clone https://code.csdn.net/openstack/glance.git --branch "stable/havana" glance_source
</code></pre>

<ul>
<li>准备环境</li>
</ul>


<p>在Vagrant启动一台新虚拟机之后，并没有pip，如果不安装pip，则会在python setup.py sdist过程中，把pip安装到源代码目录中，引起Build失败。将//vagrant/pip/pip-1.4.1.tar.gz解压缩并安装，之后安装pbr：</p>

<pre><code>tar zxvf pip-1.4.1.tar.gz
cd pip-1.4.1
sudo python setup.py install
sudo pip install pbr
</code></pre>

<ul>
<li>生成source文件</li>
</ul>


<p>进入glance_source目录，执行</p>

<pre><code>python setup.py sdist
</code></pre>

<p>生成的tar.gz文件会在glance_source/dist下，注意此时该文件的名称为：</p>

<pre><code>glance-2013.2.2.dev1.g5cd7a22.tar.gz
</code></pre>

<p>接下来我们需要将该文件重命名为：</p>

<pre><code>glance_2013.2.2.dev1.g5cd7a22~precise.orig.tar.gz
</code></pre>

<p>特别注意：glance后面已经变为下划线！！！</p>

<p>把文件移动到与glance和glance_source同一级别的目录，这样在编译的时候，才能找到source文件。此时的目录结构为：</p>

<pre><code>├── glance
│   ├── debian
├── glance_source
├── glance_2013.2.2.dev1.g5cd7a22~precise.orig.tar.gz
</code></pre>

<ul>
<li>安装依赖包</li>
</ul>


<p>为了保证顺利的完成编译，我们需要安装要编译包的所有依赖包，简单来说就是glance/debian/control文件中定义的Depends部分的内容。当然在编译的时候我们也可以完全忽略依赖，但是并不推荐。</p>

<pre><code>mk-build-deps -i -t 'apt-get -y' debian/control
</code></pre>

<p>这样系统就会自动安装所有依赖包，并且生成一个glance-build-deps_1.0_all.deb文件。</p>

<ul>
<li>生成日志信息</li>
</ul>


<p>开始编译前，我们还需要告诉编译器我们要编译的版本，还记得刚才生成的dist包吗，把那个版本拿出来作为我们commit的版本。</p>

<pre><code>cd glance
dch -b -D precise --newversion "1:2013.2.2.dev1.g5cd7a22~precise-0ubuntu1" 'This is a build test.'
debcommit
</code></pre>

<p>这样在glance/debian/changelog中就会增加一条新的日志。</p>

<ul>
<li>开始编译</li>
</ul>


<p>万事俱备，只欠东风。我们利用bzr提供的builddeb开始编译，这里我们忽略签名问题。</p>

<pre><code>cd glance
bzr builddeb -- -sa -us -uc
</code></pre>

<p>大功告成啦。快去/root/build/glance下看看你的deb包吧。</p>

<h2>总结</h2>

<p>Debian包的编译的确涉及很多知识点，而且可使用的编译工具很多，关系很复杂。这篇博文，只为了帮助大家对DEB包的编译有一个快速的认识，如果想了解更多关于编译的知识，请关注后续的博文。</p>

<p>最后，我们仍然希望有更多的热爱OpenStack的朋友们加入我们公司，如果有意向的请与我联系</p>

<ul>
<li>邮箱：<a href="&#x6d;&#97;&#105;&#x6c;&#x74;&#111;&#x3a;&#120;&#x69;&#x61;&#111;&#x71;&#117;&#x71;&#x69;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#x6f;&#x6d;">&#120;&#105;&#97;&#111;&#113;&#117;&#113;&#105;&#64;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#x63;&#111;&#109;</a></li>
<li>新浪微博：@RaySun(<a href="http://weibo.com/xiaoquqi">http://weibo.com/xiaoquqi</a>)</li>
</ul>

]]></content>
  </entry>
  
</feed>
