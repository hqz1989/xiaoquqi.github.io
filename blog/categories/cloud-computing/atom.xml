<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud Computing | RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/blog/categories/cloud-computing/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2015-10-30T18:16:35+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[深度解读OpenStack Liberty国内代码贡献]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/"/>
    <updated>2015-10-29T18:56:06+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty</id>
    <content type="html"><![CDATA[<p>又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。</p>

<p>OpenStack Liberty完整版本的翻译可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans">https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans</a></p>

<p>本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。</p>

<!-- more -->


<h2>社区贡献分析</h2>

<p>本次统计，并没有采用Review的数量为依据，而直接采用commits的方式，也就是代码实际merge入库的数量。</p>

<p>我们仍然要先看一下模块的贡献情况：</p>

<p><img class="left" src="/images/blogs/contribution-in-liberty-contribution-by-modules.png" width="400"></p>

<p>与之前Release的特点相似，OpenStack早期的核心模块Nova, Keystone代码commits数量出现明显下滑状态，而Neutron, Heat, Trove, Ceilometer, Cinder等模块都保持着稳中有升的态势。值得关注的是，在排名前20名的项目中，出现了两个直接与Docker有关的项目Kolla和Magnum，一个与docker间接有关的项目Murano。可以预见，OpenStack下一步发展的热点就是在与Docker之间的勾勾搭搭。</p>

<p>特别需要注意的是，在stackalytics.com统计的模块中，在Kilo中是259个，而到了Liberty到了389个，当然有一些项目并非完全是OpenStack的项目，但是也从一个侧面反映出OpenStack以及周边项目的蓬勃发展。</p>

<p>从更新日志中我们也能看到，本次Release的正式项目中，变动较大的是Neutron和Heat两个模块。在经历不断锤炼后，Neutron逐渐走向成熟，但是从生产级别角度看，Neutron的确还有很长的路要走。</p>

<h2>国内社区贡献分析</h2>

<p><img class="center" src="/images/blogs/contribution-in-liberty-contributor.png" width="400"></p>

<p>从全球企业的贡献排名来看，排名状况基本变化不大，仍然是HP, Redhat, Mirantis, IBM, Rackspace, Intel, Cisco，但是非常欣喜的，国内的IT的航空母舰华为已经成功杀入前十名，这无疑是振奋人心的事情，希望华为未来能多一些对OpenStack社区的主导力，提高中国在OpenStack社区的地位，当然最好也能扶植一下国内的OpenStack创业公司，实现共同发展、共同进步。华为的主要代码贡献集中在dragonflow，magnum，heat等模块，特别是在dragonflow上，几乎全部是华为贡献的，magnum上也将近有五分之一的代码。</p>

<p><strong><em>华为社区贡献统计</em></strong></p>

<p><img class="center" src="/images/blogs/contribution-in-liberty-huawei.png" width="800"></p>

<p>记得在OpenStack五周年的庆祝活动上，Intel的陈绪博士说过，国内OpenStack贡献企业，就是一朵大云，四朵小云，下面让我们来看看这几朵小云在这个版本的表现。</p>

<p><strong><em> 99cloud社区贡献统计</em></strong></p>

<p><img class="center" src="/images/blogs/contribution-in-liberty-99cloud.png" width="800"></p>

<p>排名第16位的是99cloud，99cloud自上一个版本排名四朵小云之首后，本次继续强劲来袭，排名创造历史新高，第16名。通过对贡献模块的分析，我们能看出99cloud最大的贡献来自于社区文档，而在项目方面的贡献则主要来自murano-dashboard，horizon，neutron等项目上，从中可以看出99cloud对murano这个applicaton catalog的项目关注程度比较高，可能会在将来的产品中有所体现。从贡献中，我们隐约看到了九州云的副总裁李开总的提交，由此可见九州云为社区贡献的积极程度。
更加难能可贵的是，Horizon的全球贡献99cloud是全球前十，Tempest全球前八，Murano项目更是进入全球前三，相当给力。</p>

<p><strong><em> UnitedStack社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-unitedstack.png" width="800"></p>

<p>排在第30位的是UnitedStack，经过了上一个版本的短暂沉寂后，这个版本卷土重来，杀回前30。从代码贡献来看，UnitedStack的主要贡献来自python-openstackclient以及部署用到的puppet相关代码，当然对neutron、trove、kolla、heat等也有一定数量的贡献。</p>

<p><strong><em> Kylin Cloud社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-kylincloud.png" width="800"></p>

<p>排名第38位的是麒麟云，其实麒麟云每次Release中总是有她的身影，但好像总是被忽略的。麒麟云最大的贡献来自Horizon项目，其他模块也有一定数量的贡献。总之，我们想到OpenStack企业的时候，的确应该时常提起麒麟云。</p>

<p><strong><em> Easystack社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-easystack.png" width="800"></p>

<p>排名第70位的是Easystack，Easystack也属于OpenStack早期创业的公司，对于OpenStack的贡献也是持续的。Easystack最大的贡献来自nova，虽然数量不是很多，但是在国内企业里应该算名列前茅的啦。Easystack对Nova的贡献主要来自对libvirt层的bug修复。</p>

<p><strong><em> Awcloud社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-awcloud.png" width="800"></p>

<p>排名第75位的是海云捷迅，海云应该算是在国内发展比较迅猛的一家OpenStack早期创业公司。他们的贡献主要来自Neutron相关的项目，看起来应该是为了解决项目中出现的实际问题所做的努力。海云的马力应该是公司内部贡献排名第一的，尤其是前一段时间发布的两篇关于&#8221;Neutron &amp; OpenStack漫谈&#8221;，非常值得一读。</p>

<p><strong><em> LeTV社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-letv.png" width="800"></p>

<p><strong><em> Netease社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-netease.png" width="800"></p>

<p>排名第94和95位的分别是两家互联网企业，乐视和网易，乐视是最近互联网中使用OpenStack动静最大的一家了，应该能在大规模应用中发现OpenStack很多问题吧。</p>

<p><strong><em> Huron社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-huron.png" width="800"></p>

<p>排名第122位的是我的公司——北京休伦科技有限公司，其实我们公司也算是国内最早一批从事OpenStack创业的公司，z早在2013年的时候就已经开始投入OpenStack私有云产品相关的研发。我们贡献的代码主要来自Nova和Murano两个模块中，都是我们在开发和项目使用中发现的问题，修复后回馈给社区的，我也希望我们能在下一个版本Release中贡献更多的力量。</p>

<p><strong><em> China Mobile社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-chinamobile.png" width="800"></p>

<p>排名第133位的是中国移动，之前并没有在哪一个排名上看到过中国移动在OpenStack贡献，我也是第一次发现。中国移动应该算是国内运营商领域技术实力较强的一家，也是运营商里开始从事OpenStack预研较早的一家。中国移动有大量的IT资源和设备，理应像AT&amp;T一样在OpenStack领域大有所为。纵观中国移动的社区贡献，主要来自Neutron和Ceilometer两个项目，几个Bug修复都是与Volume相关。</p>

<p><strong><em> Lenovo社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-lenovo.png" width="800"></p>

<p>排名第135位的是联想。不评论了。</p>

<p>排名第139位的是清华大学医学院附属医院，这个有点意思。但是stackalytics.com有Bug，他们的具体统计显示不出来。</p>

<p><strong><em> H3C社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-h3c.png" width="800"></p>

<p>排名第143位的是H3C。贡献是Nova中的关于VMware的Bug Fix。</p>

<p>由于stackalytics并没有按照区域统计的功能，所以本次统计完全是全自动统计(全靠我自己手动)，所以难免遗漏了为OpenStack贡献的国内企业，如果发生该情况请及时告知。</p>

<h2>社区贡献内容分析</h2>

<p><img class="center" src="/images/blogs/contribution-in-liberty-complete-blueprints.png" width="800"></p>

<p>从贡献的commits的类型来区分，国内贡献出的代码主要还是以bug为主，这可能也与我们使用的都是OpenStack较成熟的模块有关，本身这些模块成熟程度较高，所以想做blueprint很难。另外一个很重要的原因是和OpenStack管理流程有关的，现在像Nova, Cinder等项目都是需要先Review Specs的，其实就是所谓的设计文档，语言成为国内很多工程师贡献的最大障碍，所以这也导致了Blueprint的贡献度在国内并不高。</p>

<p><strong><em> Huawei社区贡献——完成Blueprint </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-blueprint-huawei.png" width="800"></p>

<p>纵观整个Blueprint的完成统计情况，华为作为国内最有实力的企业，高居全球第五名，完成最多的模块为cinder和mistral。</p>

<p>之后能完成Blueprint的企业还包括UnitedStack、中国移动、麒麟云、海云捷迅和九州云，但是相比来说数量较少，都是个位数字。</p>

<p>OpenStack在国内发展已经超过了四年的时间，但是遗憾的一点，尽管我们拥有世界上最多的开发人员，但是我们对社区仍然没有话语权，国内的用户的需求无法对社区上游形成影响，导致很多本地化定制的需求无法真正的在社区版本代码得到体现。所以如何让中国的声音出现在社区，是我们所有OpenStack人需要思考的问题。欣喜的一点，本土的巨头华为已经身先士卒，投入很大的力量搞OpenStack的社区贡献，我们更希望越来越多的国内传统IT巨头能够意识到这个问题，投身于开源的事业中，否则我们又在起跑线上输给了别人。</p>

<p>以上仅代表个人观点，如有任何异议，欢迎批评指正。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Kilo)Devstack完全用户手册]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/09/03/devstack-guide/"/>
    <updated>2015-09-03T18:34:20+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/09/03/devstack-guide</id>
    <content type="html"><![CDATA[<p>Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。</p>

<p>本篇Blog主要介绍以下几个实用场景：</p>

<ul>
<li>如何利用Devstack构建一套完美的开发环境</li>
<li>提高Devstack安装成功率的方法</li>
<li>Devstack的实用技巧</li>
<li>各种场景下的配置和注意事项</li>
</ul>


<p>本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。</p>

<!-- more -->


<h2>运行环境的选择</h2>

<p>对于刚刚接触OpenStack的开发者而言，没有太多闲置的资源，所以比较容易的上手方式就是使用虚拟机。对于桌面的虚拟机软件来说，主流的软件无外乎VMWare Workstation和Oracle Virtualbox，对于OpenStack开发而言，二者并无太大差异。以下几点可能会作为选择的主要依据：</p>

<ul>
<li>VMWare Workstation是收费软件，Virtualbox是免费软件</li>
<li>VMWare Workstation支持nested virtualization，就是安装完的devstack virt type是kvm，节省资源，Virtualbox安装以后只能使用qemu，虽然在Virtualbox 5以上版本号称支持，但是实际验证中仍然不能生效，还在研究中</li>
<li>VMWare Workstation使用NAT方式时，内部的IP可以在HOST主机直接访问到，Virtualbox还需要端口转发，所以建议单独增加一块Host-only的Apdaptor便于调试</li>
<li>使用Virtualbox时，为了让虚拟机能够访问外部网络，并且允许Host通过Floating IP对虚拟机进行访问，需要在Host层面设置NAT规则，转换到可以访问的物理网卡上，详情请见下文</li>
</ul>


<h2>Virtualbox网络设置</h2>

<p><img class="center" src="/images/blogs/devstack-guide-network-topology.jpg"></p>

<ul>
<li>Nova Network网卡配置</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet static
address 192.168.56.101
netmask 255.255.255.0

auto eth2
iface eth1 inet static
address 172.16.0.101
netmask 255.255.255.0
</code></pre>

<ul>
<li>Neutron网卡配置</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet static
address 192.168.56.101
netmask 255.255.255.0

auto eth2
iface eth2 inet manual
up ip link set dev $IFACE up
down ip link set dev $IFACE down
</code></pre>

<ul>
<li>MAC网卡NAT映射</li>
</ul>


<p>我们将第三块网卡作为提供外部网络的接口，采用系统层面的NAT方式让该网卡能够访问外部网络。</p>

<pre><code class="plain bash">sudo sysctl net.inet.ip.forwarding=1
</code></pre>

<p>在nat-anchor后面添加</p>

<pre><code class="plain /etc/pf.conf">nat on en0 from 172.16.0.0/24 -&gt; (en0)
</code></pre>

<p>之后加载</p>

<pre><code class="bash bash">sudo pfctl -e -f /etc/pf.conf
</code></pre>

<ul>
<li>Linux网卡NAT映射</li>
</ul>


<pre><code class="plain bash">echo 1 &gt; /proc/sys/net/ipv4/ip_forward
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
</code></pre>

<h2>Devstack快速开始</h2>

<p>其实，Devstack本身并不需要很复杂的配置就可以成功运行，但是仍然有几个需要注意的地方：</p>

<ul>
<li>Ubuntu 14.04 64bit(LTS), 12.04已经逐渐退出历史舞台，所以这里推荐14.04</li>
<li>不能使用root用户，即使你使用root用户执行Devstack，默认也会为你建立一个stack用户，所以不如老老实实的直接使用普通用户运行Devstack，或者提前建立好stack用户，切换后再执行</li>
<li>默认获取Devstack进行安装，安装的是master分支的代码，但是在实际开发中(比如我们做产品的时候)，都是基于某个stable分支进行，所以一般情况在clone devstack的时候需要指定stable分支</li>
</ul>


<p>下面给出一个最简安装步骤：</p>

<pre><code class="plain"># adduser stack
# apt-get install sudo -y
# echo "stack ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoers
# sudo su - stack

(stack)$ git clone https://git.openstack.org/openstack-dev/devstack --branch=stable/kilo
(stack)$ cd devstack &amp;&amp; ./stack.sh
</code></pre>

<h2>提高Devstack安装成功率</h2>

<p>估计在国内使用Devstack的人基本都遇到过安装失败的状况，为了节约大家的时间，先分析一下Devstack为什么会失败，我们先从这张时序图看一下Devstack执行的过程：</p>

<p><img class="center" src="/images/blogs/devstack-guide-flow.png"></p>

<p>从上述流程图中可以很清楚的看到Devstack有以下几个地方需要访问网络：</p>

<ul>
<li>安装依赖时，需要访问Ubuntu的源</li>
<li>执行get_pip.sh时，地址是彻底被墙的，需要访问<a href="https://bootstrap.pypa.io/get-pip.py">https://bootstrap.pypa.io/get-pip.py</a></li>
<li>从github clone源代码，github在国内访问速度并不很快而且间歇性被墙</li>
<li>安装过程中执行pip install requirements，需要访问pip repo</li>
<li>下载镜像，这一步骤取决于你需要安装的模块，如果默认安装只会下载cirros镜像，但是如果是安装类似Trove的模块，可能需要下载的更多</li>
</ul>


<hr />

<p>所以综上所述，为了提高devstack的安装成功率，需要从这几个方面着手优化：</p>

<ul>
<li>使用国内源</li>
</ul>


<pre><code class="plain /etc/apt/sources.list">deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse
</code></pre>

<ul>
<li>从国内源获取get-pip.py，从源代码可以分析出，检测get-pip.py的方式，这里面有两种方式一种是手动下载get-pip.py之后，注释代码，还有一种就是修改PIP_GET_PIP_URL的地址，但是这里只能通过修改install_pip.sh的方式，暂时无法从环境变量里获取</li>
</ul>


<pre><code class="bash devstack/tools/install_pip.sh">FILES=$TOP_DIR/files

PIP_GET_PIP_URL=https://bootstrap.pypa.io/get-pip.py
LOCAL_PIP="$FILES/$(basename $PIP_GET_PIP_URL)"

function install_get_pip {
    # The OpenStack gate and others put a cached version of get-pip.py
    # for this to find, explicitly to avoid download issues.
    #
    # However, if DevStack *did* download the file, we want to check
    # for updates; people can leave their stacks around for a long
    # time and in the mean-time pip might get upgraded.
    #
    # Thus we use curl's "-z" feature to always check the modified
    # since and only download if a new version is out -- but only if
    # it seems we downloaded the file originally.
    if [[ ! -r $LOCAL_PIP || -r $LOCAL_PIP.downloaded ]]; then
        curl --retry 6 --retry-delay 5 \
            -z $LOCAL_PIP -o $LOCAL_PIP $PIP_GET_PIP_URL || \
            die $LINENO "Download of get-pip.py failed"
        touch $LOCAL_PIP.downloaded
    fi
    sudo -H -E python $LOCAL_PIP
}
</code></pre>

<p>修改为我在coding.net上缓存的get-pip脚本</p>

<pre><code class="bash devstack/tools/install_pip.sh">PIP_GET_PIP_URL=https://coding.net/u/xiaoquqi/p/pip/git/raw/master/contrib/get-pip.py
</code></pre>

<ul>
<li>国内的代码托管服务器有从github上定期同步源代码的，但是经过实际测试都不是很理想，所以可能这是最不稳定的一部分，但是可以提前使用脚本，人工的下载所有代码，之后我会尝试在我自己的源中定时同步OpenStack源代码，敬请关注</li>
<li>现在pip的安装速度明显提升，原来还需要使用国内源，例如豆瓣，现在即使不修改也能很快的进行安装</li>
<li>镜像下载建议使用一些下载工具，然后放到指定的目录中，这样最有效</li>
</ul>


<h2>无网络状况下安装Devstack</h2>

<p>因为我们是做OpenStack的产品的公司，所以就要求我们的Devstack要能够满足无网络状况下的安装，之前也写过一篇详细介绍无网络安装Devstack博客,由于时间关系，可能一些内容已经过时了，这里面再进行一下更新，思路还是上面的思路，这里给出一些使用的工具，如果不清楚如何使用的话，可以参考我之前的博客。</p>

<ul>
<li>本地源的缓存使用apt-mirror，这是一个需要时间的工作，第一次同步的时间会非常长，准备好大约100G左右的空间吧</li>
<li>缓存get-pip.py，这个比较容易，搭建一个Apache服务器，但是需要把端口修改为10000，否则在安装好OpenStack后，会占用80端口，重新执行Devstack时候会出现错误</li>
<li>建立本地的Gerrit，并且上传所有代码</li>
<li>从requirements项目中，下载所有的pip，建立本地的pip缓存源，如果是搭建研发环境，可能还需要下载test-requirements的内容和tox</li>
<li>将镜像下载到刚刚创建的Apache服务器</li>
</ul>


<p>完成以上步骤，你可以尽情断掉外网，愉快的进行Devstack的安装了，稍后我会将以上步骤进行进一步完善。</p>

<h2>OFFLINE模式下安装Devstack</h2>

<p>在Devstack中提供了一种OFFLINE的方式，这种方式的含义就是，当你第一次完成安装后，所有需要的内容已经下载到本地，再次运行就没有必要访问网络了(前提是你不想升级)，所以可以将安装模式设置为OFFLINE，避免网络的访问，方法为：</p>

<pre><code class="bash devstack/localrc">OFFLINE=True
</code></pre>

<h2>虚拟机重启后，如何利用rejoin-stack.sh，免重新安装</h2>

<p>其实使用OFFLINE模式，可以在离线状态下无数次重新运行devstack，但是如果不是为了重新配置，我们并没有需要每次重新运行stack.sh。在Devstack中提供了另外一个脚本叫做rejoin-stack.sh，原理很简单就是把所有的进程重新组合进screen，所以我们借助这个脚本完全可以不重新执行stack.sh，快速恢复环境。但是当虚拟机重启后，cinder使用的卷组并不会自动重建，所以在运行rejoin之前，需要将恢复卷组的工作，放入开机启动的脚本中。</p>

<pre><code class="bash /etc/init.d/cinder-setup-backing-file">losetup /dev/loop1 /opt/stack/data/stack-volumes-default-backing-file
losetup /dev/loop2 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file
exit 0
</code></pre>

<pre><code class="bash Run as root">chmod 755 /etc/init.d/cinder-setup-backing-file
ln -s /etc/init.d/cinder-setup-backing-file /etc/rc2.d/S10cinder-setup-backing-file
</code></pre>

<pre><code class="bash Run as normal user">cd $HOME/devstack
./rejoin-stack.sh
</code></pre>

<h2>Scenario 0: 公共部分</h2>

<pre><code class="bash devstack/localrc"># Misc
ADMIN_PASSWORD=sysadmin
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=$ADMIN_PASSWORD

# Target Path
DEST=/opt/stack.kilo

# Enable Logging
LOGFILE=$DEST/logs/stack.sh.log
VERBOSE=True
LOG_COLOR=True
SCREEN_LOGDIR=$DEST/logs
</code></pre>

<h2>Scenario 1: 单节点Nova-Network的安装</h2>

<p>这应该就是Devstack默认的模式，有以下几点需要注意：</p>

<ul>
<li>根据上面的网卡配置</li>
</ul>


<blockquote><p>第一块网卡为NAT方式，用于访问外部网络</p>

<p>第二块为Host-only Adaptor，用于访问云平台</p>

<p>第三块为Host-only Adaptor，用于虚拟机桥接网路</p>

<p>需要注意的是：这种方式下并不能让虚拟机正常访问外部网络，可以通过将eth2设置为Bridge模式，但是这样会造成DHCP冲突(如果外部网络有DHCP)，所以暂时没有完美的解决方案</p></blockquote>

<ul>
<li>打开novnc和consoleauth，否则无法访问VNC</li>
</ul>


<p>这里给出的配置方案是第一种网络配置，即虚拟机无法网络外部网络的情况</p>

<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth

FLAT_INTERFACE=eth1
# eth1 address
HOST_IP=192.168.56.101
FIXED_RANGE=172.24.17.0/24
FIXED_NETWORK_SIZE=254
FLOATING_RANGE=172.16.0.128/25
</code></pre>

<h2>Scenario 2: 双节点Nova-Network的安装</h2>

<ul>
<li>控制节点</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth
disable_service n-cpu n-net n-api-meta c-vol

# current host ip
HOST_IP=192.168.56.101
FLAT_INTERFACE=eth1
MULTI_HOST=1
</code></pre>

<ul>
<li>计算节点</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth
ENABLED_SERVICES=n-cpu,n-net,n-api-meta,c-vol

# current host ip
HOST_IP=192.168.56.101
FLAT_INTERFACE=eth1
# needed by cinder-volume service
DATABASE_TYPE=mysql

# controller ip
SERVICE_HOST=192.168.56.101
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
NOVA_VNC_ENABLED=True
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html"
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</code></pre>

<h2>Scenario 3: 单节点Neutron的安装</h2>

<ul>
<li>基本配置</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth

# Neutron
disable_service n-net
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron
ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas

HOST_IP=192.168.56.101
FIXED_RANGE=20.0.0.0/24
NETWORK_GATEWAY=20.0.0.1
FLOATING_RANGE=172.16.0.0/24
PUBLIC_NETWORK_GATEWAY=172.16.0.1
Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200
</code></pre>

<ul>
<li>OVS设置</li>
</ul>


<p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p>

<pre><code class="bash bash">sudo ip addr flush dev br-ex
</code></pre>

<p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p>

<pre><code class="bash bash">sudo ovs-vsctl add-port br-ex eth2
</code></pre>

<h2>Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)</h2>

<ul>
<li>控制/网络节点
<figure class='code'><figcaption><span>devstack/localrc</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&lt;h1&gt;Nova&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;enable_service n-novnc n-cauth
</span><span class='line'>HOST_IP=192.168.56.101
</span><span class='line'>disable_service n-cpu n-net n-api-meta c-vol&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Neutron&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;disable_service n-net
</span><span class='line'>ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta
</span><span class='line'>FIXED_RANGE=20.0.0.0/24
</span><span class='line'>NETWORK_GATEWAY=20.0.0.1
</span><span class='line'>FLOATING_RANGE=172.16.0.0/24
</span><span class='line'>PUBLIC_NETWORK_GATEWAY=172.16.0.1
</span><span class='line'>Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200</span></code></pre></td></tr></table></div></figure></p>

<ul>
<li>计算节点
<figure class='code'><figcaption><span>devstack/localrc</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&lt;h1&gt;Nova&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;disable_all_services
</span><span class='line'>ENABLED_SERVICES=n-cpu,rabbit,neutron,q-agt,c-vol&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;current host ip&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;HOST_IP=192.168.56.103&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;needed by cinder-volume service&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;DATABASE_TYPE=mysql&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;controller ip&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SERVICE_HOST=192.168.56.101
</span><span class='line'>MYSQL_HOST=$SERVICE_HOST
</span><span class='line'>RABBIT_HOST=$SERVICE_HOST
</span><span class='line'>GLANCE_HOSTPORT=$SERVICE_HOST:9292
</span><span class='line'>NOVA_VNC_ENABLED=True
</span><span class='line'>NOVNCPROXY_URL=&ldquo;&lt;a href="http://$SERVICE_HOST:6080/vnc_auto.html"&gt;http://$SERVICE_HOST:6080/vnc_auto.html&lt;/a&gt;&rdquo;
</span><span class='line'>VNCSERVER_LISTEN=$HOST_IP
</span><span class='line'>VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</span><span class='line'>Q_HOST=$SERVICE_HOST</span></code></pre></td></tr></table></div></figure></p>

<ul>
<li>OVS设置</li>
</ul>


<p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p>

<pre><code class="bash bash">sudo ip addr flush dev br-ex
</code></pre>

<p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p>

<pre><code class="bash bash">sudo ovs-vsctl add-port br-ex eth2
</code></pre>

<h2>Scenario 5: 从源代码安装客户端</h2>

<p>新的Devstack里面默认不再提供client的源代码的安装方式，需要使用localrc中的环境变量进行开启，否则将直接从master获取的client代码进行安装，当然这样会造成系统无法正常使用。那么如何才能确定client在当前Devstack可用的版本呢？最简单的方法可以先从pip中安装包，之后通过pip list | grep client的方式获取client的源代码。这里面提供我在Kilo中使用的版本依赖。</p>

<pre><code class="bash devstack/localrc">KEYSTONECLIENT_BRANCH=1.3.1
CINDERCLIENT_BRANCH=1.1.1
GLANCECLIENT_BRANCH=0.17.1
HEATCLIENT_BRANCH=0.4.0
NEUTRONCLIENT_BRANCH=2.4.0
NOVACLIENT_BRANCH=2.23.0
SWIFTCLIENT_BRANCH=2.4.0

# client code
LIBS_FROM_GIT=python-keystoneclient,python-glanceclient,python-novaclient,python-neutronclient,python-swiftclient,python-cinderclient
</code></pre>

<h2>Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift</h2>

<pre><code class="bash devstack/localrc"># Ceilometer
enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api
enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator

# Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Trove
enable_service trove tr-api tr-tmgr tr-cond

# Sahara
enable_service sahara

# Swift
enable_service s-proxy s-object s-container s-account
SWIFT_REPLICAS=1
SWIFT_HASH=011688b44136573e209e
</code></pre>

<h2>Scenario 7: 安装Ceph</h2>

<pre><code class="bash devstack/localrc"># Ceph
ENABLED_SERVICES+=,ceph
CEPH_LOOPBACK_DISK_SIZE=200G
CEPH_CONF=/etc/ceph/ceph.conf
CEPH_REPLICAS=1

# Glance - Image Service
GLANCE_CEPH_USER=glance
GLANCE_CEPH_POOL=glance-pool

# Cinder - Block Device Service
CINDER_DRIVER=ceph
CINDER_CEPH_USER=cinder
CINDER_CEPH_POOL=cinder-pool
CINDER_CEPH_UUID=1b1519e4-5ecd-11e5-8559-080027f18a73
CINDER_BAK_CEPH_POOL=cinder-backups
CINDER_BAK_CEPH_USER=cinder-backups
CINDER_ENABLED_BACKENDS=ceph
CINDER_ENABLED_BACKENDS=ceph

# Nova - Compute Service
NOVA_CEPH_POOL=nova-pool
</code></pre>

<h2>Scenario 8: 安装Murano</h2>

<p>想通过这个例子演示，对于一个新的OpenStack项目，如何使用Devstack尝鲜。</p>

<pre><code class="bash bash">cd /opt/stack.kilo
git clone https://github.com/openstack/murano --branch=stable/kilo
cd murano/contrib/devstack
cp lib/murano ${DEVSTACK_DIR}/lib
cp lib/murano-dashboard ${DEVSTACK_DIR}/lib
cp extras.d/70-murano.sh ${DEVSTACK_DIR}/extras.d
</code></pre>

<pre><code class="plain devstack/localrc"># Enable Neutron
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron

# Enable Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Enable Murano
enable_service murano murano-api murano-engine
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Kilo)Devstack Kilo版本localrc推荐]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/11/best-localrc-for-devstack-kilo/"/>
    <updated>2015-05-11T11:33:44+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/11/best-localrc-for-devstack-kilo</id>
    <content type="html"><![CDATA[<p>Devstack在Kilo版本中发生了一些变化，其中一个commit(279cfe75198c723519f1fb361b2bff3c641c6cef)的就是优化默认启动的程序，尽量减小对硬件的要求。如果不修改默认的配置进行安装，会产生一些问题，例如VNC无法打开，Heat模块没有加载等。这里给出一个个人比较常用的localrc，供大家参考。该配置在Ubuntu 14.04 Server LTS进行了测试。</p>

<!-- more -->


<p>该配置文件中开启了所有的OpenStack的核心模块，以下几点需要注意：</p>

<ul>
<li>为了运行Neutron，服务器必须是双网卡，否则外网不会通</li>
<li>我的实验网段为200.21.0.0/16，eth0的IP为200.21.1.61，eth1与eth0为同一网段</li>
<li>eth1为公网访问网络，floating网络范围200.21.50.1/24，配置的GATEWAY为200.21.50.2</li>
<li>保证eth1所处的网段能够连接外网，但是配置为manual模式，配置如下：</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth1
iface eth1 inet manual
up ifconfig $IFACE 0.0.0.0 up
down ifconfig $IFACE 0.0.0.0 down
</code></pre>

<ul>
<li>localrc的配置</li>
</ul>


<pre><code class="plain localrc"># Misc
ADMIN_PASSWORD=sysadmin
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=$ADMIN_PASSWORD

# Target Path
DEST=/opt/stack.kilo

# Enable Logging
LOGFILE=$DEST/logs/stack.sh.log
VERBOSE=True
LOG_COLOR=True
SCREEN_LOGDIR=$DEST/logs

# Nova
enable_service n-novnc n-cauth

# Neutron
disable_service n-net
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron
ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas

# Ceilometer
enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api
enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator

# Enable Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Trove
enable_service trove tr-api tr-tmgr tr-cond

# Sahara
enable_service sahara

#FIXED_RANGE=10.0.0.0/24
HOST_IP=200.21.1.61
FLOATING_RANGE=200.21.50.1/24
PUBLIC_NETWORK_GATEWAY=200.21.50.2
Q_FLOATING_ALLOCATION_POOL=start=200.21.50.100,end=200.21.50.150
</code></pre>

<ul>
<li><p>确认br-ex是否存在
<code>plain sudo ovs-vsctl show
Bridge br-ex
  Port br-ex
      Interface br-ex
          type: internal
  Port "qg-7ec5be02-69"
      Interface "qg-7ec5be02-69"
          type: internal
ovs_version: "2.0.2"
</code></p></li>
<li><p>将eth1作为br-ex的接口
<code>bash
sudo ovs-vsctl add-port br-ex eth1
</code></p></li>
</ul>


<pre><code class="plain sudo ovs-vsctl show">Bridge br-ex
    Port br-ex
        Interface br-ex
            type: internal
    Port "qg-7ec5be02-69"
        Interface "qg-7ec5be02-69"
            type: internal
    Port "eth1"
        Interface "eth1"
ovs_version: "2.0.2"
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack Kilo版本新功能分析]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/04/what-is-new-in-kilo/"/>
    <updated>2015-05-04T10:37:22+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/04/what-is-new-in-kilo</id>
    <content type="html"><![CDATA[<p>OpenStack Kilo版本已经于2015年4月30日正式Release，这是OpenStack第11个版本，距离OpenStack项目推出已经整整过去了5年多的时间。在这个阶段OpenStack得到不断的增强，同时OpenStack社区也成为即Linux之后的第二大开源社区，参与的人数、厂商众多，也成就了OpenStack今天盛世的局面。虽然OpenStack在今年经历了Nebula的倒闭，但是随着国内的传统行业用户对OpenStack越来越重视，我们坚信OpenStack明天会更好。</p>

<p>OpenStack Kilo版本的完整翻译版本可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans">https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans</a></p>

<p>OpenStack Kilo版本的翻译工作由我和我的同事裴莹莹(Wendy)共同完成，翻译校对工作由裴莹莹完成。如果翻译有任何问题，请各位多多指正。</p>

<!-- more -->


<h2>社区贡献分析</h2>

<p>我们先来看一下OpenStack在最近的4个稳定版本发布中，每一个项目的贡献情况：</p>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-contribution-by-modules.jpg"></p>

<p>我们能够很明显的发现，OpenStack最早的几大核心模块(Nova, Cinder, Glance, Keystone, Horizon, Swift)的代码贡献所占比例呈明显下降趋势，这里强调一下，是比例而不是数量，从数量上来看，版本之间相差并不大，以Nova为例，从Havana版本的24%下降到如今的10%。这从一个侧面反映了OpenStack的核心模块日趋稳定，更多的关注集中到更高层次或者功能优化上。</p>

<p>Neutron模块则一直处于稳中有升的状态,从Havana版本的7%上升到10%，说明Neutron仍然处于需要进一步完善的状态。</p>

<p>对于Ceilometer，Heat，Sahara，Ironic, Trove等新晋的核心模块，都处于稳步增长的阶段。贡献的比例在四个版本中基本保持持平的态势。在Kilo版本中，Sahara和Heat进入了前十名。</p>

<p>从Kilo版本的比例来看，Others的比例过半，Others主要包括了OpenStack测试相关项目，例如Rally；开发相关项目，例如Devstack;以及一些新的模块，例如：Manila，Magnum等众多进入孵化器的项目;还包括所有的Client以及Spec等。可以预见，OpenStack的开发重心逐步从底层的核心模块，逐步向更高层次、提供更丰富功能的方向发展。</p>

<h2>国内社区贡献分析</h2>

<p><img class="center" src="/images/blogs/what-is-new-in-kilo-contributor.png"></p>

<p>从企业贡献排名来看，几大巨头企业牢牢占据贡献榜的前几名，OpenStack最成功的公司-Mirantis排名紧追Redhat成为第二贡献大户。排名前几位的公司还包括：IBM, Rackspace, Cisco, Suse, VMware, Intel等。</p>

<p>国内方面，华为继续稳定在第13名，但Review的数量从Juno版本的1353提升到2548个，贡献的项目几乎涵盖所有的项目，主要贡献来自Heat，Ceilometer, Horizon，Neutron, Nova等项目。</p>

<p>国内排名第2的贡献企业是九州云，排名达到了21位，看来龚永生的到来为九州云添加了无限活力。九州云的贡献主要来自Horizon和Neutron两个项目，龚永生不愧为Neutron的Core，在网络方面的贡献，九州云的确很给力。</p>

<p>排名第3的企业是海云捷迅，排名为44位，海云是国内比较早的一批OpenStack创业企业，贡献方面以Sahara，Neutron，Nova，oslo.messaging以及Cinder为主，从之前了解的情况来看，海云的项目不少，可能提交的修改是与在实际项目中遇到的问题有关。</p>

<p>排名之后的企业还有Kylin Cloud，UnitedStack，EasyStack等。由于是手工统计，在统计过程中如有遗漏，希望大家多多指正。</p>

<h2>Horizon新功能</h2>

<p>Horizon在K版本除了增强了对新增模块的支持，从UE的角度也为我们带来了很多新功能</p>

<ul>
<li>支持向导式的创建虚拟机，现在还处于beta版本，如果想在Horizon里激活，可以通过设置local_setting.py的配置实现：</li>
</ul>


<pre><code class="plain local_setting.py">LAUNCH_INSTANCE_NG_ENABLED = True
</code></pre>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-instance-guide1.png"></p>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-instance-guide2.png"></p>

<ul>
<li>支持简单的主题，主要通过修改<em>variables.scss和</em>style.scss完成对主题颜色和简单样式的修改，但是格局不能改变，修改local_settings.py</li>
</ul>


<pre><code class="plain local_setting.py">CUSTOM_THEME_PATH = 'static/themes/blue'
</code></pre>

<pre><code class="plain static/themes/blue/_variables.scss">$gray:                   #2751DB !default;
$gray-darker:            #94A5F2 !default;
$gray-dark:              #0C0CED !default;
$gray-light:             #C7CFF2 !default;
$gray-lighter:           #DCE1F5 !default;

$brand-primary:         #375A7F !default;
$brand-success:         #00bc8c !default;
$brand-info:            #34DB98 !default;
$brand-warning:         #F39C12 !default;
$brand-danger:          #E74C3C !default;
</code></pre>

<pre><code class="plain static/themes/blue/_style.scss">// Blue
// ----

@mixin btn-shadow($color) {
  @include gradient-vertical-three-colors(lighten($color, 3%), $color, 6%, darken($color, 3%));
  filter: none;
  border: 1px solid darken($color, 10%);
}

// Buttons ====================================================================

.btn-default,
.btn-default:hover {
  @include btn-shadow($btn-default-bg);
}

.btn-primary,
.btn-primary:hover {
  @include btn-shadow($btn-primary-bg);
}
</code></pre>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-horizon-theme1.png"></p>

<p><img class="left" src="/images/blogs/what-is-new-in-kilo-horizon-theme2.png"></p>

<h2>Nova新功能</h2>

<h3>Nova Scheduler</h3>

<ul>
<li>标准化了conductor，compute与scheduler的接口，为之后的接口分离做好准备，对于部分直接访问nova数据库的filters进行了优化，不再允许直接访问，参考链接：<a href="https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst">https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst</a></li>
<li>对Scheduler做了一些优化，例如：Scheduler对于每一个请求都会重新进行Filters/Weighers，为了优化这个问题，将filter/weighter的初始化从handler移到scheduler，这样每次请求的时候都可以重新使用了。</li>
</ul>


<h3>Libvirt NFV相关功能</h3>

<ul>
<li>NUMA(Non Uniform Memory Architecture)，在这个架构下，每个处理器都会访问“本地”的内存池，从而在CPU和存储之间有更小的延迟和更大的带宽。</li>
<li>在Kilo版本中针对此功能的实现包括：基于NUMA的调度的实现；可以将vCPU绑定在物理CPU上；超大页的支持。以上提到的三点都是通过Flavor的Extra Spces完成定义的。</li>
</ul>


<h3>EC2 API</h3>

<ul>
<li>EC2 API被从Nova中踢出去了</li>
<li>取而代之的是在stackforge的EC2 API转换服务</li>
</ul>


<h3>API Microversioning</h3>

<p>先来解释一下为什么需要API的微版本：主要原因在于现在这种API扩展方式，对于API实现的代码的增加或减少管理非常不方便，容易导致不一致性。引入微版本主要目的就是让开发人员在修改API代码时能够向前兼容，而不是加入一个新的API扩展；用户通过指定API的版本，在请求时也能决定是使用的具体的动作。</p>

<p>包含版本的返回:
<code>plain Result
GET /
{
     "versions": [
        {
            "id": "v2.1",
            "links": [
                  {
                    "href": "http://localhost:8774/v2/",
                    "rel": "self"
                }
            ],
            "status": "CURRENT",
            "version": "5.2"
            "min_version": "2.1"
        },
   ]
}
</code></p>

<p>客户端的Header信息：
<code>plain Header
X-OpenStack-Nova-API-Version: 2.114
</code></p>

<h3>一个已知的问题：Evacuate</h3>

<p>这个问题的产生主要是因为Evacuate的清理机制，主机名的变化会导致nova-compute重启过程中误删所有虚拟机，所以一个变通的方法是设置</p>

<pre><code class="plain nova.conf">destroy_after_evacuate=False
</code></pre>

<p>这个问题会在Liberty中得到修复，相关的Spec：<a href="https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst">https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst</a></p>

<h2>Glance新功能</h2>

<ul>
<li>自动进行镜像格式转化，例如，Ceph是使用RAW格式的，假如我们上传的是QCOW2，创建虚拟机时，就会经历一番上传下载的过程，速度异常缓慢。而且RAW格式通常都是原始大小，上传时候非常慢，完全可以通过上传小镜像自动转换为指定格式。</li>
<li>Glance支持多字段排序
<code>plain API
/images?sort_key=status&amp;sort_dir=asc&amp;sort_key=name&amp;sort_dir=asc&amp;sort_key=created_at&amp;sort_dir=desc
</code></li>
<li>临时将镜像设置为非活跃状态，假如一个镜像里有病毒，管理员就会将该镜像设置为非活跃状态，在清理后重新发布该镜像，在这个过程中，所有非管理员用户都无法使用或者下载这个镜像</li>
<li>免重启动态加载配置文件，配置文件改动后重启服务，现在可以给glance服务发送SIGHUP触发，这样升级就可以零当机时间。</li>
<li>使用多个Swift容器存储镜像，减少大规模部署时的请求瓶颈</li>
</ul>


<h2>Cinder新功能</h2>

<ul>
<li>实现服务逻辑代码与数据库结构之间的解耦，支持Rolling更新</li>
<li>一致性组是指具备公共操作的卷，逻辑上化为一组。在K版本中对增强一致性组的功能：可以添加、删除卷，从已经存在的快照创建新的组，关于一致性组的详细操作可以参考：<a href="http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html">http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html</a></li>
</ul>


<pre><code class="plain cinder">cinder consisgroup-update
[--name NAME]
[--description DESCRIPTION]
[--add-volumes UUID1,UUID2,......]
[--remove-volumes UUID3,UUID4,......]
CG
</code></pre>

<pre><code class="plain cinder">cinder consisgroup-create-from-src
[--cgsnapshot CGSNAPSHOT]
[--name NAME]
[--description DESCRIPTION]
</code></pre>

<ul>
<li>卷类型的增强功能主要包含两个：为某一项目创建私有的卷类型和为卷类型增加描述信息</li>
</ul>


<pre><code class="plain cinder">cinder type-create &lt;name&gt; --is-public
cinder type-create &lt;name&gt; &lt;description&gt;
</code></pre>

<h2>Neutron新功能</h2>

<ul>
<li>DVR支持OVS中的VLANs</li>
<li>新的V2版本的LBaas的API</li>
<li>新的插件的更新，详情请见更新日志中</li>
<li>一些高级服务的分离，例如：L3, ML2, VPNaaS, LBaaS</li>
</ul>


<p>网络方面我不是权威，希望有高人能出来讲讲Kilo中的Neutron新功能。</p>

<h2>Keystone新功能</h2>

<ul>
<li>项目嵌套，创建一个新的Project时候，可以指定parent的Project</li>
</ul>


<pre><code class="plain keystone">POST /projects

{
    "project": {
        "description": "Project space for Test Group",
        "domain_id": "1789d1",
        "enabled": true,
        "name": "Test Group",
        "parent_id": "7fa612"
    }
}
</code></pre>

<ul>
<li>Keystone与Keystone的联盟，有了这个功能两个或者更多的云服务提供者就可以共享资源，这个功能在J版本引入，在K版本中主要针对该功能的进一步增强，具体的使用方法可参考这篇博文：<a href="http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/">http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/</a></li>
<li>针对新人授权的一些增强功能</li>
<li>keystone的配置中有部分配置发生了变化，例如：keystone.token.backends.memcache被keystone.token.persistence.backends.memcache取代，更多详细内容请参考更新日志</li>
</ul>


<h2>Swift新功能</h2>

<ul>
<li>纠删码的加入应该是这个版本最大的亮点，但是纠删码作为beta版本发布，并不推荐应用于生产环境，关于纠删码的详细介绍可以参考：<a href="http://docs.openstack.org/developer/swift/overview_erasure_code.html">http://docs.openstack.org/developer/swift/overview_erasure_code.html</a></li>
<li>复合型令牌，简而言之就是需要用户加上服务的Token才能对Swfit存放的内容进行操作，如下图所示：</li>
</ul>


<pre><code class="plain swift">client
   \
    \   &lt;request&gt;: &lt;path-specific-to-the-service&gt;
     \  x-auth-token: &lt;user-token&gt;
      \
    SERVICE
       \
        \    PUT: /v1/SERVICE_1234/&lt;container&gt;/&lt;object&gt;
         \   x-auth-token: &lt;user-token&gt;
          \  x-service-token: &lt;service-token&gt;
           \
          Swift
</code></pre>

<p>具体的设计文档：<a href="http://docs.openstack.org/developer/swift/overview_backing_store.html">http://docs.openstack.org/developer/swift/overview_backing_store.html</a></p>

<ul>
<li>全局性的集群复制优化，大幅提高效率，避免经过广域网传播的数据</li>
</ul>


<h2>Ceilometer新功能</h2>

<ul>
<li>支持Ceph对象存储监控，当对象存储为Ceph而不是Swfit的时候，使用Polling机制，使用Ceph的Rados Gateway的API接口获取数据，具体的设计文档：<a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst</a></li>
<li>Ceilometer API RBAC - 更细粒度的权限控制: <a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst</a>
<code>plain Ceilometer
{
  "context_is_admin": [["role:admin"]]
}
</code>
更细粒度的控制
<code>plain Ceilometer
{
   "context_is_admin": [["role:admin"]],
   "admin_or_cloud_admin": [["rule:context_is_admin"],
            ["rule:admin_and_matching_project_domain_id"]],
   "telemetry:alarm_delete": [["rule:admin_or_cloud_admin"]]
}
</code></li>
<li>接口中的模糊查询，增加了一个新的查询符号=~</li>
<li>支持更多的测量，包括Hyper-V，IPMI相关的</li>
</ul>


<h2>Ironic新功能</h2>

<ul>
<li>iLO的优化</li>
<li>使用Config Drive替代Metadata服务</li>
<li>全盘镜像支持，可以跳过raddisk和kernel，这样就可以部署Windows的镜像了</li>
<li>使用本地盘启动，替代PXE方式，可以通过设置flavor的capabilities:boot_option实现</li>
</ul>


<h2>Oslo</h2>

<p>解决了很多之前遗留的技术债，还有一些命名规范的问题。olso.messaging实现了心跳，olso.log在所有项目中使用，优化了oslo.db的代码。</p>

<h2>OpenStack文档</h2>

<p>优化了docs.openstack.org页面，也可以从中选择相应的语言。有专门的团队负责安装、网络和高可靠的文档。</p>

<h2>其他模块</h2>

<p>对于Sahara, Heat, Trove等模块的更新没有在这里Highlight出来，大家可以参考更新日志里的内容，或者查看specs中的具体描述。</p>

<h2>总结</h2>

<p>通过Kilo的一些更新可以看到，Kilo版本在不断优化代码结构的基础上，增加了一些新功能，也偿还了一些技术债，总体来说是一种稳中有升的态势，但是总体感觉并没有太多的惊喜和出人意料。相信随着更多的孵化项目进入正式版本中，OpenStack一定会向更多元化的方向发展。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph可靠性的量化分析]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability/"/>
    <updated>2015-03-04T08:36:06+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability</id>
    <content type="html"><![CDATA[<p>在开始正文之前，首先要感谢UnitedStack工程师朱荣泽对这篇博文的大力帮助和悉心指教。本文主要针对UnitedStack公司在巴黎峰会上对Ceph可靠性的计算方法(<a href="https://www.ustack.com/blog/build-block-storage-service/">https://www.ustack.com/blog/build-block-storage-service/</a>)做了一个更明确的分析和阐述，供对此话题感兴趣的朋友一起来讨论、研究，文章中如果有不当之处，还请各位高人指教。</p>

<!-- more -->


<h2>什么情况下数据会丢失？</h2>

<p>这个话题的另外一种提法就是存储的可靠性，所谓存储的可靠性最基本的一点就是数据不要丢失，也就是我们俗称的“找不回来了”。所以，要分析Ceph的可靠性我们只需要搞清楚，到底在什么情况下我们的数据会丢失，并且再也无法恢复，基于此我们便可以创建我们的计算模型。</p>

<p>我们先来假定一套简单的Ceph环境，3个OSD节点，每个OSD节点对应一块物理硬盘，副本数为3。那么我们排除MON的因素影响Ceph集群的运行的问题，显而易见，当三个OSD对应的物理硬盘全部损坏时，数据必然无法恢复。所以此时集群的可靠性是与硬盘本身的可靠性直接相关。</p>

<p>我们再来假定一套更大的Ceph环境，30个OSD节点，分3个机架摆放，每一个机架有10个OSD节点，每个OSD节点仍然对应一块物理硬盘，副本数为3，并且通过CRUSH MAP，将每一份副本均匀分布在三个机架上，不会出现两份副本同时出现在一个机架的情况。此时，什么时候会出现数据丢失的情况呢？当三个机架上都有一块硬盘损坏，而恰恰这三块硬盘又保存了同一个Object的全部副本，此时数据就会出现丢失的情况。</p>

<p>所以根据以上的分析，我们认为，Ceph的可靠性的计算是与OSD的数量(N)、副本数(R)、每一个服务节点的OSD数量(S)、硬盘的年失败概率(AFR)。这里我们使用UnitedStack相关参数进行计算。</p>

<p>具体取值如下图所示：</p>

<p><img src="/images/blogs/ceph-reliability-formula.jpg" width="640" height="480"></p>

<p><img src="/images/blogs/ceph-reliability-constant.jpg" width="640" height="480"></p>

<h2>硬盘年失败概率</h2>

<p>根据维基百科的计算方法(<a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">http://en.wikipedia.org/wiki/Annualized_failure_rate</a>)，AFR的计算方法如下：</p>

<p><img src="/images/blogs/ceph-reliability-afr.jpg" width="640" height="480"></p>

<p>例如，计算Seagate某企业级硬盘的AFR，根据文档查到MTBF为1,200,000小时，则AFR为0.73%，计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-afr-example.jpg" width="640" height="480"></p>

<p>但是，根据Google的相关计算，在一个大规模集群环境下，往往AFR的值并没有硬盘厂商那样乐观，下面的统计告诉了我们在真实环境下AFR变化的情况：</p>

<p><img src="/images/blogs/ceph-reliability-google-afr.jpg" width="640" height="480"></p>

<p>所以我们可以看到实际的AFR的变化范围随着年份而变化，取值范围在1.7%-8%左右，所以本文中AFR为1.7%。</p>

<h2>硬盘在一年之内损坏的概率</h2>

<p>有了AFR，我们就可以尝试计算硬盘在一年中出现故障的概率，根据相关研究，硬盘在一定时间内的失败概率符合Possion分布(已经把知识还给老师的同学请移步：<a href="http://en.wikipedia.org/wiki/Poisson_distribution">http://en.wikipedia.org/wiki/Poisson_distribution</a>)。具体的计算公式为：</p>

<p><img src="/images/blogs/ceph-reliability-Pn.jpg" width="640" height="480"></p>

<p>当我最初拿到这个计算公式时，一下子懵了，到底该如何确定数学期望值lamda呢？</p>

<h2>lamda的计算过程</h2>

<p>根据相关的研究资料，单块的硬盘损坏的期望值(Failures in Time)是指每10亿小时硬盘的失败率(Failure Rate λ)，计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-fit.jpg" width="640" height="480"></p>

<p>这里的Af(Acceleration Factor)是由测试时间乘以阿伦尼乌斯方程的值得出来的结果，好吧，我承认，我也是现学现卖，这个方程式是化学反应的速率常数与温度之间的关系式，适用于基元反应和非基元反应，甚至某些非均相反应。不过可以看出Failure Rate的计算过程实质主要是计算环境因素引起的物理变化，最终导致失败的数学期望值。所以根据相关研究，最终FIT的计算方法为：</p>

<p><img src="/images/blogs/ceph-reliability-fit-afr.jpg" width="640" height="480"></p>

<p>有了这些参数后，我们就可以开始正式计算Ceph集群中，不同机架上有三块硬盘同时出现损坏的概率啦。</p>

<h2>任意一个OSD出现损坏的概率P1(any)</h2>

<p>我们不太容易直接去计算任意一个OSD出现损坏的概率，但是我们很容易计算没有OSD出现问题的概率，方法如下，用一减去无OSD节点出现问题的概率，得到P1(any)。</p>

<p><img src="/images/blogs/ceph-reliability-osd1-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第二个节点出现故障的概率P2(any)</h2>

<p>我们知道当Ceph发现一个有问题的OSD节点时，会自动的将节点OUT出去，这个时间大约为10min，同时Ceph的自我修复机制会自动平衡数据，将故障节点的数据重新分配在其他的OSD节点上。</p>

<p>我们假设我们单盘的容量为1000 GB，使用率为75%，也就是此时将有750 GB的数据需要同步。我们的数据只在本机架平衡，节点写入速度为50 MB/s，计算方法如下：</p>

<p><img src="/images/blogs/ceph-reliability-recovery-time.jpg" width="640" height="480"></p>

<p>注意：由于每个节点有三个OSD，所以要求每台物理机所承受的节点带宽至少要大于150 MB/s。并且在这个计算模型下，并没有计算元数据、请求数据、IP包头等额外的信息的大小。</p>

<p>有了Recovery Time，我们就可以计算我们第二个节点在Recovery Time内失败的概率，具体的计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-osd2-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第三个节点出现故障的概率P3(any)</h2>

<p>计算方法同上，计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-osd3-failure.jpg" width="640" height="480"></p>

<h2>一年内任意副本数&reg;个OSD出现故障的概率</h2>

<p>所以将上述概率相乘即可得到一年内任意副本数&reg;个OSD出现故障的概率。</p>

<p><img src="/images/blogs/ceph-reliability-arbitrary-osd-failure.jpg" width="640" height="480"></p>

<h2>Copy Sets(M)</h2>

<p>在这个计算模型中，因为任意R个OSD节点的损坏并不意外着副本的完全丢失，因为损坏的R个OSD未必保存着一个Object的全部副本信息，所以未必造成数据不可恢复，所以这里引入了Copy Sets的概念。简单来说，Copyset就是存放所有拷贝的一个集合，具体的定义和计算方法可以查看参考链接。那么这里的场景下，Copy Sets为三个机架OSD数量相乘，即M=24<em>24</em>24。当然如果是两个副本的情况下，M应该为24<em>24+24</em>24+24*24。</p>

<p><img src="/images/blogs/ceph-reliability-copysets.jpg" width="640" height="480"></p>

<h2>CEPH的可靠性</h2>

<p>所以最终归纳出CEPH可靠性的算法为：</p>

<p><img src="/images/blogs/ceph-reliability-copysets-failure.jpg" width="640" height="480"></p>

<p>可以看出Ceph三副本的可靠性大约为9个9，由于Recovery Time和AFR取值的问题，所以计算结果和UnitedStack上略有出入。</p>

<h2>参考链接</h2>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">Annualized Failure Rate</a></li>
<li><a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a></li>
<li><a href="http://www.microsemi.com/document-portal/doc_view/124041-calculating-reliability-using-fit-mttf-arrhenius-htol-model">Calculating Reliability using FIT &amp; MTTF: Arrhenius HTOL Model</a></li>
<li><a href="http://storagemojo.com/2007/02/19/googles-disk-failure-experience/">Google’s Disk Failure Experience</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/disk_failures.pdf">Failure Trends in a Large Disk Drive Population</a></li>
<li><a href="http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/11727-atc13-cidon.pdf">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
