<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud Computing | RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/blog/categories/cloud-computing/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2015-12-24T17:56:14+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Consul的安装方法]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/"/>
    <updated>2015-12-07T10:00:13+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/12/07/consul-installation</id>
    <content type="html"><![CDATA[<h2>什么是Consul?</h2>

<p>Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。</p>

<!-- more -->


<h2>安装Consul</h2>

<p>Consul的安装比较简单，下载之后直接解压缩就可以了，下载地址：<a href="https://www.consul.io/downloads.html">https://www.consul.io/downloads.html</a></p>

<p>我们把consul直接放在/usr/local/bin目录中。</p>

<h2>Consul Server</h2>

<pre><code>$ /usr/local/bin/consul agent -server -bootstrap-expect 3 -data-dir /tmp/consul -node=server1 -bind=10.10.10.10
</code></pre>

<h3>参数说明</h3>

<ul>
<li>-server - Serve模式</li>
<li>-bootstrap-expect - Server数量</li>
<li>-data-dir - 数据目录</li>
<li>-node - Node名称</li>
<li>-bind - 集群通讯地址</li>
</ul>


<h3>输出</h3>

<pre><code>==&gt; WARNING: Expect Mode enabled, expecting 3 servers
==&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1
==&gt; Starting Consul agent...
==&gt; Starting Consul agent RPC...
==&gt; Consul agent running!
         Node name: 'server1.consul.com'
        Datacenter: 'dc1'
            Server: true (bootstrap: false)
       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)
      Cluster Addr: 200.21.1.101 (LAN: 8301, WAN: 8302)
    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false
             Atlas: &lt;disabled&gt;

==&gt; Log data will now stream in as it occurs:

    2015/12/23 03:13:36 [WARN] memberlist: Binding to public address without encryption!
    2015/12/23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101
    2015/12/23 03:13:36 [WARN] memberlist: Binding to public address without encryption!
    2015/12/23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com.dc1 200.21.1.101
    2015/12/23 03:13:36 [INFO] raft: Node at 200.21.1.101:8300 [Follower] entering Follower state
    2015/12/23 03:13:36 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/23 03:13:36 [INFO] consul: adding server server1.consul.com.dc1 (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/23 03:13:36 [ERR] agent: failed to sync remote state: No cluster leader
    2015/12/23 03:13:37 [WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.
    2015/12/23 03:13:51 [ERR] agent: failed to sync remote state: No cluster leader
==&gt; Newer Consul version available: 0.6.0
    2015/12/23 03:14:17 [ERR] agent: failed to sync remote state: No cluster leader
</code></pre>

<h3>查看成员</h3>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
</code></pre>

<h2>Consul Agent</h2>

<pre><code>$ /usr/local/bin/consul agent -data-dir /tmp/consul -node=agent1 -bind=10.10.10.100 -config-dir /etc/consul.d
</code></pre>

<ul>
<li>输出</li>
</ul>


<pre><code>==&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1
==&gt; Starting Consul agent...
==&gt; Starting Consul agent RPC...
==&gt; Consul agent running!
         Node name: 'agent1.consul.com'
        Datacenter: 'dc1'
            Server: false (bootstrap: false)
       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)
      Cluster Addr: 200.21.1.201 (LAN: 8301, WAN: 8302)
    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false
             Atlas: &lt;disabled&gt;

==&gt; Log data will now stream in as it occurs:

    2015/12/24 08:09:51 [WARN] memberlist: Binding to public address without encryption!
    2015/12/24 08:09:51 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201
    2015/12/24 08:09:51 [ERR] agent: failed to sync remote state: No known Consul servers
    2015/12/24 08:09:56 [INFO] agent.rpc: Accepted client: 127.0.0.1:42794
    2015/12/24 08:09:56 [INFO] agent: (LAN) joining: [200.21.1.101 200.21.1.102 200.21.1.103]
    2015/12/24 08:09:56 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101
    2015/12/24 08:09:56 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/24 08:09:58 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:10:02 [INFO] agent: (LAN) joined: 1 Err: &lt;nil&gt;
    2015/12/24 08:10:02 [INFO] agent.rpc: Accepted client: 127.0.0.1:42800
==&gt; Newer Consul version available: 0.6.0
    2015/12/24 08:10:21 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:10:22 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:10:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:01 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:11:02 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:23 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:41 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:11:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:12:12 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:12:21 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:12:36 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:13:01 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:13:03 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
</code></pre>

<ul>
<li>server日志输出</li>
</ul>


<pre><code>    2015/12/24 08:09:58 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201
</code></pre>

<h3>查看成员</h3>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1
</code></pre>

<h2>最终结果</h2>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1
agent2.consul.com   200.21.1.202:8301  alive   client  0.5.2  2         dc1
server2.consul.com  200.21.1.102:8301  alive   server  0.5.2  2         dc1
server3.consul.com  200.21.1.103:8301  alive   server  0.5.2  2         dc1
agent3.consul.com   200.21.1.203:8301  alive   client  0.5.2  2         dc1
</code></pre>

<h2>参考文档</h2>

<ul>
<li><a href="https://www.consul.io/intro/getting-started/install.html">https://www.consul.io/intro/getting-started/install.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Grafana+Diamond+Graphite构造完美监控面板]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/12/01/use-grafana-to-monitor-your-cluster/"/>
    <updated>2015-12-01T07:59:46+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/12/01/use-grafana-to-monitor-your-cluster</id>
    <content type="html"><![CDATA[<p>服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。</p>

<p>本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：</p>

<p><img class="center" src="/images/blogs/grafana-screenshot.png" width="800"></p>

<!-- more -->


<h2>安装及配置Dimaond</h2>

<p>安装Diamond最直接和简单的方法就是自己编译RPM或者DEB的安装包, Diamond在这方面提供了比较好的支持。</p>

<pre><code class="bash bash"># cd /root
# yum install -y git rpm-build python-configobj python-setuptools
# git clone https://github.com/python-diamond/Diamond
# cd Diamond
# make rpm
# cd dist
# rpm -ivh diamond-*.noarch.rpm
</code></pre>

<p>默认情况下，Diamond开启了基本的监控信息，包括CPU、内存、磁盘的性能数据。当然，我们可以通过配置启动相应的监控项，也能通过自定义的方式进行相应的扩展。这里，我们在/etc/diamond/collectors加载额外的插件，下面的例子中开启了网络的监控。</p>

<pre><code class="bash bash"># cp -f /etc/diamond/diamond.conf.example /etc/diamond/diamond.conf

# cat &lt;&lt; EOF | tee -a /etc/diamond/diamond.conf
[configs]
path = "/etc/diamond/collectors/"
extension = ".conf"
EOF

# cat &lt;&lt; EOF | tee /etc/diamond/collectors/net.conf
[collectors]

[[NetworkCollector]]
enabled = True
EOF
</code></pre>

<p>那么到目前为止，Diamond的基本安装和配置已经完成，但是现在只是简单的采集数据，并没有指明数据要发送给谁，所以下一步我们来开始配置Graphite。</p>

<h2>安装及配置Graphite</h2>

<p>Graphite主要做两件事情：按照时间存储数据、生成图表，在我们的场景里面，实质上就是把Graphite作为数据源给Grafana提供数据。另外还需要安装的是carbon，负责通过网络接受数据并保存到后端存储中；另外还需要whisper，负责生成Graphite样式的基于文件的时间序列的数据库。</p>

<h3>安装软件包</h3>

<pre><code class="bash bash"># yum install -y graphite-web graphite-web-selinux
# yum install -y mysql mysql-server MySQL-python
# yum install -y python-carbon python-whisper
</code></pre>

<h3>配置MySQL</h3>

<pre><code class="bash bash"># /etc/init.d/mysqld start

# mysql -e "CREATE DATABASE graphite;" -u root
# mysql -e "GRANT ALL PRIVILEGES ON graphite.* TO 'graphite'@'localhost' IDENTIFIED BY 'sysadmin';" -u root
# mysql -e 'FLUSH PRIVILEGES;' -u root
</code></pre>

<h3>配置Graphite</h3>

<ul>
<li>local setting</li>
</ul>


<pre><code class="bash /etc/graphite-web/local_settings.py"># SECRET_KEY=$(md5sum /etc/passwd | awk {'print $1'})

# echo "SECRET_KEY = '$SECRET_KEY'" | tee -a /etc/graphite-web/local_settings.py
# echo "TIME_ZONE = 'Asia/Shanghai'" | tee -a /etc/graphite-web/local_settings.py

# cat &lt;&lt; EOF | tee -a /etc/graphite-web/local_settings.py
DATABASES = {
    'default': {
        'NAME': 'graphite',
        'ENGINE': 'django.db.backends.mysql',
        'USER': 'graphite',
        'PASSWORD': 'sysadmin',
    }
}
EOF

# cd /usr/lib/python2.6/site-packages/graphite
# ./manage.py syncdb --noinput

# echo "from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'admin@hihuron.com', 'sysadmin')" | ./manage.py shell
</code></pre>

<ul>
<li>Apache配置</li>
</ul>


<pre><code class="bash /etc/httpd/conf.d/graphite-web.conf">Listen 0.0.0.0:10000
&lt;VirtualHost *:10000&gt;
    ServerName graphite-web
    DocumentRoot "/usr/share/graphite/webapp"
    ErrorLog /var/log/httpd/graphite-web-error.log
    CustomLog /var/log/httpd/graphite-web-access.log common
    Alias /media/ "/usr/lib/python2.6/site-packages/django/contrib/admin/media/"

    WSGIScriptAlias / /usr/share/graphite/graphite-web.wsgi
    WSGIImportScript /usr/share/graphite/graphite-web.wsgi process-group=%{GLOBAL} application-group=%{GLOBAL}

    &lt;Location "/content/"&gt;
        SetHandler None
    &lt;/Location&gt;

    &lt;Location "/media/"&gt;
        SetHandler None
    &lt;/Location&gt;
&lt;/VirtualHost&gt;
</code></pre>

<ul>
<li>Diamond配置</li>
</ul>


<pre><code class="bash bash"># HOST_IP=$(ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\.){3}[0-9]*).*/\2/p' | head -1)

# sed  -i "/^\[\[GraphiteHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf
# sed  -i "/^\[\[GraphitePickleHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf
</code></pre>

<h3>启动服务</h3>

<pre><code class="bash bash"># service carbon-cache restart
# service httpd restart
# service diamond restart
</code></pre>

<h2>安装和配置Grafana</h2>

<p>Grafana最主要的功能就是对数据的呈现，基于一切可提供time series的后台服务。这里面我们使用Graphite为Grafana提供数据。</p>

<h3>安装及配置</h3>

<pre><code class="bash bash"># yum install -y nodejs
# rpm -ivh https://grafanarel.s3.amazonaws.com/builds/grafana-2.5.0-1.x86_64.rpm
# sudo /sbin/chkconfig --add grafana-server
# sed -i 's/^;http_port = 3000$/http_port = 10001/g' /etc/grafana/grafana.ini
# sudo service grafana-server start
</code></pre>

<h3>添加datasource</h3>

<p>Grafana提供了非常丰富的REST API，我们不仅可以直接利用Grafana作为数据呈现层，还可以利用REST API直接将Grafana的Graph集成在我们的应用中。下面我们利用REST API为Grafana添加datasource。</p>

<pre><code class="bash bash"># curl -i 'http://admin:admin@localhost:10001/api/datasources' -X POST -H "Accept: application/json" -H "Content-Type: application/json" -d '{"name": "graphite", "type": "graphite", "url": "http://localhost:10000", "access": "proxy", "basicAuth": false}'
</code></pre>

<h2>Ceph监控</h2>

<h3>修改ceph脚本兼容性</h3>

<p>Diamond是基于Python开发的，但是由于CentOS 6.5的Python版本较低(2.6)，所以直接使用社区版本的Ceph监控时，会导致错误。可以通过简单的修改进行修复。</p>

<pre><code class="python /usr/share/diamond/collectors/ceph/ceph.py">    def _get_stats_from_socket(self, name):
        """Return the parsed JSON data returned when ceph is told to
        dump the stats from the named socket.

        In the event of an error error, the exception is logged, and
        an empty result set is returned.
        """
        try:
            #json_blob = subprocess.check_output(
            #    [self.config['ceph_binary'],
            #     '--admin-daemon',
            #     name,
            #     'perf',
            #     'dump',
            #     ])
            cmd = [
                 self.config['ceph_binary'],
                 '--admin-daemon',
                 name,
                 'perf',
                 'dump',
            ]
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
            json_blob = process.communicate()[0]
</code></pre>

<h3>增加对ceph osd perf监控</h3>

<p>在实际运维Ceph过程中，ceph osd perf是一个非常重要的指令，能够观察出集群中磁盘的latency的信息，通过观察变化，可以辅助判断磁盘出现性能问题。Diamond的设计中，每个Diamond Agent只会采集自己本机的指标，所以我们在添加的时候，只需要在一个节点上增加这个监控就可以了。在ceph.py中结尾处新增加一个类。</p>

<pre><code class="python /usr/share/diamond/collectors/ceph/ceph.py">class CephOsdCollector(CephCollector):

    def _get_stats(self):
        """Return the parsed JSON data returned when ceph is told to
        dump the stats from the named socket.

        In the event of an error error, the exception is logged, and
        an empty result set is returned.
        """
        try:
            #json_blob = subprocess.check_output(
            #    [self.config['ceph_binary'],
            #     '--admin-daemon',
            #     name,
            #     'perf',
            #     'dump',
            #     ])
            cmd = [
                 self.config['ceph_binary'],
                 'osd',
                 'perf',
                 '--format=json',
            ]
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
            json_blob = process.communicate()[0]
        except subprocess.CalledProcessError, err:
            self.log.info('Could not get stats from %s: %s',
                          name, err)
            self.log.exception('Could not get stats from %s' % name)
            return {}

        try:
            json_data = json.loads(json_blob)
        except Exception, err:
            self.log.info('Could not parse stats from %s: %s',
                          name, err)
            self.log.exception('Could not parse stats from %s' % name)
            return {}

        return json_data

    def _publish_stats(self, stats):
        """Given a stats dictionary from _get_stats_from_socket,
        publish the individual values.
        """
        for perf in stats['osd_perf_infos']:
            counter_prefix = 'osd.' + str(perf['id'])
            for stat_name, stat_value in flatten_dictionary(
                perf['perf_stats'],
                prefix=counter_prefix,
            ):
              self.log.info('stat_name is %s', stat_name)
              self.log.info('stat_value is %s', stat_value)
              self.publish_gauge(stat_name, stat_value)

    def collect(self):
        """
        Collect stats
        """
        self.log.info('in ceph osd collector')
        stats = self._get_stats()
        self._publish_stats(stats)
</code></pre>

<h3>修改Diamond监控配置</h3>

<pre><code class="bash /etc/diamond/collectors/ceph.conf"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/ceph.conf
[collectors]

[[CephCollector]]
enabled = True

[[CephOsdCollector]]
enabled = True
EOF
</code></pre>

<pre><code class="bash bash"># service diamond restart
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度解读OpenStack Liberty国内代码贡献]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/"/>
    <updated>2015-10-29T18:56:06+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty</id>
    <content type="html"><![CDATA[<p>又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。</p>

<p>OpenStack Liberty完整版本的翻译可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans">https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans</a></p>

<p>本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。</p>

<!-- more -->


<h2>社区贡献分析</h2>

<p>本次统计，并没有采用Review的数量为依据，而直接采用commits的方式，也就是代码实际merge入库的数量。</p>

<p>我们仍然要先看一下模块的贡献情况：</p>

<p><img class="left" src="/images/blogs/contribution-in-liberty-contribution-by-modules.png" width="400"></p>

<p>与之前Release的特点相似，OpenStack早期的核心模块Nova, Keystone代码commits数量出现明显下滑状态，而Neutron, Heat, Trove, Ceilometer, Cinder等模块都保持着稳中有升的态势。值得关注的是，在排名前20名的项目中，出现了两个直接与Docker有关的项目Kolla和Magnum，一个与docker间接有关的项目Murano。可以预见，OpenStack下一步发展的热点就是在与Docker之间的勾勾搭搭。</p>

<p>特别需要注意的是，在stackalytics.com统计的模块中，在Kilo中是259个，而到了Liberty到了389个，当然有一些项目并非完全是OpenStack的项目，但是也从一个侧面反映出OpenStack以及周边项目的蓬勃发展。</p>

<p>从更新日志中我们也能看到，本次Release的正式项目中，变动较大的是Neutron和Heat两个模块。在经历不断锤炼后，Neutron逐渐走向成熟，但是从生产级别角度看，Neutron的确还有很长的路要走。</p>

<h2>国内社区贡献分析</h2>

<p><img class="center" src="/images/blogs/contribution-in-liberty-contributor.png" width="400"></p>

<p>从全球企业的贡献排名来看，排名状况基本变化不大，仍然是HP, Redhat, Mirantis, IBM, Rackspace, Intel, Cisco，但是非常欣喜的，国内的IT的航空母舰华为已经成功杀入前十名，这无疑是振奋人心的事情，希望华为未来能多一些对OpenStack社区的主导力，提高中国在OpenStack社区的地位，当然最好也能扶植一下国内的OpenStack创业公司，实现共同发展、共同进步。华为的主要代码贡献集中在dragonflow，magnum，heat等模块，特别是在dragonflow上，几乎全部是华为贡献的，magnum上也将近有五分之一的代码。</p>

<p><strong><em>华为社区贡献统计</em></strong></p>

<p><img class="center" src="/images/blogs/contribution-in-liberty-huawei.png" width="800"></p>

<p>记得在OpenStack五周年的庆祝活动上，Intel的陈绪博士说过，国内OpenStack贡献企业，就是一朵大云，四朵小云，下面让我们来看看这几朵小云在这个版本的表现。</p>

<p><strong><em> 99cloud社区贡献统计</em></strong></p>

<p><img class="center" src="/images/blogs/contribution-in-liberty-99cloud.png" width="800"></p>

<p>排名第16位的是99cloud，99cloud自上一个版本排名四朵小云之首后，本次继续强劲来袭，排名创造历史新高，第16名。通过对贡献模块的分析，我们能看出99cloud最大的贡献来自于社区文档，而在项目方面的贡献则主要来自murano-dashboard，horizon，neutron等项目上，从中可以看出99cloud对murano这个applicaton catalog的项目关注程度比较高，可能会在将来的产品中有所体现。从贡献中，我们隐约看到了九州云的副总裁李开总的提交，由此可见九州云为社区贡献的积极程度。
更加难能可贵的是，Horizon的全球贡献99cloud是全球前十，Tempest全球前八，Murano项目更是进入全球前三，相当给力。</p>

<p><strong><em> UnitedStack社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-unitedstack.png" width="800"></p>

<p>排在第30位的是UnitedStack，经过了上一个版本的短暂沉寂后，这个版本卷土重来，杀回前30。从代码贡献来看，UnitedStack的主要贡献来自python-openstackclient以及部署用到的puppet相关代码，当然对neutron、trove、kolla、heat等也有一定数量的贡献。</p>

<p><strong><em> Kylin Cloud社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-kylincloud.png" width="800"></p>

<p>排名第38位的是麒麟云，其实麒麟云每次Release中总是有她的身影，但好像总是被忽略的。麒麟云最大的贡献来自Horizon项目，其他模块也有一定数量的贡献。总之，我们想到OpenStack企业的时候，的确应该时常提起麒麟云。</p>

<p><strong><em> Easystack社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-easystack.png" width="800"></p>

<p>排名第70位的是Easystack，Easystack也属于OpenStack早期创业的公司，对于OpenStack的贡献也是持续的。Easystack最大的贡献来自nova，虽然数量不是很多，但是在国内企业里应该算名列前茅的啦。Easystack对Nova的贡献主要来自对libvirt层的bug修复。</p>

<p><strong><em> Awcloud社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-awcloud.png" width="800"></p>

<p>排名第75位的是海云捷迅，海云应该算是在国内发展比较迅猛的一家OpenStack早期创业公司。他们的贡献主要来自Neutron相关的项目，看起来应该是为了解决项目中出现的实际问题所做的努力。海云的马力应该是公司内部贡献排名第一的，尤其是前一段时间发布的两篇关于&#8221;Neutron &amp; OpenStack漫谈&#8221;，非常值得一读。</p>

<p><strong><em> LeTV社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-letv.png" width="800"></p>

<p><strong><em> Netease社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-netease.png" width="800"></p>

<p>排名第94和95位的分别是两家互联网企业，乐视和网易，乐视是最近互联网中使用OpenStack动静最大的一家了，应该能在大规模应用中发现OpenStack很多问题吧。</p>

<p><strong><em> Huron社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-huron.png" width="800"></p>

<p>排名第122位的是我的公司——北京休伦科技有限公司，其实我们公司也算是国内最早一批从事OpenStack创业的公司，z早在2013年的时候就已经开始投入OpenStack私有云产品相关的研发。我们贡献的代码主要来自Nova和Murano两个模块中，都是我们在开发和项目使用中发现的问题，修复后回馈给社区的，我也希望我们能在下一个版本Release中贡献更多的力量。</p>

<p><strong><em> China Mobile社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-chinamobile.png" width="800"></p>

<p>排名第133位的是中国移动，之前并没有在哪一个排名上看到过中国移动在OpenStack贡献，我也是第一次发现。中国移动应该算是国内运营商领域技术实力较强的一家，也是运营商里开始从事OpenStack预研较早的一家。中国移动有大量的IT资源和设备，理应像AT&amp;T一样在OpenStack领域大有所为。纵观中国移动的社区贡献，主要来自Neutron和Ceilometer两个项目，几个Bug修复都是与Volume相关。</p>

<p><strong><em> Lenovo社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-lenovo.png" width="800"></p>

<p>排名第135位的是联想。不评论了。</p>

<p>排名第139位的是清华大学医学院附属医院，这个有点意思。但是stackalytics.com有Bug，他们的具体统计显示不出来。</p>

<p><strong><em> H3C社区贡献统计 </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-h3c.png" width="800"></p>

<p>排名第143位的是H3C。贡献是Nova中的关于VMware的Bug Fix。</p>

<p>由于stackalytics并没有按照区域统计的功能，所以本次统计完全是全自动统计(全靠我自己手动)，所以难免遗漏了为OpenStack贡献的国内企业，如果发生该情况请及时告知。</p>

<h2>社区贡献内容分析</h2>

<p><img class="center" src="/images/blogs/contribution-in-liberty-complete-blueprints.png" width="800"></p>

<p>从贡献的commits的类型来区分，国内贡献出的代码主要还是以bug为主，这可能也与我们使用的都是OpenStack较成熟的模块有关，本身这些模块成熟程度较高，所以想做blueprint很难。另外一个很重要的原因是和OpenStack管理流程有关的，现在像Nova, Cinder等项目都是需要先Review Specs的，其实就是所谓的设计文档，语言成为国内很多工程师贡献的最大障碍，所以这也导致了Blueprint的贡献度在国内并不高。</p>

<p><strong><em> Huawei社区贡献——完成Blueprint </em></strong>
<img class="center" src="/images/blogs/contribution-in-liberty-blueprint-huawei.png" width="800"></p>

<p>纵观整个Blueprint的完成统计情况，华为作为国内最有实力的企业，高居全球第五名，完成最多的模块为cinder和mistral。</p>

<p>之后能完成Blueprint的企业还包括UnitedStack、中国移动、麒麟云、海云捷迅和九州云，但是相比来说数量较少，都是个位数字。</p>

<p>OpenStack在国内发展已经超过了四年的时间，但是遗憾的一点，尽管我们拥有世界上最多的开发人员，但是我们对社区仍然没有话语权，国内的用户的需求无法对社区上游形成影响，导致很多本地化定制的需求无法真正的在社区版本代码得到体现。所以如何让中国的声音出现在社区，是我们所有OpenStack人需要思考的问题。欣喜的一点，本土的巨头华为已经身先士卒，投入很大的力量搞OpenStack的社区贡献，我们更希望越来越多的国内传统IT巨头能够意识到这个问题，投身于开源的事业中，否则我们又在起跑线上输给了别人。</p>

<p>以上仅代表个人观点，如有任何异议，欢迎批评指正。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Kilo)Devstack完全用户手册]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/09/03/devstack-guide/"/>
    <updated>2015-09-03T18:34:20+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/09/03/devstack-guide</id>
    <content type="html"><![CDATA[<p>Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。</p>

<p>本篇Blog主要介绍以下几个实用场景：</p>

<ul>
<li>如何利用Devstack构建一套完美的开发环境</li>
<li>提高Devstack安装成功率的方法</li>
<li>Devstack的实用技巧</li>
<li>各种场景下的配置和注意事项</li>
</ul>


<p>本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。</p>

<!-- more -->


<h2>运行环境的选择</h2>

<p>对于刚刚接触OpenStack的开发者而言，没有太多闲置的资源，所以比较容易的上手方式就是使用虚拟机。对于桌面的虚拟机软件来说，主流的软件无外乎VMWare Workstation和Oracle Virtualbox，对于OpenStack开发而言，二者并无太大差异。以下几点可能会作为选择的主要依据：</p>

<ul>
<li>VMWare Workstation是收费软件，Virtualbox是免费软件</li>
<li>VMWare Workstation支持nested virtualization，就是安装完的devstack virt type是kvm，节省资源，Virtualbox安装以后只能使用qemu，虽然在Virtualbox 5以上版本号称支持，但是实际验证中仍然不能生效，还在研究中</li>
<li>VMWare Workstation使用NAT方式时，内部的IP可以在HOST主机直接访问到，Virtualbox还需要端口转发，所以建议单独增加一块Host-only的Apdaptor便于调试</li>
<li>使用Virtualbox时，为了让虚拟机能够访问外部网络，并且允许Host通过Floating IP对虚拟机进行访问，需要在Host层面设置NAT规则，转换到可以访问的物理网卡上，详情请见下文</li>
</ul>


<h2>Virtualbox网络设置</h2>

<p><img class="center" src="/images/blogs/devstack-guide-network-topology.jpg"></p>

<ul>
<li>Nova Network网卡配置</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet static
address 192.168.56.101
netmask 255.255.255.0

auto eth2
iface eth1 inet static
address 172.16.0.101
netmask 255.255.255.0
</code></pre>

<ul>
<li>Neutron网卡配置</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet static
address 192.168.56.101
netmask 255.255.255.0

auto eth2
iface eth2 inet manual
up ip link set dev $IFACE up
down ip link set dev $IFACE down
</code></pre>

<ul>
<li>MAC网卡NAT映射</li>
</ul>


<p>我们将第三块网卡作为提供外部网络的接口，采用系统层面的NAT方式让该网卡能够访问外部网络。</p>

<pre><code class="plain bash">sudo sysctl net.inet.ip.forwarding=1
</code></pre>

<p>在nat-anchor后面添加</p>

<pre><code class="plain /etc/pf.conf">nat on en0 from 172.16.0.0/24 -&gt; (en0)
</code></pre>

<p>之后加载</p>

<pre><code class="bash bash">sudo pfctl -e -f /etc/pf.conf
</code></pre>

<ul>
<li>Linux网卡NAT映射</li>
</ul>


<pre><code class="plain bash">echo 1 &gt; /proc/sys/net/ipv4/ip_forward
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
</code></pre>

<h2>Devstack快速开始</h2>

<p>其实，Devstack本身并不需要很复杂的配置就可以成功运行，但是仍然有几个需要注意的地方：</p>

<ul>
<li>Ubuntu 14.04 64bit(LTS), 12.04已经逐渐退出历史舞台，所以这里推荐14.04</li>
<li>不能使用root用户，即使你使用root用户执行Devstack，默认也会为你建立一个stack用户，所以不如老老实实的直接使用普通用户运行Devstack，或者提前建立好stack用户，切换后再执行</li>
<li>默认获取Devstack进行安装，安装的是master分支的代码，但是在实际开发中(比如我们做产品的时候)，都是基于某个stable分支进行，所以一般情况在clone devstack的时候需要指定stable分支</li>
</ul>


<p>下面给出一个最简安装步骤：</p>

<pre><code class="plain"># adduser stack
# apt-get install sudo -y
# echo "stack ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoers
# sudo su - stack

(stack)$ git clone https://git.openstack.org/openstack-dev/devstack --branch=stable/kilo
(stack)$ cd devstack &amp;&amp; ./stack.sh
</code></pre>

<h2>提高Devstack安装成功率</h2>

<p>估计在国内使用Devstack的人基本都遇到过安装失败的状况，为了节约大家的时间，先分析一下Devstack为什么会失败，我们先从这张时序图看一下Devstack执行的过程：</p>

<p><img class="center" src="/images/blogs/devstack-guide-flow.png"></p>

<p>从上述流程图中可以很清楚的看到Devstack有以下几个地方需要访问网络：</p>

<ul>
<li>安装依赖时，需要访问Ubuntu的源</li>
<li>执行get_pip.sh时，地址是彻底被墙的，需要访问<a href="https://bootstrap.pypa.io/get-pip.py">https://bootstrap.pypa.io/get-pip.py</a></li>
<li>从github clone源代码，github在国内访问速度并不很快而且间歇性被墙</li>
<li>安装过程中执行pip install requirements，需要访问pip repo</li>
<li>下载镜像，这一步骤取决于你需要安装的模块，如果默认安装只会下载cirros镜像，但是如果是安装类似Trove的模块，可能需要下载的更多</li>
</ul>


<hr />

<p>所以综上所述，为了提高devstack的安装成功率，需要从这几个方面着手优化：</p>

<ul>
<li>使用国内源</li>
</ul>


<pre><code class="plain /etc/apt/sources.list">deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse
deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse
</code></pre>

<ul>
<li>从国内源获取get-pip.py，从源代码可以分析出，检测get-pip.py的方式，这里面有两种方式一种是手动下载get-pip.py之后，注释代码，还有一种就是修改PIP_GET_PIP_URL的地址，但是这里只能通过修改install_pip.sh的方式，暂时无法从环境变量里获取</li>
</ul>


<pre><code class="bash devstack/tools/install_pip.sh">FILES=$TOP_DIR/files

PIP_GET_PIP_URL=https://bootstrap.pypa.io/get-pip.py
LOCAL_PIP="$FILES/$(basename $PIP_GET_PIP_URL)"

function install_get_pip {
    # The OpenStack gate and others put a cached version of get-pip.py
    # for this to find, explicitly to avoid download issues.
    #
    # However, if DevStack *did* download the file, we want to check
    # for updates; people can leave their stacks around for a long
    # time and in the mean-time pip might get upgraded.
    #
    # Thus we use curl's "-z" feature to always check the modified
    # since and only download if a new version is out -- but only if
    # it seems we downloaded the file originally.
    if [[ ! -r $LOCAL_PIP || -r $LOCAL_PIP.downloaded ]]; then
        curl --retry 6 --retry-delay 5 \
            -z $LOCAL_PIP -o $LOCAL_PIP $PIP_GET_PIP_URL || \
            die $LINENO "Download of get-pip.py failed"
        touch $LOCAL_PIP.downloaded
    fi
    sudo -H -E python $LOCAL_PIP
}
</code></pre>

<p>修改为我在coding.net上缓存的get-pip脚本</p>

<pre><code class="bash devstack/tools/install_pip.sh">PIP_GET_PIP_URL=https://coding.net/u/xiaoquqi/p/pip/git/raw/master/contrib/get-pip.py
</code></pre>

<ul>
<li>国内的代码托管服务器有从github上定期同步源代码的，但是经过实际测试都不是很理想，所以可能这是最不稳定的一部分，但是可以提前使用脚本，人工的下载所有代码，之后我会尝试在我自己的源中定时同步OpenStack源代码，敬请关注</li>
<li>现在pip的安装速度明显提升，原来还需要使用国内源，例如豆瓣，现在即使不修改也能很快的进行安装</li>
<li>镜像下载建议使用一些下载工具，然后放到指定的目录中，这样最有效</li>
</ul>


<h2>无网络状况下安装Devstack</h2>

<p>因为我们是做OpenStack的产品的公司，所以就要求我们的Devstack要能够满足无网络状况下的安装，之前也写过一篇详细介绍无网络安装Devstack博客,由于时间关系，可能一些内容已经过时了，这里面再进行一下更新，思路还是上面的思路，这里给出一些使用的工具，如果不清楚如何使用的话，可以参考我之前的博客。</p>

<ul>
<li>本地源的缓存使用apt-mirror，这是一个需要时间的工作，第一次同步的时间会非常长，准备好大约100G左右的空间吧</li>
<li>缓存get-pip.py，这个比较容易，搭建一个Apache服务器，但是需要把端口修改为10000，否则在安装好OpenStack后，会占用80端口，重新执行Devstack时候会出现错误</li>
<li>建立本地的Gerrit，并且上传所有代码</li>
<li>从requirements项目中，下载所有的pip，建立本地的pip缓存源，如果是搭建研发环境，可能还需要下载test-requirements的内容和tox</li>
<li>将镜像下载到刚刚创建的Apache服务器</li>
</ul>


<p>完成以上步骤，你可以尽情断掉外网，愉快的进行Devstack的安装了，稍后我会将以上步骤进行进一步完善。</p>

<h2>OFFLINE模式下安装Devstack</h2>

<p>在Devstack中提供了一种OFFLINE的方式，这种方式的含义就是，当你第一次完成安装后，所有需要的内容已经下载到本地，再次运行就没有必要访问网络了(前提是你不想升级)，所以可以将安装模式设置为OFFLINE，避免网络的访问，方法为：</p>

<pre><code class="bash devstack/localrc">OFFLINE=True
</code></pre>

<h2>虚拟机重启后，如何利用rejoin-stack.sh，免重新安装</h2>

<p>其实使用OFFLINE模式，可以在离线状态下无数次重新运行devstack，但是如果不是为了重新配置，我们并没有需要每次重新运行stack.sh。在Devstack中提供了另外一个脚本叫做rejoin-stack.sh，原理很简单就是把所有的进程重新组合进screen，所以我们借助这个脚本完全可以不重新执行stack.sh，快速恢复环境。但是当虚拟机重启后，cinder使用的卷组并不会自动重建，所以在运行rejoin之前，需要将恢复卷组的工作，放入开机启动的脚本中。</p>

<pre><code class="bash /etc/init.d/cinder-setup-backing-file">losetup /dev/loop1 /opt/stack/data/stack-volumes-default-backing-file
losetup /dev/loop2 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file
exit 0
</code></pre>

<pre><code class="bash Run as root">chmod 755 /etc/init.d/cinder-setup-backing-file
ln -s /etc/init.d/cinder-setup-backing-file /etc/rc2.d/S10cinder-setup-backing-file
</code></pre>

<pre><code class="bash Run as normal user">cd $HOME/devstack
./rejoin-stack.sh
</code></pre>

<h2>Scenario 0: 公共部分</h2>

<pre><code class="bash devstack/localrc"># Misc
ADMIN_PASSWORD=sysadmin
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=$ADMIN_PASSWORD

# Target Path
DEST=/opt/stack.kilo

# Enable Logging
LOGFILE=$DEST/logs/stack.sh.log
VERBOSE=True
LOG_COLOR=True
SCREEN_LOGDIR=$DEST/logs
</code></pre>

<h2>Scenario 1: 单节点Nova-Network的安装</h2>

<p>这应该就是Devstack默认的模式，有以下几点需要注意：</p>

<ul>
<li>根据上面的网卡配置</li>
</ul>


<blockquote><p>第一块网卡为NAT方式，用于访问外部网络</p>

<p>第二块为Host-only Adaptor，用于访问云平台</p>

<p>第三块为Host-only Adaptor，用于虚拟机桥接网路</p>

<p>需要注意的是：这种方式下并不能让虚拟机正常访问外部网络，可以通过将eth2设置为Bridge模式，但是这样会造成DHCP冲突(如果外部网络有DHCP)，所以暂时没有完美的解决方案</p></blockquote>

<ul>
<li>打开novnc和consoleauth，否则无法访问VNC</li>
</ul>


<p>这里给出的配置方案是第一种网络配置，即虚拟机无法网络外部网络的情况</p>

<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth

FLAT_INTERFACE=eth1
# eth1 address
HOST_IP=192.168.56.101
FIXED_RANGE=172.24.17.0/24
FIXED_NETWORK_SIZE=254
FLOATING_RANGE=172.16.0.128/25
</code></pre>

<h2>Scenario 2: 双节点Nova-Network的安装</h2>

<ul>
<li>控制节点</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth
disable_service n-cpu n-net n-api-meta c-vol

# current host ip
HOST_IP=192.168.56.101
FLAT_INTERFACE=eth1
MULTI_HOST=1
</code></pre>

<ul>
<li>计算节点</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth
ENABLED_SERVICES=n-cpu,n-net,n-api-meta,c-vol

# current host ip
HOST_IP=192.168.56.101
FLAT_INTERFACE=eth1
# needed by cinder-volume service
DATABASE_TYPE=mysql

# controller ip
SERVICE_HOST=192.168.56.101
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
NOVA_VNC_ENABLED=True
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html"
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</code></pre>

<h2>Scenario 3: 单节点Neutron的安装</h2>

<ul>
<li>基本配置</li>
</ul>


<pre><code class="bash devstack/localrc"># Nova
enable_service n-novnc n-cauth

# Neutron
disable_service n-net
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron
ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas

HOST_IP=192.168.56.101
FIXED_RANGE=20.0.0.0/24
NETWORK_GATEWAY=20.0.0.1
FLOATING_RANGE=172.16.0.0/24
PUBLIC_NETWORK_GATEWAY=172.16.0.1
Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200
</code></pre>

<ul>
<li>OVS设置</li>
</ul>


<p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p>

<pre><code class="bash bash">sudo ip addr flush dev br-ex
</code></pre>

<p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p>

<pre><code class="bash bash">sudo ovs-vsctl add-port br-ex eth2
</code></pre>

<h2>Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)</h2>

<ul>
<li>控制/网络节点
<figure class='code'><figcaption><span>devstack/localrc</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&lt;h1&gt;Nova&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;enable_service n-novnc n-cauth
</span><span class='line'>HOST_IP=192.168.56.101
</span><span class='line'>disable_service n-cpu n-net n-api-meta c-vol&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Neutron&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;disable_service n-net
</span><span class='line'>ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta
</span><span class='line'>FIXED_RANGE=20.0.0.0/24
</span><span class='line'>NETWORK_GATEWAY=20.0.0.1
</span><span class='line'>FLOATING_RANGE=172.16.0.0/24
</span><span class='line'>PUBLIC_NETWORK_GATEWAY=172.16.0.1
</span><span class='line'>Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200</span></code></pre></td></tr></table></div></figure></p>

<ul>
<li>计算节点
<figure class='code'><figcaption><span>devstack/localrc</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&lt;h1&gt;Nova&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;disable_all_services
</span><span class='line'>ENABLED_SERVICES=n-cpu,rabbit,neutron,q-agt,c-vol&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;current host ip&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;HOST_IP=192.168.56.103&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;needed by cinder-volume service&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;DATABASE_TYPE=mysql&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;controller ip&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SERVICE_HOST=192.168.56.101
</span><span class='line'>MYSQL_HOST=$SERVICE_HOST
</span><span class='line'>RABBIT_HOST=$SERVICE_HOST
</span><span class='line'>GLANCE_HOSTPORT=$SERVICE_HOST:9292
</span><span class='line'>NOVA_VNC_ENABLED=True
</span><span class='line'>NOVNCPROXY_URL=&ldquo;&lt;a href="http://$SERVICE_HOST:6080/vnc_auto.html"&gt;http://$SERVICE_HOST:6080/vnc_auto.html&lt;/a&gt;&rdquo;
</span><span class='line'>VNCSERVER_LISTEN=$HOST_IP
</span><span class='line'>VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</span><span class='line'>Q_HOST=$SERVICE_HOST</span></code></pre></td></tr></table></div></figure></p>

<ul>
<li>OVS设置</li>
</ul>


<p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p>

<pre><code class="bash bash">sudo ip addr flush dev br-ex
</code></pre>

<p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p>

<pre><code class="bash bash">sudo ovs-vsctl add-port br-ex eth2
</code></pre>

<h2>Scenario 5: 从源代码安装客户端</h2>

<p>新的Devstack里面默认不再提供client的源代码的安装方式，需要使用localrc中的环境变量进行开启，否则将直接从master获取的client代码进行安装，当然这样会造成系统无法正常使用。那么如何才能确定client在当前Devstack可用的版本呢？最简单的方法可以先从pip中安装包，之后通过pip list | grep client的方式获取client的源代码。这里面提供我在Kilo中使用的版本依赖。</p>

<pre><code class="bash devstack/localrc">KEYSTONECLIENT_BRANCH=1.3.1
CINDERCLIENT_BRANCH=1.1.1
GLANCECLIENT_BRANCH=0.17.1
HEATCLIENT_BRANCH=0.4.0
NEUTRONCLIENT_BRANCH=2.4.0
NOVACLIENT_BRANCH=2.23.0
SWIFTCLIENT_BRANCH=2.4.0

# client code
LIBS_FROM_GIT=python-keystoneclient,python-glanceclient,python-novaclient,python-neutronclient,python-swiftclient,python-cinderclient
</code></pre>

<h2>Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift</h2>

<pre><code class="bash devstack/localrc"># Ceilometer
enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api
enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator

# Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Trove
enable_service trove tr-api tr-tmgr tr-cond

# Sahara
enable_service sahara

# Swift
enable_service s-proxy s-object s-container s-account
SWIFT_REPLICAS=1
SWIFT_HASH=011688b44136573e209e
</code></pre>

<h2>Scenario 7: 安装Ceph</h2>

<pre><code class="bash devstack/localrc"># Ceph
ENABLED_SERVICES+=,ceph
CEPH_LOOPBACK_DISK_SIZE=200G
CEPH_CONF=/etc/ceph/ceph.conf
CEPH_REPLICAS=1

# Glance - Image Service
GLANCE_CEPH_USER=glance
GLANCE_CEPH_POOL=glance-pool

# Cinder - Block Device Service
CINDER_DRIVER=ceph
CINDER_CEPH_USER=cinder
CINDER_CEPH_POOL=cinder-pool
CINDER_CEPH_UUID=1b1519e4-5ecd-11e5-8559-080027f18a73
CINDER_BAK_CEPH_POOL=cinder-backups
CINDER_BAK_CEPH_USER=cinder-backups
CINDER_ENABLED_BACKENDS=ceph
CINDER_ENABLED_BACKENDS=ceph

# Nova - Compute Service
NOVA_CEPH_POOL=nova-pool
</code></pre>

<h2>Scenario 8: 安装Murano</h2>

<p>想通过这个例子演示，对于一个新的OpenStack项目，如何使用Devstack尝鲜。</p>

<pre><code class="bash bash">cd /opt/stack.kilo
git clone https://github.com/openstack/murano --branch=stable/kilo
cd murano/contrib/devstack
cp lib/murano ${DEVSTACK_DIR}/lib
cp lib/murano-dashboard ${DEVSTACK_DIR}/lib
cp extras.d/70-murano.sh ${DEVSTACK_DIR}/extras.d
</code></pre>

<pre><code class="plain devstack/localrc"># Enable Neutron
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron

# Enable Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Enable Murano
enable_service murano murano-api murano-engine
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Kilo)Devstack Kilo版本localrc推荐]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/11/best-localrc-for-devstack-kilo/"/>
    <updated>2015-05-11T11:33:44+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/11/best-localrc-for-devstack-kilo</id>
    <content type="html"><![CDATA[<p>Devstack在Kilo版本中发生了一些变化，其中一个commit(279cfe75198c723519f1fb361b2bff3c641c6cef)的就是优化默认启动的程序，尽量减小对硬件的要求。如果不修改默认的配置进行安装，会产生一些问题，例如VNC无法打开，Heat模块没有加载等。这里给出一个个人比较常用的localrc，供大家参考。该配置在Ubuntu 14.04 Server LTS进行了测试。</p>

<!-- more -->


<p>该配置文件中开启了所有的OpenStack的核心模块，以下几点需要注意：</p>

<ul>
<li>为了运行Neutron，服务器必须是双网卡，否则外网不会通</li>
<li>我的实验网段为200.21.0.0/16，eth0的IP为200.21.1.61，eth1与eth0为同一网段</li>
<li>eth1为公网访问网络，floating网络范围200.21.50.1/24，配置的GATEWAY为200.21.50.2</li>
<li>保证eth1所处的网段能够连接外网，但是配置为manual模式，配置如下：</li>
</ul>


<pre><code class="plain /etc/network/interface">auto eth1
iface eth1 inet manual
up ifconfig $IFACE 0.0.0.0 up
down ifconfig $IFACE 0.0.0.0 down
</code></pre>

<ul>
<li>localrc的配置</li>
</ul>


<pre><code class="plain localrc"># Misc
ADMIN_PASSWORD=sysadmin
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=$ADMIN_PASSWORD

# Target Path
DEST=/opt/stack.kilo

# Enable Logging
LOGFILE=$DEST/logs/stack.sh.log
VERBOSE=True
LOG_COLOR=True
SCREEN_LOGDIR=$DEST/logs

# Nova
enable_service n-novnc n-cauth

# Neutron
disable_service n-net
ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron
ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas

# Ceilometer
enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api
enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator

# Enable Heat
enable_service heat h-api h-api-cfn h-api-cw h-eng

# Trove
enable_service trove tr-api tr-tmgr tr-cond

# Sahara
enable_service sahara

#FIXED_RANGE=10.0.0.0/24
HOST_IP=200.21.1.61
FLOATING_RANGE=200.21.50.1/24
PUBLIC_NETWORK_GATEWAY=200.21.50.2
Q_FLOATING_ALLOCATION_POOL=start=200.21.50.100,end=200.21.50.150
</code></pre>

<ul>
<li><p>确认br-ex是否存在
<code>plain sudo ovs-vsctl show
Bridge br-ex
  Port br-ex
      Interface br-ex
          type: internal
  Port "qg-7ec5be02-69"
      Interface "qg-7ec5be02-69"
          type: internal
ovs_version: "2.0.2"
</code></p></li>
<li><p>将eth1作为br-ex的接口
<code>bash
sudo ovs-vsctl add-port br-ex eth1
</code></p></li>
</ul>


<pre><code class="plain sudo ovs-vsctl show">Bridge br-ex
    Port br-ex
        Interface br-ex
            type: internal
    Port "qg-7ec5be02-69"
        Interface "qg-7ec5be02-69"
            type: internal
    Port "eth1"
        Interface "eth1"
ovs_version: "2.0.2"
</code></pre>
]]></content>
  </entry>
  
</feed>
