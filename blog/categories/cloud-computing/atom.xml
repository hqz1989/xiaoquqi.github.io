<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud Computing | RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/blog/categories/cloud-computing/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2016-10-01T16:15:03+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[深度解读OpenStack Newton国内代码贡献]]></title>
    <link href="http://xiaoquqi.github.io/blog/2016/10/01/contricution-in-newton/"/>
    <updated>2016-10-01T09:00:49+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2016/10/01/contricution-in-newton</id>
    <content type="html"><![CDATA[<p>今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。</p>

<p>这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。</p>

<p>最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。</p>

<p>还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。</p>

<p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p>

<p>OpenStack Mitaka深度解读请见：<a href="http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/">http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/</a></p>

<!-- more -->


<h2>Release项目简介</h2>

<p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/">http://releases.openstack.org/</a></p>

<p>下面是最近三个版本Release的详细对比：</p>

<p>让我们先来关注一些这次Release中的三个新项目：</p>

<p><img class="center" src="/images/blogs/contribution-in-newton-projects.png"></p>

<h3>Panko(计量服务事件消息存储)</h3>

<p>Panko是计量模块中的一部分，主要是为了计量模块提供事件消息存储，众所周知，在上一个OpenStack Release中，Ceilometer被一分为三，分别为aodh(告警服务)/Gnocchi(基于时间的数据库服务)/Ceilometer，为了解决当前Ceilometer中存在的性能问题，提高更好的扩展性。</p>

<p>现在Panko的文档并不是很丰富，如果有需要了解更多详细内容的，可以关注Developer的文档：<a href="http://docs.openstack.org/developer/panko/">http://docs.openstack.org/developer/panko/</a></p>

<h3>Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)</h3>

<p>Vitrage是一个OpenStack RCA(Root Cause Analysis)服务，用于组织、分析和扩展OpenStack的告警和事件，在真正的问题发生前找到根本原因。</p>

<p>众所周知，OpenStack平台最大的优势来自于架构的可扩展性，这也是OpenStack能够在基础架构曾一枝独秀的重要原因。分布式架构最大的优势在于扩展，但是过于灵活的扩展性为运维带来的极大的困难，所以Vitrage的出现在一定程度上缓解了OpenStack运维上的痛点。</p>

<p>我们来简单看一下他的架构，更多详细的介绍请查看WIKI：<a href="https://wiki.openstack.org/wiki/Vitrage">https://wiki.openstack.org/wiki/Vitrage</a></p>

<p><img class="center" src="/images/blogs/contribution-in-newton-vitrage-architecture.png"></p>

<h3>Watcher(OpenStack平台优化服务)</h3>

<p>从名字上看，我们并不能理解这个模块的具体左右，我们通过文档中用户应用场景来了解一下Watcher的作用：</p>

<p>作为一名云平台的管理员在云平台使用一段时间后，想根据一些物理特性对云平台虚拟机的分布进行重新平衡，例如服务器的温度、电源的状态等信息，那么这时候就可以通过watcher，利用Nova虚拟机的在线迁移对整个数据中心云平台的虚拟机进行一些优化处理，从而达到某种平衡。我认为这其实类似于VMWare的DRS功能。</p>

<p>当然Watcher还有更多的应用场景，更多详细的介绍请查看：<a href="https://wiki.openstack.org/wiki/Watcher">https://wiki.openstack.org/wiki/Watcher</a></p>

<p>我们来简单看一下他的架构，更多架构方面的详细的介绍请查看：<a href="http://docs.openstack.org/developer/watcher/architecture.html">http://docs.openstack.org/developer/watcher/architecture.html</a></p>

<p><img class="center" src="/images/blogs/contribution-in-newton-watcher-architecture.svg"></p>

<h2>社区贡献总体分析</h2>

<p>本次统计的方法仍然为commits和blueprints的方式，统计范围为stackalystatics默认统计的全部项目。</p>

<p>从总体参与的公司和贡献者来说，都有所上升，这也不难理解，随着OpenStack模块增加，势必涉及更多的领域，所以更多的公司加入了这个生态圈。</p>

<p><img class="center" src="/images/blogs/contribution-in-newton-companies-contributors.png"></p>

<p>从commits角度进行分析，传统几大好强几乎没有变化，日本的Fujitsu在commits上挤掉了华为，进入了前十名的位置。模块方面，核心模块的贡献仍然位于前十名，也说明是应用最多的模块，所以才会不断的发现问题。本次统计的总项目数量为629个，可能stackalytics在统计策略上有所调整。</p>

<p><img class="center" src="/images/blogs/contribution-in-newton-companies-modules-commits.png"></p>

<p>单从commits角度统计其实有失偏颇，真正能够体现公司在OpenStack实力的指标应该是Blueprints。我认为完成Blueprints至少具备三个必要条件：英语要好、在社区有一定的影响力、架构设计能力。这些都是需要不断在社区进行积累和沉淀的。</p>

<p>本次release周期内，能够完成Blueprints的公司为64个，国内的华为和九州云均进入前10名，排名比较靠前的国内企业还包括：Easystack、中兴。</p>

<p>完成Blueprints最多的仍然是核心模块，排在第二名的是kolla，看来在上一个周期中，kolla项目的活跃程度是较高的。</p>

<p><img class="center" src="/images/blogs/contribution-in-newton-companies-modules-blueprints.png"></p>

<h2>OpenStack国内社区分析</h2>

<p>看完总体的状况，再来关注一些国内的贡献情况，与去年相比，今年上榜的国内企业达到了21家，创历年之最，比去年的15家企业整整多了7家，并且我们发现在这些新增企业中大部分都是提供企业服务的公司，说明OpenStack在国内的企业级市场开始站稳脚跟。下面我们来做一个详细的分析：</p>

<h3>贡献企业</h3>

<p>在最近的三个版本连续对社区有贡献的企业包括：华为，Easystack，九州云，海云捷迅，华三，Unitedstack，乐视，中国移动和北京休伦科技(Huron)。</p>

<p>本次爬升最快的企业：中兴，从108位攀升至13位。</p>

<p>本次统计新增的7家企业：云途腾(t2cloud)，大唐高鸿数据(GohighSec)，华云数据，烽火通信，爱数，北京国电通，云英，中国银联，赛特斯信息。</p>

<p>本次排名中OpenStack的直接用户：中国移动和中国银联。中国移动更是参选了OpenStack SuperAward的评比，预祝他们能顺利当选。</p>

<p><img class="center" src="/images/blogs/contribution-in-newton-china-companies.png"></p>

<h3>人员投入分析</h3>

<p>我们再来从人员投入来分析贡献情况一下：</p>

<ul>
<li>投入人数最多的仍然是华为，有65名工程师贡献了本次的commits</li>
<li>中兴无疑是本次人员投入增长最快的，从6名工程师一下子扩张到61名，也是唯一能和华为抗衡的</li>
<li>超过2位数人员投入的包括，Easystack，九州云和Unitedstack，另外海云捷迅有9人，华三有8人，中国移动有7人参与社区贡献</li>
</ul>


<p><img class="center" src="/images/blogs/contribution-in-newton-companies-effort.png"></p>

<h3>模块贡献分析</h3>

<p>从模块贡献角度来分析，国内企业的贡献仍然没有出现一个统一的趋势，与Mitaka Release相比，贡献涉及模块的总量从192个增加至Newton Release的246个，一方面说明OpenStack本身模块的增加，也说明国内企业使用或开发OpenStack在方向上的多元化。</p>

<p>从贡献的模块来看，华为主导的dargonflow高居榜首，紧随其后的是手册和clients两个项目，随后的贡献集中在OpenStack的核心模块，与Docker相关的几个模块中。Kolla项目无疑是最近关注的热点，随着Docker的快速发展，OpenStack和Docker不断碰撞出新的火花。</p>

<p><img class="center" src="/images/blogs/contribution-in-newton-modules.png"></p>

<h3>投入产出比</h3>

<p>这个问题仍然是比较敏感的问题，只有每个公司的CEO能够回答这个问题，这里面我从融资的角度来回顾一下2015至2016年之间在OpenStack领域发生过什么。</p>

<ul>
<li>2015年9月17日，英特尔投资部门披露了此前投资的中国8家公司名单。投资总额达6700万美元，领域覆盖了新材料、智能设备、物联网、云服务等领域。其中包含九州云和海云捷迅两家OpenStack企业。(<a href="http://tech.qq.com/a/20150917/038604.htm">http://tech.qq.com/a/20150917/038604.htm</a>)</li>
<li>2015年10月17日，中国最大的独立公有云提供商UCloud和全球领先的OpenStack厂商Mirantis在东京的OpenStack峰会上正式宣布成立合资公司UMCloud，以求更好的在中国做OpenStack。(<a href="http://www.doit.com.cn/article/1027290510.html">http://www.doit.com.cn/article/1027290510.html</a>)</li>
<li>2015年12月16日，UnitedStack有云宣布完成C轮融资，该轮融资由思科和红杉资本投资，具体数额未公布(<a href="http://www.infoq.com/cn/news/2015/12/unitedstack-financing">http://www.infoq.com/cn/news/2015/12/unitedstack-financing</a>)</li>
<li>2016年5月20日，云途腾(T2Cloud)完成A轮3650万融资(<a href="http://iimedia.cn/42262.html">http://iimedia.cn/42262.html</a>)</li>
<li>2016年9月21日，腾讯与海云捷迅昨日下午在京共同宣布达成战略投资合作关系，海云捷迅接受腾讯的战略投资(<a href="http://www.36dsj.com/archives/62353">http://www.36dsj.com/archives/62353</a>)</li>
</ul>


<h2>总结</h2>

<p>回到开篇的那句话，OpenStack贡献量只能反应中国企业对于开源项目的重视程度，无法反应真实的用户需求。VMWare花了将近10年的时间教育用户，说服用户把应用从物理机迁移至虚拟机。OpenStack从2011年出生到现在也仅仅短短的5年，可见OpenStack还有很长的路要走。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度解读OpenStack Mitaka国内代码贡献]]></title>
    <link href="http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/"/>
    <updated>2016-04-07T23:19:39+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka</id>
    <content type="html"><![CDATA[<p>转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。</p>

<p>去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。</p>

<p>这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。</p>

<p>对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。</p>

<p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p>

<!-- more -->


<h2>Release项目简介</h2>

<p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/">http://releases.openstack.org/</a></p>

<p>在Big Tent公布之后，OpenStack的项目被分为Core Projects和Big Tent Projects。</p>

<p><img class="center" src="/images/blogs/contribution-in-mitaka-big-tent.jpg"></p>

<p>让我们来看一下在Mitaka版本中，多了哪些新项目。</p>

<ul>
<li>几个与Docker相关的项目被发布出来，magnum, senlin, solum</li>
<li>数据备份容灾的项目：freezer</li>
<li>计费的项目：cloudkitty</li>
<li>NFV相关的项目：tracker</li>
<li>监控相关的项目：monasca</li>
</ul>


<p>关于这些新项目的一些介绍，我将放在另外一篇博客里，敬请关注。</p>

<p><img class="center" src="/images/blogs/contribution-in-mitaka-projects.png"></p>

<h2>社区贡献总体分析</h2>

<p>本次统计的方法仍然为commits的方式，统计范围为stackalystatics默认统计的全部项目。</p>

<p>从总体参与的公司数量来看，Mitaka版本略有下降，但是参与的人数多了100多人。</p>

<p><img class="center" src="/images/blogs/contribution-in-mitaka-companies-contributors.png"></p>

<p>整个社区的公司贡献排名上没有明显的变化，传统的几大豪强仍然霸占公司排名的前十位，华为表现依然强劲，是中国区唯一能进入前十名的公司。</p>

<p>在模块方面，整体统计的绝大部分比例已经被others所占据，说明在Big Tent计划下，OpenStack正在朝更多元化的方向演进。在Mitaka排名前十位的项目中，fuel相关的两个项目都进入了前十，说明fuel在OpenStack部署的地位已经越来越重要了。同时，核心项目中的nova，neutron，cinder项目仍然在前十名的范围内，贡献量基本保持不变。值得一提的是，在Mitaka统计的项目数量已经从Liberty的708个增长到了829个，可见在短短的6个月内，OpenStack社区的蓬勃发展。</p>

<p><img class="center" src="/images/blogs/contribution-in-mitaka-companies-modules.png"></p>

<h2>OpenStack国内社区分析</h2>

<p>看完了整体统计，我们再回到国内，因为已经有文章做了我在Liberty时候的分析，所以这里我换个角度来看国内的社区贡献，首先是统计排名的变化。</p>

<h3>贡献企业</h3>

<p>在Liberty中，有13家国内企业为社区做了贡献，在Mitaka中这个数量增加到了15家企业，这里简单的将这些企业做了一下分类：</p>

<ul>
<li>互联网用户：乐视、新浪、网易</li>
<li>电信用户：中国移动</li>
<li>传统IT服务商：华为、中兴、华三</li>
<li>私有云服务商：Easystack、九州云、海云捷迅、北京有云、麒麟云、UMCloud、象云、Huron(休伦科技)</li>
</ul>


<p><img class="center" src="/images/blogs/contribution-in-mitaka-china-companies.png"></p>

<h3>行业分析</h3>

<p>通过行业的分析我们可以看出，国内的主要贡献仍然来自私有云服务商和传统IT服务商，换言之来自于以OpenStack提供产品或者服务的公司。厂商们贡献的目的很明确，主要为了展示自身在开源项目中的积累和专家形象。而用户的贡献主要来自平时在使用OpenStack时候遇到Bug，就是在实际应用过程中出现的问题。</p>

<p><img class="center" src="/images/blogs/contribution-in-mitaka-china-by-industry.png"></p>

<h3>人员投入分析</h3>

<p>单纯的社区贡献排名的比较仅仅是一个维度，下面我们来看一下各个公司的人员投入情况：</p>

<ul>
<li>排名前几位的公司对社区投入的人力基本都是两位数，相对于Liberty版本，人员均有所增加</li>
<li>在人均贡献投入上，99cloud是国内最高的，平均达到了59天，甚至超过了华为，这个统计不仅仅包含了代码贡献，还包含了邮件、Review、Blueprint的时间，基本可以衡量每个公司在OpenStack社区贡献方面的投入力量</li>
<li>人员投入来看，Easystack和中国移动无疑是最下本的两家，Easystack从Liberty的3人，增长到了23人，一下子增加了20人；中国移动也从最初的4个人，增加到了13个人，可见中国移动未来对OpenStack的野心</li>
</ul>


<p><img class="center" src="/images/blogs/contribution-in-mitaka-companies-effort.png"></p>

<h3>贡献模块分析</h3>

<p>从模块的角度进行统计，国内企业的贡献情况并未出现一个统一的趋势，总体的贡献项目为193个，项目几乎涉及OpenStack所有最活跃的项目，从排名前十的项目来看：</p>

<ul>
<li>得益于华为的主导，dargonflow项目的贡献量超高</li>
<li>紧随其后的，也是当下的热点，容器相关的两个项目</li>
<li>几大OpenStack老模块贡献量也高居前十位，说明这些模块是在解决方案中使用频率较高的</li>
</ul>


<p><img class="center" src="/images/blogs/contribution-in-mitaka-modules.png"></p>

<h3>投入产出比</h3>

<p>这是一个很敏感的话题，每个公司对社区的投入到底换来多少项目上的回报呢？可能这只有每个公司的CEO能够回答的问题了。我在这里就不多做过多的分析，留给大家充分讨论的空间吧。</p>

<h2>总结</h2>

<p>刚刚结束在南京的OpenStack开发培训，也了解到5G的通信网络上已经确定引入了OpenStack，虽然我说不清楚他的具体用途，但是我相信这对OpenStack这个项目、社区是一个重大的利好消息。我也相信，通过国内企业的集体努力，一定能让OpenStack在中国遍地开花结果。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Consul主要使用场景]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/12/24/use-consul/"/>
    <updated>2015-12-24T18:07:24+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/12/24/use-consul</id>
    <content type="html"><![CDATA[<p>假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：<a href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/">http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/</a></p>

<p>这篇文章里主要介绍Consul的使用场景，服务和健康检查。</p>

<!-- more -->


<h2>Service</h2>

<p>服务注册有点像OpenStack Keystone的Endpoints，可以通过API方式查询到所有服务的端点信息。</p>

<p>在Agent的节点上添加一个service，之后重启服务。</p>

<ul>
<li>添加一个服务</li>
</ul>


<pre><code>$ echo '{"service": {"name": "web", "tags": ["rails"], "port": 80}}' \
    &gt;/etc/consul.d/web.json
</code></pre>

<ul>
<li>重启agent</li>
</ul>


<p>重新加载新的服务并不需要杀死进程重启服务，只需要给进程直接发送一个SIGHUP。</p>

<pre><code>$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk '{print $2}')
</code></pre>

<ul>
<li>日志输出</li>
</ul>


<p>从输出的日志上都可以看到加载了新的服务web。</p>

<pre><code>==&gt; Caught signal: hangup
==&gt; Reloading configuration...
==&gt; WARNING: Expect Mode enabled, expecting 3 servers
    2015/12/24 12:01:11 [INFO] agent: Synced service 'web'
</code></pre>

<ul>
<li>利用API查询</li>
</ul>


<p>我们在任意节点上利用REST API查看服务。</p>

<pre><code>$ curl http://localhost:8500/v1/catalog/service/web
</code></pre>

<pre><code>[{"Node":"server1.consul.com","Address":"200.21.1.101","ServiceID":"web","ServiceName":"web","ServiceTags":["rails"],"ServiceAddress":"","ServicePort":80}]
</code></pre>

<h2>Health Check</h2>

<p>健康检查的方法主要是通过运行一小段脚本的方式，根据运行的结果判断检查对象的健康状况。所以可以通过任意语言定义这个脚本，脚本运行将通过和consul执行的相同用户执行。</p>

<ul>
<li>添加一个健康检查</li>
</ul>


<p>每30秒ping google.com</p>

<pre><code>$ echo '{"check": {"name": "ping",
  "script": "ping -c1 google.com &gt;/dev/null", "interval": "30s"}}' \
    &gt; /etc/consul.d/ping.json
</code></pre>

<p>为刚才的服务添加健康检查</p>

<pre><code>$ echo '{"service": {"name": "web", "tags": ["rails"], "port": 80,
  "check": {"script": "curl localhost &gt;/dev/null 2&gt;&amp;1", "interval": "10s"}}}' \
    &gt; /etc/consul.d/web.json
</code></pre>

<ul>
<li>重启agent</li>
</ul>


<pre><code>$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk '{print $2}')
</code></pre>

<ul>
<li>日志输出</li>
</ul>


<p>从输出的日志上都可以看到加载了新的服务web。</p>

<pre><code>==&gt; Caught signal: hangup
==&gt; Reloading configuration...
==&gt; WARNING: Expect Mode enabled, expecting 3 servers
    2015/12/24 12:43:56 [INFO] agent: Synced service 'web'
    2015/12/24 12:43:56 [INFO] agent: Synced check 'ping'
</code></pre>

<p>经过一段时间后出现了critical和warning日志</p>

<pre><code>    2015/12/24 12:43:58 [WARN] agent: Check 'service:web' is now critical
    2015/12/24 12:44:08 [WARN] agent: Check 'ping' is now warning
</code></pre>

<ul>
<li>利用API查询</li>
</ul>


<p>Health check的状态包含了很多种，有any, unkown, passing, warning, critical。any包含了所有状态。</p>

<pre><code>$ curl http://localhost:8500/v1/health/state/critical
</code></pre>

<pre><code>[{"Node":"server1.consul.com","CheckID":"service:web","Name":"Service 'web' check","Status":"critical","Notes":"","Output":"","ServiceID":"web","ServiceName":"web"}]
</code></pre>

<h2>参考文档</h2>

<ul>
<li><a href="http://www.consul.io/docs/agent/http/catalog.html">http://www.consul.io/docs/agent/http/catalog.html</a></li>
<li><a href="http://www.consul.io/docs/agent/http/health.html">http://www.consul.io/docs/agent/http/health.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Consul的安装方法]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/"/>
    <updated>2015-12-07T10:00:13+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/12/07/consul-installation</id>
    <content type="html"><![CDATA[<h2>什么是Consul?</h2>

<p>Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。</p>

<!-- more -->


<h2>安装Consul</h2>

<p>Consul的安装比较简单，下载之后直接解压缩就可以了，下载地址：<a href="https://www.consul.io/downloads.html">https://www.consul.io/downloads.html</a></p>

<p>我们把consul直接放在/usr/local/bin目录中。</p>

<h2>Consul Server</h2>

<pre><code>$ /usr/local/bin/consul agent -server -bootstrap-expect 3 -data-dir /tmp/consul -node=server1 -bind=10.10.10.10
</code></pre>

<h3>参数说明</h3>

<ul>
<li>-server - Serve模式</li>
<li>-bootstrap-expect - Server数量</li>
<li>-data-dir - 数据目录</li>
<li>-node - Node名称</li>
<li>-bind - 集群通讯地址</li>
</ul>


<h3>输出</h3>

<pre><code>==&gt; WARNING: Expect Mode enabled, expecting 3 servers
==&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1
==&gt; Starting Consul agent...
==&gt; Starting Consul agent RPC...
==&gt; Consul agent running!
         Node name: 'server1.consul.com'
        Datacenter: 'dc1'
            Server: true (bootstrap: false)
       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)
      Cluster Addr: 200.21.1.101 (LAN: 8301, WAN: 8302)
    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false
             Atlas: &lt;disabled&gt;

==&gt; Log data will now stream in as it occurs:

    2015/12/23 03:13:36 [WARN] memberlist: Binding to public address without encryption!
    2015/12/23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101
    2015/12/23 03:13:36 [WARN] memberlist: Binding to public address without encryption!
    2015/12/23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com.dc1 200.21.1.101
    2015/12/23 03:13:36 [INFO] raft: Node at 200.21.1.101:8300 [Follower] entering Follower state
    2015/12/23 03:13:36 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/23 03:13:36 [INFO] consul: adding server server1.consul.com.dc1 (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/23 03:13:36 [ERR] agent: failed to sync remote state: No cluster leader
    2015/12/23 03:13:37 [WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.
    2015/12/23 03:13:51 [ERR] agent: failed to sync remote state: No cluster leader
==&gt; Newer Consul version available: 0.6.0
    2015/12/23 03:14:17 [ERR] agent: failed to sync remote state: No cluster leader
</code></pre>

<h3>查看成员</h3>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
</code></pre>

<h2>Consul Agent</h2>

<pre><code>$ /usr/local/bin/consul agent -data-dir /tmp/consul -node=agent1 -bind=10.10.10.100 -config-dir /etc/consul.d
</code></pre>

<ul>
<li>输出</li>
</ul>


<pre><code>==&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1
==&gt; Starting Consul agent...
==&gt; Starting Consul agent RPC...
==&gt; Consul agent running!
         Node name: 'agent1.consul.com'
        Datacenter: 'dc1'
            Server: false (bootstrap: false)
       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)
      Cluster Addr: 200.21.1.201 (LAN: 8301, WAN: 8302)
    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false
             Atlas: &lt;disabled&gt;

==&gt; Log data will now stream in as it occurs:

    2015/12/24 08:09:51 [WARN] memberlist: Binding to public address without encryption!
    2015/12/24 08:09:51 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201
    2015/12/24 08:09:51 [ERR] agent: failed to sync remote state: No known Consul servers
    2015/12/24 08:09:56 [INFO] agent.rpc: Accepted client: 127.0.0.1:42794
    2015/12/24 08:09:56 [INFO] agent: (LAN) joining: [200.21.1.101 200.21.1.102 200.21.1.103]
    2015/12/24 08:09:56 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101
    2015/12/24 08:09:56 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/24 08:09:58 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:10:02 [INFO] agent: (LAN) joined: 1 Err: &lt;nil&gt;
    2015/12/24 08:10:02 [INFO] agent.rpc: Accepted client: 127.0.0.1:42800
==&gt; Newer Consul version available: 0.6.0
    2015/12/24 08:10:21 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:10:22 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:10:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:01 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:11:02 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:23 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:41 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:11:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:12:12 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:12:21 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:12:36 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:13:01 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:13:03 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
</code></pre>

<ul>
<li>server日志输出</li>
</ul>


<pre><code>    2015/12/24 08:09:58 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201
</code></pre>

<h3>查看成员</h3>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1
</code></pre>

<h2>最终结果</h2>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1
agent2.consul.com   200.21.1.202:8301  alive   client  0.5.2  2         dc1
server2.consul.com  200.21.1.102:8301  alive   server  0.5.2  2         dc1
server3.consul.com  200.21.1.103:8301  alive   server  0.5.2  2         dc1
agent3.consul.com   200.21.1.203:8301  alive   client  0.5.2  2         dc1
</code></pre>

<h2>参考文档</h2>

<ul>
<li><a href="https://www.consul.io/intro/getting-started/install.html">https://www.consul.io/intro/getting-started/install.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Grafana+Diamond+Graphite构造完美监控面板]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/12/01/use-grafana-to-monitor-your-cluster/"/>
    <updated>2015-12-01T07:59:46+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/12/01/use-grafana-to-monitor-your-cluster</id>
    <content type="html"><![CDATA[<p>服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。</p>

<p>本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：</p>

<p><img class="center" src="/images/blogs/grafana-screenshot.png" width="800"></p>

<!-- more -->


<h2>安装及配置Dimaond</h2>

<p>安装Diamond最直接和简单的方法就是自己编译RPM或者DEB的安装包, Diamond在这方面提供了比较好的支持。</p>

<pre><code class="bash bash"># cd /root
# yum install -y git rpm-build python-configobj python-setuptools
# git clone https://github.com/python-diamond/Diamond
# cd Diamond
# make rpm
# cd dist
# rpm -ivh diamond-*.noarch.rpm
</code></pre>

<p>默认情况下，Diamond开启了基本的监控信息，包括CPU、内存、磁盘的性能数据。当然，我们可以通过配置启动相应的监控项，也能通过自定义的方式进行相应的扩展。这里，我们在/etc/diamond/collectors加载额外的插件，下面的例子中开启了网络的监控。</p>

<pre><code class="bash bash"># cp -f /etc/diamond/diamond.conf.example /etc/diamond/diamond.conf

# cat &lt;&lt; EOF | tee -a /etc/diamond/diamond.conf
[configs]
path = "/etc/diamond/collectors/"
extension = ".conf"
EOF

# cat &lt;&lt; EOF | tee /etc/diamond/collectors/net.conf
[collectors]

[[NetworkCollector]]
enabled = True
EOF
</code></pre>

<p>那么到目前为止，Diamond的基本安装和配置已经完成，但是现在只是简单的采集数据，并没有指明数据要发送给谁，所以下一步我们来开始配置Graphite。</p>

<h2>安装及配置Graphite</h2>

<p>Graphite主要做两件事情：按照时间存储数据、生成图表，在我们的场景里面，实质上就是把Graphite作为数据源给Grafana提供数据。另外还需要安装的是carbon，负责通过网络接受数据并保存到后端存储中；另外还需要whisper，负责生成Graphite样式的基于文件的时间序列的数据库。</p>

<h3>安装软件包</h3>

<pre><code class="bash bash"># yum install -y graphite-web graphite-web-selinux
# yum install -y mysql mysql-server MySQL-python
# yum install -y python-carbon python-whisper
</code></pre>

<h3>配置MySQL</h3>

<pre><code class="bash bash"># /etc/init.d/mysqld start

# mysql -e "CREATE DATABASE graphite;" -u root
# mysql -e "GRANT ALL PRIVILEGES ON graphite.* TO 'graphite'@'localhost' IDENTIFIED BY 'sysadmin';" -u root
# mysql -e 'FLUSH PRIVILEGES;' -u root
</code></pre>

<h3>配置Graphite</h3>

<ul>
<li>local setting</li>
</ul>


<pre><code class="bash /etc/graphite-web/local_settings.py"># SECRET_KEY=$(md5sum /etc/passwd | awk {'print $1'})

# echo "SECRET_KEY = '$SECRET_KEY'" | tee -a /etc/graphite-web/local_settings.py
# echo "TIME_ZONE = 'Asia/Shanghai'" | tee -a /etc/graphite-web/local_settings.py

# cat &lt;&lt; EOF | tee -a /etc/graphite-web/local_settings.py
DATABASES = {
    'default': {
        'NAME': 'graphite',
        'ENGINE': 'django.db.backends.mysql',
        'USER': 'graphite',
        'PASSWORD': 'sysadmin',
    }
}
EOF

# cd /usr/lib/python2.6/site-packages/graphite
# ./manage.py syncdb --noinput

# echo "from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'admin@hihuron.com', 'sysadmin')" | ./manage.py shell
</code></pre>

<ul>
<li>Apache配置</li>
</ul>


<pre><code class="bash /etc/httpd/conf.d/graphite-web.conf">Listen 0.0.0.0:10000
&lt;VirtualHost *:10000&gt;
    ServerName graphite-web
    DocumentRoot "/usr/share/graphite/webapp"
    ErrorLog /var/log/httpd/graphite-web-error.log
    CustomLog /var/log/httpd/graphite-web-access.log common
    Alias /media/ "/usr/lib/python2.6/site-packages/django/contrib/admin/media/"

    WSGIScriptAlias / /usr/share/graphite/graphite-web.wsgi
    WSGIImportScript /usr/share/graphite/graphite-web.wsgi process-group=%{GLOBAL} application-group=%{GLOBAL}

    &lt;Location "/content/"&gt;
        SetHandler None
    &lt;/Location&gt;

    &lt;Location "/media/"&gt;
        SetHandler None
    &lt;/Location&gt;
&lt;/VirtualHost&gt;
</code></pre>

<ul>
<li>Diamond配置</li>
</ul>


<pre><code class="bash bash"># HOST_IP=$(ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\.){3}[0-9]*).*/\2/p' | head -1)

# sed  -i "/^\[\[GraphiteHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf
# sed  -i "/^\[\[GraphitePickleHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf
</code></pre>

<h3>启动服务</h3>

<pre><code class="bash bash"># service carbon-cache restart
# service httpd restart
# service diamond restart
</code></pre>

<h2>安装和配置Grafana</h2>

<p>Grafana最主要的功能就是对数据的呈现，基于一切可提供time series的后台服务。这里面我们使用Graphite为Grafana提供数据。</p>

<h3>安装及配置</h3>

<pre><code class="bash bash"># yum install -y nodejs
# rpm -ivh https://grafanarel.s3.amazonaws.com/builds/grafana-2.5.0-1.x86_64.rpm
# sudo /sbin/chkconfig --add grafana-server
# sed -i 's/^;http_port = 3000$/http_port = 10001/g' /etc/grafana/grafana.ini
# sudo service grafana-server start
</code></pre>

<h3>添加datasource</h3>

<p>Grafana提供了非常丰富的REST API，我们不仅可以直接利用Grafana作为数据呈现层，还可以利用REST API直接将Grafana的Graph集成在我们的应用中。下面我们利用REST API为Grafana添加datasource。</p>

<pre><code class="bash bash"># curl -i 'http://admin:admin@localhost:10001/api/datasources' -X POST -H "Accept: application/json" -H "Content-Type: application/json" -d '{"name": "graphite", "type": "graphite", "url": "http://localhost:10000", "access": "proxy", "basicAuth": false}'
</code></pre>

<h2>Ceph监控</h2>

<h3>修改ceph脚本兼容性</h3>

<p>Diamond是基于Python开发的，但是由于CentOS 6.5的Python版本较低(2.6)，所以直接使用社区版本的Ceph监控时，会导致错误。可以通过简单的修改进行修复。</p>

<pre><code class="python /usr/share/diamond/collectors/ceph/ceph.py">    def _get_stats_from_socket(self, name):
        """Return the parsed JSON data returned when ceph is told to
        dump the stats from the named socket.

        In the event of an error error, the exception is logged, and
        an empty result set is returned.
        """
        try:
            #json_blob = subprocess.check_output(
            #    [self.config['ceph_binary'],
            #     '--admin-daemon',
            #     name,
            #     'perf',
            #     'dump',
            #     ])
            cmd = [
                 self.config['ceph_binary'],
                 '--admin-daemon',
                 name,
                 'perf',
                 'dump',
            ]
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
            json_blob = process.communicate()[0]
</code></pre>

<h3>增加对ceph osd perf监控</h3>

<p>在实际运维Ceph过程中，ceph osd perf是一个非常重要的指令，能够观察出集群中磁盘的latency的信息，通过观察变化，可以辅助判断磁盘出现性能问题。Diamond的设计中，每个Diamond Agent只会采集自己本机的指标，所以我们在添加的时候，只需要在一个节点上增加这个监控就可以了。在ceph.py中结尾处新增加一个类。</p>

<pre><code class="python /usr/share/diamond/collectors/ceph/ceph.py">class CephOsdCollector(CephCollector):

    def _get_stats(self):
        """Return the parsed JSON data returned when ceph is told to
        dump the stats from the named socket.

        In the event of an error error, the exception is logged, and
        an empty result set is returned.
        """
        try:
            #json_blob = subprocess.check_output(
            #    [self.config['ceph_binary'],
            #     '--admin-daemon',
            #     name,
            #     'perf',
            #     'dump',
            #     ])
            cmd = [
                 self.config['ceph_binary'],
                 'osd',
                 'perf',
                 '--format=json',
            ]
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
            json_blob = process.communicate()[0]
        except subprocess.CalledProcessError, err:
            self.log.info('Could not get stats from %s: %s',
                          name, err)
            self.log.exception('Could not get stats from %s' % name)
            return {}

        try:
            json_data = json.loads(json_blob)
        except Exception, err:
            self.log.info('Could not parse stats from %s: %s',
                          name, err)
            self.log.exception('Could not parse stats from %s' % name)
            return {}

        return json_data

    def _publish_stats(self, stats):
        """Given a stats dictionary from _get_stats_from_socket,
        publish the individual values.
        """
        for perf in stats['osd_perf_infos']:
            counter_prefix = 'osd.' + str(perf['id'])
            for stat_name, stat_value in flatten_dictionary(
                perf['perf_stats'],
                prefix=counter_prefix,
            ):
              self.log.info('stat_name is %s', stat_name)
              self.log.info('stat_value is %s', stat_value)
              self.publish_gauge(stat_name, stat_value)

    def collect(self):
        """
        Collect stats
        """
        self.log.info('in ceph osd collector')
        stats = self._get_stats()
        self._publish_stats(stats)
</code></pre>

<h3>修改Diamond监控配置</h3>

<pre><code class="bash /etc/diamond/collectors/ceph.conf"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/ceph.conf
[collectors]

[[CephCollector]]
enabled = True

[[CephOsdCollector]]
enabled = True
EOF
</code></pre>

<pre><code class="bash bash"># service diamond restart
</code></pre>
]]></content>
  </entry>
  
</feed>
