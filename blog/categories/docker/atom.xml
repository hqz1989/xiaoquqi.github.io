<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2016-06-14T08:34:57+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[使用Docker部署Ceph]]></title>
    <link href="http://xiaoquqi.github.io/blog/2016/06/13/bootstrap-your-ceph-cluster-in-docker/"/>
    <updated>2016-06-13T15:20:50+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2016/06/13/bootstrap-your-ceph-cluster-in-docker</id>
    <content type="html"><![CDATA[<p>这篇文章是根据Sébastien Han的<a href="https://www.youtube.com/watch?v=FUSTjTBA8f8&amp;feature=youtu.be">演示视频</a>进行整理的，对过程中有问题的部分进行了修复。</p>

<p>Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。</p>

<!-- more -->


<h2>部署环境</h2>

<ul>
<li>至少需要三台虚拟机或者物理机，每台虚拟机或者物理机至少有两块硬盘，这里我是在一台物理机上用vagrant模拟出三台CentOS 6.6虚拟机进行的实验</li>
<li>三台虚拟机需要安装docker，本文附带Docker加速方案</li>
<li>获取ceph/daemon镜像</li>
</ul>


<h2>部署流程</h2>

<p><img class="center" src="/images/blogs/bootstrap-ceph-docker-flow.png"></p>

<h2>部署架构</h2>

<p>主机名和集群的对应关系如下：</p>

<ul>
<li>node1 -> 192.168.33.11</li>
<li>node2 -> 192.168.33.12</li>
<li>node3 -> 192.168.33.13</li>
</ul>


<p><img class="center" src="/images/blogs/bootstrap-ceph-docker-architecture.png"></p>

<h2>环境准备</h2>

<h3>安装Docker，下载镜像</h3>

<p>国内安装Dcoker还是速度很慢的，这里推荐使用daocloud的加速方案。不但docker安装速度提高了，pull镜像的速度也大幅度提高。</p>

<pre><code>curl -sSL https://get.daocloud.io/docker | sh
</code></pre>

<p>我是在CentOS系统上进行的测试，将docker加入自动启动，并启动docker，接下来pull ceph daemon镜像，该镜像包含了所有的ceph服务和entrypoint。</p>

<pre><code>chkconfig docker
service docker start
docker pull ceph/daemon
</code></pre>

<h3>启动第一个Monitor</h3>

<p>在node1上启动第一个Monitor，注意，如果你的环境中IP和我不同，请修改MON_IP。</p>

<pre><code>sudo docker run -d \
     --net=host \
     -v /etc/ceph:/etc/ceph \
     -v /var/lib/ceph/:/var/lib/ceph/ \
     -e MON_IP=192.168.33.11 \
     -e CEPH_PUBLIC_NETWORK=192.168.33.0/24 \
     ceph/daemon mon
</code></pre>

<p>验证一下效果：</p>

<pre><code>docker ps
</code></pre>

<pre><code>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
7babea544ef1        ceph/daemon         "/entrypoint.sh mon"   3 seconds ago       Up 2 seconds                            backstabbing_brattain
</code></pre>

<p>查看一下集群状态：</p>

<pre><code>docker exec 7babea544ef1 ceph -s
</code></pre>

<p>当前集群状态，能看到当前已经有一个mon启动起来了。</p>

<pre><code>    cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439
     health HEALTH_ERR
            64 pgs stuck inactive
            64 pgs stuck unclean
            no osds
     monmap e1: 1 mons at {node1.docker.com=192.168.33.11:6789/0}
            election epoch 2, quorum 0 node1.docker.com
     osdmap e1: 0 osds: 0 up, 0 in
            flags sortbitwise
      pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects
            0 kB used, 0 kB / 0 kB avail
                  64 creating
</code></pre>

<h3>复制配置文件</h3>

<p>接下来需要将node1的配置文件复制到node2和node3上，复制的路径包含/etc/ceph和/var/lib/ceph/bootstrap-*下的所有内容。这些配置文件非常重要，如果没有这些配置文件的存在，我们在其他节点启动新的docker ceph daemon的时候会被认为是一个新的集群。
我们在node1执行以下命令：</p>

<pre><code>ssh root@node2 mkdir -p /var/lib/ceph
scp -r /etc/ceph root@node2:/etc
scp -r /var/lib/ceph/bootstrap* root@node2:/var/lib/ceph

ssh root@node3 mkdir -p /var/lib/ceph
scp -r /etc/ceph root@node3:/etc
scp -r /var/lib/ceph/bootstrap* root@node3:/var/lib/ceph
</code></pre>

<h3>启动第二个和第三个Monitor</h3>

<p>在node2上执行：</p>

<pre><code>sudo docker run -d \
     --net=host \
     -v /etc/ceph:/etc/ceph \
     -v /var/lib/ceph/:/var/lib/ceph/ \
     -e MON_IP=192.168.33.12 \
     -e CEPH_PUBLIC_NETWORK=192.168.33.0/24 \
     ceph/daemon mon
</code></pre>

<p>在node3上执行：</p>

<pre><code>sudo docker run -d \
     --net=host \
     -v /etc/ceph:/etc/ceph \
     -v /var/lib/ceph/:/var/lib/ceph/ \
     -e MON_IP=192.168.33.13 \
     -e CEPH_PUBLIC_NETWORK=192.168.33.0/24 \
     ceph/daemon mon
</code></pre>

<p>在node1上查看集群状态：</p>

<pre><code>    cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439
     health HEALTH_ERR
            64 pgs stuck inactive
            64 pgs stuck unclean
            no osds
     monmap e3: 3 mons at {node1.docker.com=192.168.33.11:6789/0,node2.docker.com=192.168.33.12:6789/0,node3.docker.com=192.168.33.13:6789/0}
            election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com
     osdmap e1: 0 osds: 0 up, 0 in
            flags sortbitwise
      pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects
            0 kB used, 0 kB / 0 kB avail
                  64 creating
</code></pre>

<h3>启动OSD的遇到的问题</h3>

<p>按照原视频的介绍的方法，启动OSD可以直接指定某个分区，然后用osd_ceph_disk作为启动ceph/daemon的参数，之后docker镜像会自动的进行分区等动作。但是经过实际验证却发现在mkjournal创建错误，OSD无法启动。</p>

<p>经过和社区确认，发现这个Bug在之前版本中得到过修复，但是之后的版本又出现了。根据社区的建议使用jewel版本的ceph daemon进行了再次验证，发现问题依旧，所以这里介绍的方法只能退而求其次，采用手动方式分区、格式化，之后用osd_directory启动ceph/daemon。</p>

<p>这是github上的相关讨论：<a href="https://github.com/ceph/ceph-docker/issues/171">https://github.com/ceph/ceph-docker/issues/171</a></p>

<p>这是用osd_ceph_disk方式启动后的错误日志：</p>

<pre><code>command_check_call: Running command: /usr/bin/ceph-osd --cluster ceph --mkfs --mkkey -i 4 --monmap /var/lib/ceph/tmp/mnt.BT8FXG/activate.monmap --osd-data /var/lib/ceph/tmp/mnt.BT8FXG --osd-journal /var/lib/ceph/tmp/mnt.BT8FXG/journal --osd-uuid 89e240e1-17e9-4d6c-8d4f-f1a3e0278b91 --keyring /var/lib/ceph/tmp/mnt.BT8FXG/keyring --setuser ceph --setgroup disk
2016-06-12 23:37:26.180610 7f8889654800 -1 filestore(/var/lib/ceph/tmp/mnt.BT8FXG) mkjournal error creating journal on /var/lib/ceph/tmp/mnt.BT8FXG/journal: (2) No such file or directory
2016-06-12 23:37:26.180752 7f8889654800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -2
2016-06-12 23:37:26.180918 7f8889654800 -1 ** ERROR: error creating empty object store in /var/lib/ceph/tmp/mnt.BT8FXG: (2) No such file or directory
mount_activate: Failed to activate
unmount: Unmounting /var/lib/ceph/tmp/mnt.BT8FXG
command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.BT8FXG
</code></pre>

<h3>启动OSD</h3>

<p>第一步先进行分区和格式化，这里只给出node1的操作方式，其他两个节点的方式类似。</p>

<p>先来安装必要的工具：</p>

<pre><code>yum install -y parted xfsprogs
</code></pre>

<pre><code>[root@node1 vagrant]# parted /dev/sdb
GNU Parted 2.1
Using /dev/sdb
(parted) mklabel
New disk label type? gpt
(parted) p
Model: ATA VBOX HARDDISK (scsi)
Disk /dev/sdb: 107GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start  End  Size  File system  Name  Flags

(parted) mkpart
Partition name?  []? "Linux filesystem"
File system type?  [ext2]? xfs
Start? 0G
End? 107GB
(parted) p
Model: ATA VBOX HARDDISK (scsi)
Disk /dev/sdb: 107GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start   End    Size   File system  Name              Flags
 1      1049kB  107GB  107GB               Linux filesystem

(parted) quit
</code></pre>

<p>格式化：</p>

<pre><code>[root@node1 vagrant]# mkfs.xfs /dev/sdb1
meta-data=/dev/sdb1              isize=256    agcount=4, agsize=6553472 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=26213888, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=12799, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
</code></pre>

<p>我们把目录在node1上进行挂载。</p>

<pre><code>mkdir -p /ceph/sdb
mount /dev/sdb1 /ceph/sdb
</code></pre>

<p>最后启动OSD，这里最重要的就是把我们刚刚挂载好的OSD的实际路径透传给Docker内部的/var/lib/ceph/osd，如果每个节点有多个OSD的情况下，只需要在Host上映射到不同的目录，启动Docker的时候变更和/var/lib/ceph/osd的映射关系即可。</p>

<pre><code>sudo docker run -d \
    --net=host \
    -v /etc/ceph:/etc/ceph \
    -v /var/lib/ceph/:/var/lib/ceph/ \
    -v /dev/:/dev/ \
    -v /ceph/sdb:/var/lib/ceph/osd \
    --privileged=true \
    ceph/daemon osd_directory
</code></pre>

<p>按照同样的方法，将node2和node3的OSD也加入到集群，最终的效果如下：</p>

<pre><code>    cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439
     health HEALTH_WARN
            clock skew detected on mon.node2.docker.com
            64 pgs degraded
            64 pgs stuck unclean
            64 pgs undersized
            Monitor clock skew detected
     monmap e3: 3 mons at {node1.docker.com=192.168.33.11:6789/0,node2.docker.com=192.168.33.12:6789/0,node3.docker.com=192.168.33.13:6789/0}
            election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com
     osdmap e13: 3 osds: 3 up, 3 in
            flags sortbitwise
      pgmap v18: 64 pgs, 1 pools, 0 bytes data, 0 objects
            4551 MB used, 11306 MB / 16720 MB avail
                  64 active+undersized+degraded
</code></pre>

<h3>创建MDS</h3>

<p>创建好基本的环境，其他的就容易了很多，下面来启动MDS。</p>

<pre><code>sudo docker run -d \
    --net=host \
    -v /etc/ceph:/etc/ceph \
    -v /var/lib/ceph/:/var/lib/ceph/ \
    -e CEPHFS_CREATE=1 \
    ceph/daemon mds
</code></pre>

<h3>启动RGW，并且映射80端口</h3>

<pre><code>sudo docker run -d \
    -p 80:80 \
    -v /etc/ceph:/etc/ceph \
    -v /var/lib/ceph/:/var/lib/ceph/ \
    ceph/daemon rgw
</code></pre>

<h3>最终的集群状态</h3>

<pre><code>    cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439
     health HEALTH_WARN
            clock skew detected on mon.node2.docker.com
            48 pgs stuck inactive
            48 pgs stuck unclean
            Monitor clock skew detected
     monmap e3: 3 mons at {node1.docker.com=192.168.33.11:6789/0,node2.docker.com=192.168.33.12:6789/0,node3.docker.com=192.168.33.13:6789/0}
            election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com
     mdsmap e5: 1/1/1 up {0=mds-node1.docker.com=up:active}
     osdmap e25: 3 osds: 3 up, 3 in
            flags sortbitwise
      pgmap v38: 128 pgs, 9 pools, 588 bytes data, 11 objects
            6791 MB used, 16996 MB / 25081 MB avail
                  80 active+clean
                  45 creating
                   3 creating+activating
</code></pre>

<h2>总结</h2>

<p>在Docker中部署Ceph并没有想象中的那么顺利，社区的版本中仍然有Bug需要解决。</p>

<p>Docker作为一种快捷的部署方式，的确可以大幅度提高Ceph的部署效率，提高扩展的速度。但是从另一个角度我们应该注意到，随着Docker的引入也改变了Ceph的运维方式，比如在OSD增减的时候，需要到容器中对Ceph集群进行维护。再比如配置文件变更后的重启问题等。</p>

<p>但是无论如何，我相信这些问题都会得到完美的解决，用Docker部署Ceph作为一种新的尝试，值得推广。
之后还会为大家带来，如何使用Ansible结合Docker更快速的部署Ceph集群，敬请期待。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Consul主要使用场景]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/12/24/use-consul/"/>
    <updated>2015-12-24T18:07:24+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/12/24/use-consul</id>
    <content type="html"><![CDATA[<p>假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：<a href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/">http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/</a></p>

<p>这篇文章里主要介绍Consul的使用场景，服务和健康检查。</p>

<!-- more -->


<h2>Service</h2>

<p>服务注册有点像OpenStack Keystone的Endpoints，可以通过API方式查询到所有服务的端点信息。</p>

<p>在Agent的节点上添加一个service，之后重启服务。</p>

<ul>
<li>添加一个服务</li>
</ul>


<pre><code>$ echo '{"service": {"name": "web", "tags": ["rails"], "port": 80}}' \
    &gt;/etc/consul.d/web.json
</code></pre>

<ul>
<li>重启agent</li>
</ul>


<p>重新加载新的服务并不需要杀死进程重启服务，只需要给进程直接发送一个SIGHUP。</p>

<pre><code>$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk '{print $2}')
</code></pre>

<ul>
<li>日志输出</li>
</ul>


<p>从输出的日志上都可以看到加载了新的服务web。</p>

<pre><code>==&gt; Caught signal: hangup
==&gt; Reloading configuration...
==&gt; WARNING: Expect Mode enabled, expecting 3 servers
    2015/12/24 12:01:11 [INFO] agent: Synced service 'web'
</code></pre>

<ul>
<li>利用API查询</li>
</ul>


<p>我们在任意节点上利用REST API查看服务。</p>

<pre><code>$ curl http://localhost:8500/v1/catalog/service/web
</code></pre>

<pre><code>[{"Node":"server1.consul.com","Address":"200.21.1.101","ServiceID":"web","ServiceName":"web","ServiceTags":["rails"],"ServiceAddress":"","ServicePort":80}]
</code></pre>

<h2>Health Check</h2>

<p>健康检查的方法主要是通过运行一小段脚本的方式，根据运行的结果判断检查对象的健康状况。所以可以通过任意语言定义这个脚本，脚本运行将通过和consul执行的相同用户执行。</p>

<ul>
<li>添加一个健康检查</li>
</ul>


<p>每30秒ping google.com</p>

<pre><code>$ echo '{"check": {"name": "ping",
  "script": "ping -c1 google.com &gt;/dev/null", "interval": "30s"}}' \
    &gt; /etc/consul.d/ping.json
</code></pre>

<p>为刚才的服务添加健康检查</p>

<pre><code>$ echo '{"service": {"name": "web", "tags": ["rails"], "port": 80,
  "check": {"script": "curl localhost &gt;/dev/null 2&gt;&amp;1", "interval": "10s"}}}' \
    &gt; /etc/consul.d/web.json
</code></pre>

<ul>
<li>重启agent</li>
</ul>


<pre><code>$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk '{print $2}')
</code></pre>

<ul>
<li>日志输出</li>
</ul>


<p>从输出的日志上都可以看到加载了新的服务web。</p>

<pre><code>==&gt; Caught signal: hangup
==&gt; Reloading configuration...
==&gt; WARNING: Expect Mode enabled, expecting 3 servers
    2015/12/24 12:43:56 [INFO] agent: Synced service 'web'
    2015/12/24 12:43:56 [INFO] agent: Synced check 'ping'
</code></pre>

<p>经过一段时间后出现了critical和warning日志</p>

<pre><code>    2015/12/24 12:43:58 [WARN] agent: Check 'service:web' is now critical
    2015/12/24 12:44:08 [WARN] agent: Check 'ping' is now warning
</code></pre>

<ul>
<li>利用API查询</li>
</ul>


<p>Health check的状态包含了很多种，有any, unkown, passing, warning, critical。any包含了所有状态。</p>

<pre><code>$ curl http://localhost:8500/v1/health/state/critical
</code></pre>

<pre><code>[{"Node":"server1.consul.com","CheckID":"service:web","Name":"Service 'web' check","Status":"critical","Notes":"","Output":"","ServiceID":"web","ServiceName":"web"}]
</code></pre>

<h2>参考文档</h2>

<ul>
<li><a href="http://www.consul.io/docs/agent/http/catalog.html">http://www.consul.io/docs/agent/http/catalog.html</a></li>
<li><a href="http://www.consul.io/docs/agent/http/health.html">http://www.consul.io/docs/agent/http/health.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Consul的安装方法]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/"/>
    <updated>2015-12-07T10:00:13+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/12/07/consul-installation</id>
    <content type="html"><![CDATA[<h2>什么是Consul?</h2>

<p>Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。</p>

<!-- more -->


<h2>安装Consul</h2>

<p>Consul的安装比较简单，下载之后直接解压缩就可以了，下载地址：<a href="https://www.consul.io/downloads.html">https://www.consul.io/downloads.html</a></p>

<p>我们把consul直接放在/usr/local/bin目录中。</p>

<h2>Consul Server</h2>

<pre><code>$ /usr/local/bin/consul agent -server -bootstrap-expect 3 -data-dir /tmp/consul -node=server1 -bind=10.10.10.10
</code></pre>

<h3>参数说明</h3>

<ul>
<li>-server - Serve模式</li>
<li>-bootstrap-expect - Server数量</li>
<li>-data-dir - 数据目录</li>
<li>-node - Node名称</li>
<li>-bind - 集群通讯地址</li>
</ul>


<h3>输出</h3>

<pre><code>==&gt; WARNING: Expect Mode enabled, expecting 3 servers
==&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1
==&gt; Starting Consul agent...
==&gt; Starting Consul agent RPC...
==&gt; Consul agent running!
         Node name: 'server1.consul.com'
        Datacenter: 'dc1'
            Server: true (bootstrap: false)
       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)
      Cluster Addr: 200.21.1.101 (LAN: 8301, WAN: 8302)
    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false
             Atlas: &lt;disabled&gt;

==&gt; Log data will now stream in as it occurs:

    2015/12/23 03:13:36 [WARN] memberlist: Binding to public address without encryption!
    2015/12/23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101
    2015/12/23 03:13:36 [WARN] memberlist: Binding to public address without encryption!
    2015/12/23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com.dc1 200.21.1.101
    2015/12/23 03:13:36 [INFO] raft: Node at 200.21.1.101:8300 [Follower] entering Follower state
    2015/12/23 03:13:36 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/23 03:13:36 [INFO] consul: adding server server1.consul.com.dc1 (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/23 03:13:36 [ERR] agent: failed to sync remote state: No cluster leader
    2015/12/23 03:13:37 [WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.
    2015/12/23 03:13:51 [ERR] agent: failed to sync remote state: No cluster leader
==&gt; Newer Consul version available: 0.6.0
    2015/12/23 03:14:17 [ERR] agent: failed to sync remote state: No cluster leader
</code></pre>

<h3>查看成员</h3>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
</code></pre>

<h2>Consul Agent</h2>

<pre><code>$ /usr/local/bin/consul agent -data-dir /tmp/consul -node=agent1 -bind=10.10.10.100 -config-dir /etc/consul.d
</code></pre>

<ul>
<li>输出</li>
</ul>


<pre><code>==&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1
==&gt; Starting Consul agent...
==&gt; Starting Consul agent RPC...
==&gt; Consul agent running!
         Node name: 'agent1.consul.com'
        Datacenter: 'dc1'
            Server: false (bootstrap: false)
       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)
      Cluster Addr: 200.21.1.201 (LAN: 8301, WAN: 8302)
    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false
             Atlas: &lt;disabled&gt;

==&gt; Log data will now stream in as it occurs:

    2015/12/24 08:09:51 [WARN] memberlist: Binding to public address without encryption!
    2015/12/24 08:09:51 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201
    2015/12/24 08:09:51 [ERR] agent: failed to sync remote state: No known Consul servers
    2015/12/24 08:09:56 [INFO] agent.rpc: Accepted client: 127.0.0.1:42794
    2015/12/24 08:09:56 [INFO] agent: (LAN) joining: [200.21.1.101 200.21.1.102 200.21.1.103]
    2015/12/24 08:09:56 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101
    2015/12/24 08:09:56 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)
    2015/12/24 08:09:58 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:10:02 [INFO] agent: (LAN) joined: 1 Err: &lt;nil&gt;
    2015/12/24 08:10:02 [INFO] agent.rpc: Accepted client: 127.0.0.1:42800
==&gt; Newer Consul version available: 0.6.0
    2015/12/24 08:10:21 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:10:22 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:10:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:01 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:11:02 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:23 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:11:41 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:11:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:12:12 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:12:21 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:12:36 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
    2015/12/24 08:13:01 [WARN] agent: Check 'ping' is now warning
    2015/12/24 08:13:03 [ERR] agent: failed to sync remote state: rpc error: No cluster leader
</code></pre>

<ul>
<li>server日志输出</li>
</ul>


<pre><code>    2015/12/24 08:09:58 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201
</code></pre>

<h3>查看成员</h3>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1
</code></pre>

<h2>最终结果</h2>

<pre><code>$ consul members
</code></pre>

<pre><code>Node                Address            Status  Type    Build  Protocol  DC
server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1
agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1
agent2.consul.com   200.21.1.202:8301  alive   client  0.5.2  2         dc1
server2.consul.com  200.21.1.102:8301  alive   server  0.5.2  2         dc1
server3.consul.com  200.21.1.103:8301  alive   server  0.5.2  2         dc1
agent3.consul.com   200.21.1.203:8301  alive   client  0.5.2  2         dc1
</code></pre>

<h2>参考文档</h2>

<ul>
<li><a href="https://www.consul.io/intro/getting-started/install.html">https://www.consul.io/intro/getting-started/install.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
