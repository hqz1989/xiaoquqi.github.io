<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ceph | RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/blog/categories/ceph/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2015-09-03T18:30:41+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ceph性能优化总结(v0.94)]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/06/28/ceph-performance-optimization-summary/"/>
    <updated>2015-06-28T14:30:22+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/06/28/ceph-performance-optimization-summary</id>
    <content type="html"><![CDATA[<p>最近一直在忙着搞Ceph存储的优化和测试，看了各种资料，但是好像没有一篇文章把其中的方法论交代清楚，所以呢想在这里进行一下总结，很多内容并不是我原创，只是做一个总结。如果其中有任何的问题，欢迎各位喷我，以便我提高。</p>

<h2>优化方法论</h2>

<p>做任何事情还是要有个方法论的，“授人以鱼不如授人以渔”的道理吧，方法通了，所有的问题就有了解决的途径。通过对公开资料的分析进行总结，对分布式存储系统的优化离不开以下几点：</p>

<h3>1. 硬件层面</h3>

<ul>
<li>硬件规划</li>
<li>SSD选择</li>
<li>BIOS设置</li>
</ul>


<h3>2. 软件层面</h3>

<ul>
<li>Linux OS</li>
<li>Ceph Configurations</li>
<li>PG Number调整</li>
<li>CRUSH Map</li>
<li>其他因素</li>
</ul>


<!-- more -->


<h2>硬件优化</h2>

<h3>1. 硬件规划</h3>

<ul>
<li>Processor</li>
</ul>


<p>ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。当然如果你使用EC方式，可能需要更多的CPU资源。</p>

<p>ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。</p>

<p>ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。</p>

<ul>
<li>内存</li>
</ul>


<p>ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存，当然2G更好。</p>

<ul>
<li>网络规划</li>
</ul>


<p>万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。</p>

<h3>2. SSD选择</h3>

<p>硬件的选择也直接决定了Ceph集群的性能，从成本考虑，一般选择SATA SSD作为Journal，<a href="http://www.intel.com/content/www/us/en/solid-state-drives/solid-state-drives-dc-s3500-series.html">Intel® SSD DC S3500 Series</a>基本是目前看到的方案中的首选。400G的规格4K随机写可以达到11000 IOPS。如果在预算足够的情况下，推荐使用PCIE SSD，性能会得到进一步提升，但是由于Journal在向数据盘写入数据时Block后续请求，所以Journal的加入并未呈现出想象中的性能提升，但是的确会对Latency有很大的改善。</p>

<p>如何确定你的SSD是否适合作为SSD Journal，可以参考SÉBASTIEN HAN的<a href="http://www.sebastien-han.fr/blog/2014/10/10/ceph-how-to-test-if-your-ssd-is-suitable-as-a-journal-device/">Ceph: How to Test if Your SSD Is Suitable as a Journal Device?</a>，这里面他也列出了常见的SSD的测试结果，从结果来看SATA SSD中，Intel S3500性能表现最好。</p>

<h3>3. BIOS设置</h3>

<ul>
<li>Hyper-Threading(HT)</li>
</ul>


<p>基本做云平台的，VT和HT打开都是必须的，超线程技术(HT)就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高的CPU的运行效率。</p>

<ul>
<li>关闭节能</li>
</ul>


<p>关闭节能后，对性能还是有所提升的，所以坚决调整成性能型(Performance)。当然也可以在操作系统级别进行调整，详细的调整过程请参考<a href="http://www.servernoobs.com/avoiding-cpu-speed-scaling-in-modern-linux-distributions-running-cpu-at-full-speed-tips/">链接</a>，但是不知道是不是由于BIOS已经调整的缘故，所以在CentOS 6.6上并没有发现相关的设置。</p>

<pre><code>for CPUFREQ in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do [ -f $CPUFREQ ] || continue; echo -n performance &gt; $CPUFREQ; done
</code></pre>

<ul>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-numa/">NUMA</a></li>
</ul>


<p>简单来说，NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存，<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2013-December/036211.html">NUMA可能会在某些情况下影响ceph-osd</a>。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。</p>

<pre><code>kernel /vmlinuz-2.6.32-504.12.2.el6.x86_64 ro root=UUID=870d47f8-0357-4a32-909f-74173a9f0633 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM   biosdevname=0 numa=off
</code></pre>

<h2>软件优化</h2>

<h3>1. Linux OS</h3>

<ul>
<li>Kernel pid max</li>
</ul>


<pre><code>echo 4194303 &gt; /proc/sys/kernel/pid_max
</code></pre>

<ul>
<li>Jumbo frames, 交换机端需要支持该功能，系统网卡设置才有效果</li>
</ul>


<pre><code>ifconfig eth0 mtu 9000
</code></pre>

<p>永久设置</p>

<pre><code>echo "MTU=9000" | tee -a /etc/sysconfig/network-script/ifcfg-eth0
/etc/init.d/networking restart
</code></pre>

<ul>
<li>read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，查看默认值</li>
</ul>


<pre><code>cat /sys/block/sda/queue/read_ahead_kb
</code></pre>

<p>根据一些Ceph的公开分享，8192是比较理想的值</p>

<pre><code>echo "8192" &gt; /sys/block/sda/queue/read_ahead_kb
</code></pre>

<ul>
<li>swappiness, 主要控制系统对swap的使用，这个参数的调整最先见于UnitedStack公开的文档中，猜测调整的原因主要是使用swap会影响系统的性能。</li>
</ul>


<pre><code>echo "vm.swappiness = 0" | tee -a /etc/sysctl.conf
</code></pre>

<ul>
<li>I/O Scheduler，关于I/O Scheculder的调整网上已经有很多资料，这里不再赘述，简单说SSD要用noop，SATA/SAS使用deadline。</li>
</ul>


<pre><code>echo "deadline" &gt; /sys/block/sd[x]/queue/scheduler
echo "noop" &gt; /sys/block/sd[x]/queue/scheduler
</code></pre>

<ul>
<li>cgroup</li>
</ul>


<p>这方面的文章好像比较少，昨天在和Ceph社区交流过程中，Jan Schermer说准备把生产环境中的一些脚本贡献出来，但是暂时还没有，他同时也列举了一些使用cgroup进行隔离的<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg21111.html">原因</a>。</p>

<blockquote><ul>
<li>不在process和thread在不同的core上移动(更好的缓存利用)</li>
<li>减少NUMA的影响</li>
<li>网络和存储控制器影响 - 较小</li>
<li>通过限制cpuset来限制Linux调度域(不确定是不是重要但是是最佳实践)</li>
<li>如果开启了HT，可能会造成OSD在thread1上，KVM在thread2上，并且是同一个core。Core的延迟和性能取决于其他一个线程做什么。</li>
</ul>
</blockquote>

<p>这一点具体实现待补充！！！</p>

<h3>2. Ceph Configurations</h3>

<h4>[global]</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> public network </td>
<td> 客户端访问网络 </td>
<td> </td>
<td> 192.168.100.0/24 </td>
</tr>
<tr>
<td> cluster network </td>
<td> 集群网络 </td>
<td> </td>
<td> 192.168.1.0/24 </td>
</tr>
<tr>
<td> max open files </td>
<td> 如果设置了该选项，Ceph会设置系统的max open fds </td>
<td> 0 </td>
<td> 131072 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>查看系统最大文件打开数可以使用命令</li>
</ul>


<pre><code>    cat /proc/sys/fs/file-max
</code></pre>

<hr />

<h4>[osd] - filestore</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> filestore xattr use omap </td>
<td> 为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用 </td>
<td> false </td>
<td> true </td>
</tr>
<tr>
<td> filestore max sync interval </td>
<td> 从日志到数据盘最大同步间隔(seconds) </td>
<td> 5 </td>
<td> 15 </td>
</tr>
<tr>
<td> filestore min sync interval </td>
<td> 从日志到数据盘最小同步间隔(seconds) </td>
<td> 0.1 </td>
<td> 10 </td>
</tr>
<tr>
<td> filestore queue max ops </td>
<td> 数据盘最大接受的操作数 </td>
<td> 500 </td>
<td> 25000 </td>
</tr>
<tr>
<td> filestore queue max bytes </td>
<td> 数据盘一次操作最大字节数(bytes) </td>
<td> 100 &lt;&lt; 20 </td>
<td> 10485760 </td>
</tr>
<tr>
<td> filestore queue committing max ops </td>
<td> 数据盘能够commit的操作数 </td>
<td> 500 </td>
<td> 5000 </td>
</tr>
<tr>
<td> filestore queue committing max bytes </td>
<td> 数据盘能够commit的最大字节数(bytes) </td>
<td> 100 &lt;&lt; 20 </td>
<td> 10485760000 </td>
</tr>
<tr>
<td> filestore op threads </td>
<td> 并发文件系统操作数 </td>
<td> 2 </td>
<td> 32 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>调整omap的原因主要是EXT4文件系统默认仅有4K</li>
<li>filestore queue相关的参数对于性能影响很小，参数调整不会对性能优化有本质上提升</li>
</ul>


<hr />

<h4>[osd] - journal</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd journal size </td>
<td> OSD日志大小(MB) </td>
<td> 5120 </td>
<td> 20000 </td>
</tr>
<tr>
<td> journal max write bytes </td>
<td> journal一次性写入的最大字节数(bytes) </td>
<td> 10 &lt;&lt; 20 </td>
<td> 1073714824 </td>
</tr>
<tr>
<td> journal max write entries </td>
<td> journal一次性写入的最大记录数 </td>
<td> 100 </td>
<td> 10000 </td>
</tr>
<tr>
<td> journal queue max ops </td>
<td> journal一次性最大在队列中的操作数 </td>
<td> 500 </td>
<td> 50000 </td>
</tr>
<tr>
<td> journal queue max bytes </td>
<td> journal一次性最大在队列中的字节数(bytes) </td>
<td> 10 &lt;&lt; 20 </td>
<td> 10485760000 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space.</li>
<li>上面这段话的意思就是，Ceph OSD进程在往数据盘上刷数据的过程中，是停止写操作的。</li>
</ul>


<hr />

<h4>[osd] - osd config tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd max write size </td>
<td> OSD一次可写入的最大值(MB) </td>
<td> 90 </td>
<td> 512 </td>
</tr>
<tr>
<td> osd client message size cap </td>
<td> 客户端允许在内存中的最大数据(bytes) </td>
<td> 524288000 </td>
<td> 2147483648 </td>
</tr>
<tr>
<td> osd deep scrub stride </td>
<td> 在Deep Scrub时候允许读取的字节数(bytes) </td>
<td> 524288 </td>
<td> 131072 </td>
</tr>
<tr>
<td> osd op threads </td>
<td> OSD进程操作的线程数 </td>
<td> 2 </td>
<td> 8 </td>
</tr>
<tr>
<td> osd disk threads </td>
<td> OSD密集型操作例如恢复和Scrubbing时的线程 </td>
<td> 1 </td>
<td> 4 </td>
</tr>
<tr>
<td> osd map cache size </td>
<td> 保留OSD Map的缓存(MB) </td>
<td> 500 </td>
<td> 1024 </td>
</tr>
<tr>
<td> osd map cache bl size </td>
<td> OSD进程在内存中的OSD Map缓存(MB) </td>
<td> 50 </td>
<td> 128 </td>
</tr>
<tr>
<td> osd mount options xfs </td>
<td> Ceph OSD xfs Mount选项 </td>
<td> rw,noatime,inode64 </td>
<td> rw,noexec,nodev,noatime,nodiratime,nobarrier </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li>增加osd op threads和disk threads会带来额外的CPU开销</li>
</ul>


<hr />

<h4>[osd] - recovery tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> osd recovery op priority </td>
<td> 恢复操作优先级，取值1-63，值越高占用资源越高 </td>
<td> 10 </td>
<td> 4 </td>
</tr>
<tr>
<td> osd recovery max active </td>
<td> 同一时间内活跃的恢复请求数 </td>
<td> 15 </td>
<td> 10 </td>
</tr>
<tr>
<td> osd max backfills </td>
<td> 一个OSD允许的最大backfills数 </td>
<td> 10 </td>
<td> 4 </td>
</tr>
</tbody>
</table>


<h4>[osd] - client tuning</h4>

<table>
<thead>
<tr>
<th> 参数名 </th>
<th> 描述 </th>
<th> 默认值 </th>
<th> 建议值 </th>
</tr>
</thead>
<tbody>
<tr>
<td> rbd cache </td>
<td> RBD缓存 </td>
<td> true </td>
<td> true </td>
</tr>
<tr>
<td> rbd cache size </td>
<td> RBD缓存大小(bytes) </td>
<td> 33554432 </td>
<td> 268435456 </td>
</tr>
<tr>
<td> rbd cache max dirty </td>
<td> 缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through </td>
<td> 25165824 </td>
<td> 134217728 </td>
</tr>
<tr>
<td> rbd cache max dirty age </td>
<td> 在被刷新到存储盘前dirty数据存在缓存的时间(seconds) </td>
<td> 1 </td>
<td> 5 </td>
</tr>
</tbody>
</table>


<h4>关闭Debug</h4>

<h3>3. PG Number</h3>

<p>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。</p>

<pre><code>Total PGs = (Total_number_of_OSD * 100) / max_replication_count
</code></pre>

<p>例如15个OSD，副本数为3的情况下，根据公式计算的结果应该为500，最接近512，所以需要设定该pool(volumes)的pg_num和pgp_num都为512.</p>

<pre><code>ceph osd pool set volumes pg_num 512
ceph osd pool set volumes pgp_num 512
</code></pre>

<h3>4. CRUSH Map</h3>

<p>CRUSH是一个非常灵活的方式，CRUSH MAP的调整取决于部署的具体环境，这个可能需要根据具体情况进行分析，这里面就不再赘述了。</p>

<h3>5. 其他因素的影响</h3>

<p>在今年的(2015年)的Ceph Day上，海云捷迅在调优过程中分享过一个由于在集群中存在一个性能不好的磁盘，导致整个集群性能下降的case。通过osd perf可以提供磁盘latency的状况，同时在运维过程中也可以作为监控的一个重要指标，很明显在下面的例子中，OSD 8的磁盘延时较长，所以需要考虑将该OSD剔除出集群：</p>

<pre><code>ceph osd perf
</code></pre>

<pre><code>osd fs_commit_latency(ms) fs_apply_latency(ms)
  0                    14                   17
  1                    14                   16
  2                    10                   11
  3                     4                    5
  4                    13                   15
  5                    17                   20
  6                    15                   18
  7                    14                   16
  8                   299                  329
</code></pre>

<h2>ceph.conf</h2>

<pre><code>[global]
fsid = 059f27e8-a23f-4587-9033-3e3679d03b31
mon_host = 10.10.20.102, 10.10.20.101, 10.10.20.100
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd pool default size = 3
osd pool default min size = 1

public network = 10.10.20.0/24
cluster network = 10.10.20.0/24

max open files = 131072

[mon]
mon data = /var/lib/ceph/mon/ceph-$id

[osd]
osd data = /var/lib/ceph/osd/ceph-$id
osd journal size = 20000
osd mkfs type = xfs
osd mkfs options xfs = -f

filestore xattr use omap = true
filestore min sync interval = 10
filestore max sync interval = 15
filestore queue max ops = 25000
filestore queue max bytes = 10485760
filestore queue committing max ops = 5000
filestore queue committing max bytes = 10485760000

journal max write bytes = 1073714824
journal max write entries = 10000
journal queue max ops = 50000
journal queue max bytes = 10485760000

osd max write size = 512
osd client message size cap = 2147483648
osd deep scrub stride = 131072
osd op threads = 8
osd disk threads = 4
osd map cache size = 1024
osd map cache bl size = 128
osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier"
osd recovery op priority = 4
osd recovery max active = 10
osd max backfills = 4

[client]
rbd cache = true
rbd cache size = 268435456
rbd cache max dirty = 134217728
rbd cache max dirty age = 5
</code></pre>

<h2>总结</h2>

<p>优化是一个长期迭代的过程，所有的方法都是别人的，只有在实践过程中才能发现自己的，本篇文章仅仅是一个开始，欢迎各位积极补充，共同完成一篇具有指导性的文章。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph集群磁盘没有剩余空间的解决方法]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full/"/>
    <updated>2015-05-12T09:21:42+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full</id>
    <content type="html"><![CDATA[<h2>故障描述</h2>

<p>OpenStack + Ceph集群在使用过程中，由于虚拟机拷入大量新的数据，导致集群的磁盘迅速消耗，没有空余空间，虚拟机无法操作，Ceph集群所有操作都无法执行。</p>

<!-- more -->


<h2>故障现象</h2>

<ul>
<li>尝试使用OpenStack重启虚拟机无效</li>
<li>尝试直接用rbd命令直接删除块失败</li>
</ul>


<pre><code class="plain">[root@controller ~]# rbd -p volumes rm volume-c55fd052-212d-4107-a2ac-cf53bfc049be
2015-04-29 05:31:31.719478 7f5fb82f7760  0 client.4781741.objecter  FULL, paused modify 0xe9a9e0 tid 6
</code></pre>

<ul>
<li>查看ceph健康状态</li>
</ul>


<pre><code class="plain ceph -s">cluster 059f27e8-a23f-4587-9033-3e3679d03b31
 health HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482/129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)
 monmap e6: 4 mons at {node-5e40.cloud.com=10.10.20.40:6789/0,node-6670.cloud.com=10.10.20.31:6789/0,node-66c4.cloud.com=10.10.20.36:6789/0,node-fb27.cloud.com=10.10.20.41:6789/0}, election epoch 886, quorum 0,1,2,3 node-6670.cloud.com,node-66c4.cloud.com,node-5e40.cloud.com,node-fb27.cloud.com
 osdmap e2743: 3 osds: 3 up, 3 in
        flags full
  pgmap v6564199: 320 pgs, 4 pools, 262 GB data, 43027 objects
        786 GB used, 47785 MB / 833 GB avail
        7482/129081 objects degraded (5.796%)
             300 active+clean
              20 active+degraded+remapped+backfill_toofull
</code></pre>

<pre><code class="plain ceph health detail">HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482/129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)
pg 3.8 is stuck unclean for 7067109.597691, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.7d is stuck unclean for 1852078.505139, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.21 is stuck unclean for 7072842.637848, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.22 is stuck unclean for 7070880.213397, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.a is stuck unclean for 7067057.863562, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.7f is stuck unclean for 7067122.493746, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.5 is stuck unclean for 7067088.369629, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1e is stuck unclean for 7073386.246281, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.19 is stuck unclean for 7068035.310269, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.5d is stuck unclean for 1852078.505949, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1a is stuck unclean for 7067088.429544, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.1b is stuck unclean for 7072773.771385, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.3 is stuck unclean for 7067057.864514, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.15 is stuck unclean for 7067088.825483, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.11 is stuck unclean for 7067057.862408, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.6d is stuck unclean for 7067083.634454, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.6e is stuck unclean for 7067098.452576, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.c is stuck unclean for 5658116.678331, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.e is stuck unclean for 7067078.646953, current state active+degraded+remapped+backfill_toofull, last acting [2,0]
pg 3.20 is stuck unclean for 7067140.530849, current state active+degraded+remapped+backfill_toofull, last acting [0,2]
pg 3.7d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.7f is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.6d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.6e is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.5d is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.20 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.21 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.22 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.1e is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.19 is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.1a is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.1b is active+degraded+remapped+backfill_toofull, acting [0,2]
pg 3.15 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.11 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.c is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.e is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.8 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.a is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.5 is active+degraded+remapped+backfill_toofull, acting [2,0]
pg 3.3 is active+degraded+remapped+backfill_toofull, acting [2,0]
recovery 7482/129081 objects degraded (5.796%)
osd.0 is full at 95%
osd.2 is full at 95%
osd.1 is near full at 93%
</code></pre>

<h2>解决方案一(已验证)</h2>

<p>增加OSD节点，这也是官方文档中推荐的做法，增加新的节点后，Ceph开始重新平衡数据，OSD使用空间开始下降</p>

<pre><code class="plain">2015-04-29 06:51:58.623262 osd.1 [WRN] OSD near full (91%)
2015-04-29 06:52:01.500813 osd.2 [WRN] OSD near full (92%)
</code></pre>

<h2>解决方案二(理论上，没有进行验证)</h2>

<p>如果在没有新的硬盘的情况下，只能采用另外一种方式。在当前状态下，Ceph不允许任何的读写操作，所以此时任何的Ceph命令都不好使，解决的方案就是尝试降低Ceph对于full的比例定义，我们从上面的日志中可以看到Ceph的full的比例为95%，我们需要做的就是提高full的比例，之后尽快尝试删除数据，将比例下降。</p>

<ul>
<li>尝试直接用命令设置，但是失败了，Ceph集群并没有重新同步数据，怀疑可能仍然需要重启服务本身</li>
</ul>


<pre><code class="plain">ceph mon tell \* injectargs '--mon-osd-full-ratio 0.98'
</code></pre>

<ul>
<li>修改配置文件，之后重启monitor服务，但是担心出问题，所以没有敢尝试该方法，后续经过在邮件列表确认，该方法应该不会对数据产生影响，但是前提是在恢复期间，所有的虚拟机不要向Ceph再写入任何数据。</li>
</ul>


<p>默认情况下full的比例是95%，而near full的比例是85%，所以需要根据实际情况对该配置进行调整。</p>

<pre><code class="plain /etc/ceph/ceph.conf">[global]
    mon osd full ratio = .98
    mon osd nearfull ratio = .80
</code></pre>

<h2>分析总结</h2>

<h3>原因</h3>

<p>根据Ceph官方文档中的描述，当一个OSD full比例达到95%时，集群将不接受任何Ceph Client端的读写数据的请求。所以导致虚拟机在重启时，无法启动的情况。</p>

<h3>解决方法</h3>

<p>从官方的推荐来看，应该比较支持添加新的OSD的方式，当然临时的提高比例是一个解决方案，但是并不推荐，因为需要手动的删除数据去解决，而且一旦再有一个新的节点出现故障，仍然会出现比例变满的状况，所以解决之道最好是扩容。</p>

<h3>思考</h3>

<p>在这次故障过程中，有两点是值得思考的：</p>

<ul>
<li>监控：由于当时服务器在配置过程中DNS配置错误，导致监控邮件无法正常发出，从而没有收到Ceph WARN的提示信息</li>
<li>云平台本身： 由于Ceph的机制，在OpenStack平台中分配中，大多时候是超分的，从用户角度看，拷贝大量数据的行为并没有不妥之处，但是由于云平台并没有相应的预警机制，导致了该问题的发生</li>
</ul>


<h2>参考文档</h2>

<ul>
<li><a href="http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity">http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph可靠性的量化分析]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability/"/>
    <updated>2015-03-04T08:36:06+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability</id>
    <content type="html"><![CDATA[<p>在开始正文之前，首先要感谢UnitedStack工程师朱荣泽对这篇博文的大力帮助和悉心指教。本文主要针对UnitedStack公司在巴黎峰会上对Ceph可靠性的计算方法(<a href="https://www.ustack.com/blog/build-block-storage-service/">https://www.ustack.com/blog/build-block-storage-service/</a>)做了一个更明确的分析和阐述，供对此话题感兴趣的朋友一起来讨论、研究，文章中如果有不当之处，还请各位高人指教。</p>

<!-- more -->


<h2>什么情况下数据会丢失？</h2>

<p>这个话题的另外一种提法就是存储的可靠性，所谓存储的可靠性最基本的一点就是数据不要丢失，也就是我们俗称的“找不回来了”。所以，要分析Ceph的可靠性我们只需要搞清楚，到底在什么情况下我们的数据会丢失，并且再也无法恢复，基于此我们便可以创建我们的计算模型。</p>

<p>我们先来假定一套简单的Ceph环境，3个OSD节点，每个OSD节点对应一块物理硬盘，副本数为3。那么我们排除MON的因素影响Ceph集群的运行的问题，显而易见，当三个OSD对应的物理硬盘全部损坏时，数据必然无法恢复。所以此时集群的可靠性是与硬盘本身的可靠性直接相关。</p>

<p>我们再来假定一套更大的Ceph环境，30个OSD节点，分3个机架摆放，每一个机架有10个OSD节点，每个OSD节点仍然对应一块物理硬盘，副本数为3，并且通过CRUSH MAP，将每一份副本均匀分布在三个机架上，不会出现两份副本同时出现在一个机架的情况。此时，什么时候会出现数据丢失的情况呢？当三个机架上都有一块硬盘损坏，而恰恰这三块硬盘又保存了同一个Object的全部副本，此时数据就会出现丢失的情况。</p>

<p>所以根据以上的分析，我们认为，Ceph的可靠性的计算是与OSD的数量(N)、副本数(R)、每一个服务节点的OSD数量(S)、硬盘的年失败概率(AFR)。这里我们使用UnitedStack相关参数进行计算。</p>

<p>具体取值如下图所示：</p>

<p><img src="/images/blogs/ceph-reliability-formula.jpg" width="640" height="480"></p>

<p><img src="/images/blogs/ceph-reliability-constant.jpg" width="640" height="480"></p>

<h2>硬盘年失败概率</h2>

<p>根据维基百科的计算方法(<a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">http://en.wikipedia.org/wiki/Annualized_failure_rate</a>)，AFR的计算方法如下：</p>

<p><img src="/images/blogs/ceph-reliability-afr.jpg" width="640" height="480"></p>

<p>例如，计算Seagate某企业级硬盘的AFR，根据文档查到MTBF为1,200,000小时，则AFR为0.73%，计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-afr-example.jpg" width="640" height="480"></p>

<p>但是，根据Google的相关计算，在一个大规模集群环境下，往往AFR的值并没有硬盘厂商那样乐观，下面的统计告诉了我们在真实环境下AFR变化的情况：</p>

<p><img src="/images/blogs/ceph-reliability-google-afr.jpg" width="640" height="480"></p>

<p>所以我们可以看到实际的AFR的变化范围随着年份而变化，取值范围在1.7%-8%左右，所以本文中AFR为1.7%。</p>

<h2>硬盘在一年之内损坏的概率</h2>

<p>有了AFR，我们就可以尝试计算硬盘在一年中出现故障的概率，根据相关研究，硬盘在一定时间内的失败概率符合Possion分布(已经把知识还给老师的同学请移步：<a href="http://en.wikipedia.org/wiki/Poisson_distribution">http://en.wikipedia.org/wiki/Poisson_distribution</a>)。具体的计算公式为：</p>

<p><img src="/images/blogs/ceph-reliability-Pn.jpg" width="640" height="480"></p>

<p>当我最初拿到这个计算公式时，一下子懵了，到底该如何确定数学期望值lamda呢？</p>

<h2>lamda的计算过程</h2>

<p>根据相关的研究资料，单块的硬盘损坏的期望值(Failures in Time)是指每10亿小时硬盘的失败率(Failure Rate λ)，计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-fit.jpg" width="640" height="480"></p>

<p>这里的Af(Acceleration Factor)是由测试时间乘以阿伦尼乌斯方程的值得出来的结果，好吧，我承认，我也是现学现卖，这个方程式是化学反应的速率常数与温度之间的关系式，适用于基元反应和非基元反应，甚至某些非均相反应。不过可以看出Failure Rate的计算过程实质主要是计算环境因素引起的物理变化，最终导致失败的数学期望值。所以根据相关研究，最终FIT的计算方法为：</p>

<p><img src="/images/blogs/ceph-reliability-fit-afr.jpg" width="640" height="480"></p>

<p>有了这些参数后，我们就可以开始正式计算Ceph集群中，不同机架上有三块硬盘同时出现损坏的概率啦。</p>

<h2>任意一个OSD出现损坏的概率P1(any)</h2>

<p>我们不太容易直接去计算任意一个OSD出现损坏的概率，但是我们很容易计算没有OSD出现问题的概率，方法如下，用一减去无OSD节点出现问题的概率，得到P1(any)。</p>

<p><img src="/images/blogs/ceph-reliability-osd1-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第二个节点出现故障的概率P2(any)</h2>

<p>我们知道当Ceph发现一个有问题的OSD节点时，会自动的将节点OUT出去，这个时间大约为10min，同时Ceph的自我修复机制会自动平衡数据，将故障节点的数据重新分配在其他的OSD节点上。</p>

<p>我们假设我们单盘的容量为1000 GB，使用率为75%，也就是此时将有750 GB的数据需要同步。我们的数据只在本机架平衡，节点写入速度为50 MB/s，计算方法如下：</p>

<p><img src="/images/blogs/ceph-reliability-recovery-time.jpg" width="640" height="480"></p>

<p>注意：由于每个节点有三个OSD，所以要求每台物理机所承受的节点带宽至少要大于150 MB/s。并且在这个计算模型下，并没有计算元数据、请求数据、IP包头等额外的信息的大小。</p>

<p>有了Recovery Time，我们就可以计算我们第二个节点在Recovery Time内失败的概率，具体的计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-osd2-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第三个节点出现故障的概率P3(any)</h2>

<p>计算方法同上，计算过程如下：</p>

<p><img src="/images/blogs/ceph-reliability-osd3-failure.jpg" width="640" height="480"></p>

<h2>一年内任意副本数&reg;个OSD出现故障的概率</h2>

<p>所以将上述概率相乘即可得到一年内任意副本数&reg;个OSD出现故障的概率。</p>

<p><img src="/images/blogs/ceph-reliability-arbitrary-osd-failure.jpg" width="640" height="480"></p>

<h2>Copy Sets(M)</h2>

<p>在这个计算模型中，因为任意R个OSD节点的损坏并不意外着副本的完全丢失，因为损坏的R个OSD未必保存着一个Object的全部副本信息，所以未必造成数据不可恢复，所以这里引入了Copy Sets的概念。简单来说，Copyset就是存放所有拷贝的一个集合，具体的定义和计算方法可以查看参考链接。那么这里的场景下，Copy Sets为三个机架OSD数量相乘，即M=24<em>24</em>24。当然如果是两个副本的情况下，M应该为24<em>24+24</em>24+24*24。</p>

<p><img src="/images/blogs/ceph-reliability-copysets.jpg" width="640" height="480"></p>

<h2>CEPH的可靠性</h2>

<p>所以最终归纳出CEPH可靠性的算法为：</p>

<p><img src="/images/blogs/ceph-reliability-copysets-failure.jpg" width="640" height="480"></p>

<p>可以看出Ceph三副本的可靠性大约为9个9，由于Recovery Time和AFR取值的问题，所以计算结果和UnitedStack上略有出入。</p>

<h2>参考链接</h2>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">Annualized Failure Rate</a></li>
<li><a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a></li>
<li><a href="http://www.microsemi.com/document-portal/doc_view/124041-calculating-reliability-using-fit-mttf-arrhenius-htol-model">Calculating Reliability using FIT &amp; MTTF: Arrhenius HTOL Model</a></li>
<li><a href="http://storagemojo.com/2007/02/19/googles-disk-failure-experience/">Google’s Disk Failure Experience</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/disk_failures.pdf">Failure Trends in a Large Disk Drive Population</a></li>
<li><a href="http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/11727-atc13-cidon.pdf">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
