<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2015-03-04T16:23:26+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ceph可靠性的量化分析]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability/"/>
    <updated>2015-03-04T08:36:06+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability</id>
    <content type="html"><![CDATA[<p>在开始正文之前，首先要感谢UnitedStack工程师朱荣泽对这篇博文的大力帮助和悉心指教。本文主要针对UnitedStack公司在巴黎峰会上对Ceph可靠性的计算方法(<a href="https://www.ustack.com/blog/build-block-storage-service/">https://www.ustack.com/blog/build-block-storage-service/</a>)做了一个更明确的分析和阐述，供对此话题感兴趣的朋友一起来讨论、研究，文章中如果有不当之处，还请各位高人指教。</p>

<!-- more -->


<h2>什么情况下数据会丢失？</h2>

<p>这个话题的另外一种提法就是存储的可靠性，所谓存储的可靠性最基本的一点就是数据不要丢失，也就是我们俗称的“找不回来了”。所以，要分析Ceph的可靠性我们只需要搞清楚，到底在什么情况下我们的数据会丢失，并且再也无法恢复，基于此我们便可以创建我们的计算模型。</p>

<p>我们先来假定一套简单的Ceph环境，3个OSD节点，每个OSD节点对应一块物理硬盘，副本数为3。那么我们排除MON的因素影响Ceph集群的运行的问题，显而易见，当三个OSD对应的物理硬盘全部损坏时，数据必然无法恢复。所以此时集群的可靠性是与硬盘本身的可靠性直接相关。</p>

<p>我们再来假定一套更大的Ceph环境，30个OSD节点，分3个机架摆放，每一个机架有10个OSD节点，每个OSD节点仍然对应一块物理硬盘，副本数为3，并且通过CRUSH MAP，将每一份副本均匀分布在三个机架上，不会出现两份副本同时出现在一个机架的情况。此时，什么时候会出现数据丢失的情况呢？当三个机架上都有一块硬盘损坏，而恰恰这三块硬盘又保存了同一个Object的全部副本，此时数据就会出现丢失的情况。</p>

<p>所以根据以上的分析，我们认为，Ceph的可靠性的计算是与OSD的数量(N)、副本数(R)、每一个服务节点的OSD数量(S)、硬盘的年失败概率(AFR)。这里我们使用UnitedStack相关参数进行计算。</p>

<p>具体取值如下图所示：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-formula.jpg" width="640" height="480"></p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-constant.jpg" width="640" height="480"></p>

<h2>硬盘年失败概率</h2>

<p>根据维基百科的计算方法(<a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">http://en.wikipedia.org/wiki/Annualized_failure_rate</a>)，AFR的计算方法如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-afr.jpg" width="640" height="480"></p>

<p>例如，计算Seagate某企业级硬盘的AFR，根据文档查到MTBF为1,200,000小时，则AFR为0.73%，计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-afr-example.jpg" width="640" height="480"></p>

<p>但是，根据Google的相关计算，在一个大规模集群环境下，往往AFR的值并没有硬盘厂商那样乐观，下面的统计告诉了我们在真实环境下AFR变化的情况：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-google-afr.jpg" width="640" height="480"></p>

<p>所以我们可以看到实际的AFR的变化范围随着年份而变化，取值范围在1.7%-8%左右，所以本文中AFR为1.7%。</p>

<h2>硬盘在一年之内损坏的概率</h2>

<p>有了AFR，我们就可以尝试计算硬盘在一年中出现故障的概率，根据相关研究，硬盘在一定时间内的失败概率符合Possion分布(已经把知识还给老师的同学请移步：<a href="http://en.wikipedia.org/wiki/Poisson_distribution">http://en.wikipedia.org/wiki/Poisson_distribution</a>)。具体的计算公式为：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-Pn.jpg" width="640" height="480"></p>

<p>当我最初拿到这个计算公式时，一下子懵了，到底该如何确定数学期望值lamda呢？</p>

<h2>lamda的计算过程</h2>

<p>根据相关的研究资料，单块的硬盘损坏的期望值(Failures in Time)是指每10亿小时硬盘的失败率(Failure Rate λ)，计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-fit.jpg" width="640" height="480"></p>

<p>这里的Af(Acceleration Factor)是由测试时间乘以阿伦尼乌斯方程的值得出来的结果，好吧，我承认，我也是现学现卖，这个方程式是化学反应的速率常数与温度之间的关系式，适用于基元反应和非基元反应，甚至某些非均相反应。不过可以看出Failure Rate的计算过程实质主要是计算环境因素引起的物理变化，最终导致失败的数学期望值。所以根据相关研究，最终FIT的计算方法为：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-fit-afr.jpg" width="640" height="480"></p>

<p>有了这些参数后，我们就可以开始正式计算Ceph集群中，不同机架上有三块硬盘同时出现损坏的概率啦。</p>

<h2>任意一个OSD出现损坏的概率P1(any)</h2>

<p>我们不太容易直接去计算任意一个OSD出现损坏的概率，但是我们很容易计算没有OSD出现问题的概率，方法如下，用一减去无OSD节点出现问题的概率，得到P1(any)。</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-osd1-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第二个节点出现故障的概率P2(any)</h2>

<p>我们知道当Ceph发现一个有问题的OSD节点时，会自动的将节点OUT出去，这个时间大约为10min，同时Ceph的自我修复机制会自动平衡数据，将故障节点的数据重新分配在其他的OSD节点上。</p>

<p>我们假设我们单盘的容量为1000 GB，使用率为75%，也就是此时将有750 GB的数据需要同步。我们的数据只在本机架平衡，节点写入速度为50 MB/s，计算方法如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-recovery-time.jpg" width="640" height="480"></p>

<p>注意：由于每个节点有三个OSD，所以要求每台物理机所承受的节点带宽至少要大于150 MB/s。并且在这个计算模型下，并没有计算元数据、请求数据、IP包头等额外的信息的大小。</p>

<p>有了Recovery Time，我们就可以计算我们第二个节点在Recovery Time内失败的概率，具体的计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-osd2-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第三个节点出现故障的概率P3(any)</h2>

<p>计算方法同上，计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-osd3-failure.jpg" width="640" height="480"></p>

<h2>一年内任意副本数&reg;个OSD出现故障的概率</h2>

<p>所以将上述概率相乘即可得到一年内任意副本数&reg;个OSD出现故障的概率。</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-arbitrary-osd-failure.jpg" width="640" height="480"></p>

<h2>Copy Sets(M)</h2>

<p>在这个计算模型中，因为任意R个OSD节点的损坏并不意外着副本的完全丢失，因为损坏的R个OSD未必保存着一个Object的全部副本信息，所以未必造成数据不可恢复，所以这里引入了Copy Sets的概念。简单来说，Copyset就是存放所有拷贝的一个集合，具体的定义和计算方法可以查看参考链接。那么这里的场景下，Copy Sets为三个机架OSD数量相乘，即M=24<em>24</em>24。当然如果是两个副本的情况下，M应该为24<em>24+24</em>24+24*24。</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-copysets.jpg" width="640" height="480"></p>

<h2>CEPH的可靠性</h2>

<p>所以最终归纳出CEPH可靠性的算法为：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-copysets-failure.jpg" width="640" height="480"></p>

<p>可以看出Ceph三副本的可靠性大约为9个9，由于Recovery Time和AFR取值的问题，所以计算结果和UnitedStack上略有出入。</p>

<h2>参考链接</h2>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">Annualized Failure Rate</a></li>
<li><a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a></li>
<li><a href="http://www.microsemi.com/document-portal/doc_view/124041-calculating-reliability-using-fit-mttf-arrhenius-htol-model">Calculating Reliability using FIT &amp; MTTF: Arrhenius HTOL Model</a></li>
<li><a href="http://storagemojo.com/2007/02/19/googles-disk-failure-experience/">Google’s Disk Failure Experience</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/disk_failures.pdf">Failure Trends in a Large Disk Drive Population</a></li>
<li><a href="http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/11727-atc13-cidon.pdf">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack虚拟机的高可靠]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/03/openstack-instance-ha-high-availability/"/>
    <updated>2015-03-03T20:58:19+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/03/openstack-instance-ha-high-availability</id>
    <content type="html"><![CDATA[<p>译者注：OpenStack虚拟机级别的HA是在企业私有云中必须提及的话题，换句话说如果没有此机制，很多企业级用户根本不会考虑使用OpenStack+开源云平台的解决方案。这一点也得益于VMware等大公司孜孜不倦、持之以恒、长年累月的对用户的洗脑，这样的洗脑让用户觉得一些VMware的功能成为了“云”的标准(HA/FT/DRS/DPM等)。个人认为，OpenStack这部分功能的缺失也直接的阻碍了OpenStack进入中国行业用户的步伐，记得在夏天的时候，华为曾经为社区提供了一个HA的解决方案(用C语言实现)，打算贡献给社区，但是经过社区激烈的讨论，终于不了了之。</p>

<p>在我所经历的私有云项目中，也尝试了一些虚拟机级别的HA解决方案，我的个人观点是，如果将OpenStack作为一个项目，可以不提供HA，但是作为产品，必须要提供HA的解决方案(私有云)。这篇文章中提到HA实现的过程，也恰恰是我们在实际中遇到的状况，下面让我们来看一看OpenStack社区是如何设想实现该问题的。感谢 @陈沙克 微博提供的资料。</p>

<!-- more -->


<p>原文地址：<a href="http://blog.russellbryant.net/2014/10/15/openstack-instance-ha-proposal/">http://blog.russellbryant.net/2014/10/15/openstack-instance-ha-proposal/</a></p>

<p>在一个完美世界中(不是那个游戏公司)，OpenStack承载的云主机上运行的应用都应当具备天然的横向扩展能力和容灾能力。但是现实世界却并不是这样。我们一直看到在OpenStack运行传统负载的需求积极伴随而来的HA的需求。</p>

<p>传统应用运行在OpenStack在大多数情况下是没有问题的。一些需要可靠性要求的应用部署在OpenStack上是没有自动提供这种能力的。如果虚拟化软件挂掉了，没人会去拯救运行在上面的虚拟机。OpenStack中有手动拯救的能力，但是这需要云平台的运维人员或者外部的工具完成。</p>

<p>这篇文章主要讨论如何在虚拟化程序失败后自动侦测并拯救运行在上面的虚拟机于水火之中。对于不同的虚拟机化软件有不同的解决方案。我这里主要关注libivrt/kvm，下面的部分也主要围绕着它进行。除非特别提及libvirt，我想所有提及的一切也同样适用于xenserver驱动。</p>

<p>这是OpenStack社区的定期讨论的话题。已经有反对将这个功能放到OpenStack。无论哪个功能组件被使用，我想我们应该提供一个针对问题的解决方案。我想使用现有的软件来解决这个问题。</p>

<h3>范围</h3>

<p>我们主要针对基础架构层的故障。也有其他的一些因素会影响应用的可靠性。客户机的操作系统或者软件自身出现故障。对于这种故障的恢复，我们主要留给应用的开发者或部署者。</p>

<p>值得注意的是，OpenStack中的libvirt/kvm驱动并不包含于客户机操作系统故障相关的功能。在Icehouse版本中的Nova实现了一个libvirt-watchdog的blueprint。这个功能允许你设置hw_watchdog_action属性在image或是flavor中。合法的值包含：poweroff，rest，pause和none。当选项被打开时，libvirt会为客户机开启i7300esb watchdog驱动，并且当watchdog被出发时发送动作。这可能是对你客户机故障恢复策略很有帮助的部件。</p>

<h3>架构</h3>

<p>解决这个问题需要一些关键的部件：</p>

<ul>
<li>监控(Monitoring) - 系统检测到虚拟化层的故障</li>
<li>隔离(或是围栏，Fencing) - 系统隔离故障计算节点</li>
<li>恢复(Recovery) - 从故障的虚拟化上恢复虚拟机</li>
</ul>


<h3>监控</h3>

<p>针对这种解决方案的监控部件有两个主要需求：</p>

<ul>
<li>检测主机是否已经失败</li>
<li>针对错误出发自动的相应(隔离和恢复)</li>
</ul>


<p>通常建议这个解决方案应该是OpenStack的一部分。很多人建议这个功能应该在Nova中实现。将这个部分放在Nova中主要是因为，Nova已经能过获知运行环境下的基础架构的健康状况。servicegroup API能够提供基础的组的成员信息。特别是他持续跟踪计算节点的活跃状态。然而，这只能提供你nova-compute服务本身的状态。对于客户的虚拟机而言(即使nova-compute不再运行)，他们也能运行良好。将基础架构层的状态信息放入Nova之中，有违Nova的层级。无论如何，这将对Nova有一个重大的(管理)范围的扩大，所以我也不指望Nova团队会同意这么做。</p>

<p>还有一种建议是将功能做进Heat里。最重要的功能问题是，我们不该让云平台用户去使用Heat重启他们出现故障的虚拟机。另外一种建议是用其他的(可能是全新的)OpenStack模块来做这件事情。像我不喜欢这些放在Nova的理由一样，我也不认为他们该放在新的模块一样( I don’t like that for many of the same reasons I don’t think it should be in Nova.)。我觉得这件事情应该是基础架构层需要支持的，而不是OpenStack自己。</p>

<p>放弃将这部分功能放入OpenStack的想法，我认为应当从基础架构的角度入手支持OpenStack部署。许多OpenStack部署中已经使用了Pacemaker提供部署中的高可靠。从历史的角度来看，由于集群中横向限制，Pacemaker并不会用在计算节点上，因为他们太多了。这个限制其实在Corosync而不是Pacemaker本身。最近，Pacemaker提供了一个新功能叫做pacemaker_remote，允许将一台主机作为Pacemaker集群的一部分，而不需要作为Corosync集群的一部分。这样看起来可以被作为OpenStack计算节点的一个合适的解决方案。</p>

<p>许多OpenStack部署中也使用监控工具像Nagios来监控他们的计算节点。这也很合理。</p>

<h3>隔离(Fencing)</h3>

<p>概括一下，隔离就是将故障的计算节点完全隔离(isolates)。举个例子，这个可能由IPMI保证失败的节点已经关机了。隔离非常重要，有以下几点原有。很多原因都会造成节点故障，我们在将同样的虚拟机恢复之前，完全确认它确实不在(completely gone)啦。我们不想让我们的虚拟机跑两份。用户也肯定不想看到。更糟糕的是，处理自动疏散(evacuate)时，OpenStack的部署可能是基于共享存储的，跑两个一样的虚拟机可能会引起数据损坏，因为两个虚拟机尝试使用同一块磁盘。另外一个问题就是，在网络上产生两个相同的IP。</p>

<p>用Pacemaker最大的好处就是他内建隔离，因为这正是HA解决方案中的一个核心组件。如果你使用Nagios，隔离的集成需要你自己实现。</p>

<h3>恢复</h3>

<p>一旦故障被检测到并且计算节点被隔离，需要触发疏散(evacuate)。概括一下，所谓疏散就是将一台故障节点的虚拟机实例在另外一个节点上启动起来。Nova提供了API去疏散一个实例。这个功能正常工作的前提是需要虚拟机磁盘文件在共享存储上。或者是从卷启动的。有意思的是，即使疏散中没有以上两个条件，API仍然可以运行。结果就是用基础镜像重新启动一个新的实例而没有任何数据。这样的唯一好处就是你得到一个和你之前虚拟机相同UUID的实例。</p>

<p>一个通用的疏散用例是“从一个指定的Host上疏散所有虚拟机”。因为这个太通用了，所以这个功能在novalcient库中实现了(译者：Juno更新日志提到了这一点)。所以监控工具可以通过novaclient触发这个功能。</p>

<p>如果这个功能在你的OpenStack的部署上用于所有虚拟机，那么我们的解决方案还是挺好的。许多人有了额外的需求：用户应该可以自行对每一个虚拟机做(HA)的设定。这个的确很合理，但是却带来了一个额外的问题。我们在OpenStack中该如何让用户指定哪台虚拟机需要被自动恢复？</p>

<p>通常的方式就是用镜像的属性或者规格的extra-specs。这样当然可以工作，但是对于我好像并不太灵活。我并不认为用户应该创建一个新的镜像叫做“让这个虚拟机一直运行”。Flavor的extra-specs还行，如果你觉得为你所有虚拟机使用特殊的flavor或者是flavor类。在任何一种情况下，都需要修改novaclient的&#8221;疏散一个Host&#8221;来支持他。</p>

<p>另外一种潜在的解决方案是使用一个用户自定义的特殊标记。已经有一个正在review的功能提供一个API来给虚拟机打标签(tagging)。对于我们的讨论，我们假设标签是“自动回复”。我们也需要更新novaclient来支持“将所有带指定标记的虚拟机从Host疏散”。监控工具也会触发这个功能，让novalcient将带有“自动恢复“标记的所有虚拟机从Host疏散。</p>

<h3>结论和下一步计划</h3>

<p>虚拟机的HA显然是许多部署需要提供的功能。我相信可以通过在部署中将现有软件进行集成方式实现，特别是Pacemaker。下一步就是提供具体的信息，如何建立已经如何测试。</p>

<p>我希望有人可能说”但是我已经使用系统Foo(Nagios或其他的什么)来监控我的计算节点“。你也可以按照这条路尝试。我不确认如何将Nagios之类的监控软件和隔离部分进行整合。如果在这个解决方案中跳过隔离，你要在失败时保持和平(keep the pieces when it breaks，译者：就是上面提到的fencing出现的问题)。除此之外，你的监控系统能够像Pacemaker一样出发novalcient的疏散功能。</p>

<p>未来非常好的开发方向可能是将这个功能集成到OpenStack管理界面。我希望通过部署的控制面板告诉我哪些失败了，出发了哪些响应动作。这个需要pcsd提供REST API(WIP)来导出这些信息。</p>

<p>最后，值得思考一下TripleO在这个问题的内容。如果你使用OpenStack部署OpenStack，解决方案是不是不同呢？在那个世界里，你所有的裸金属节点都是通过OpenStack Ironic管理的资源。Ceilometer可以被用于监控这些资源。如果是那样，OpenStack本身就有足够的信息来支持基础设施完成这个功能。再次强调，为了避免对OpenStack的改造，我们在这种条件小也应该使用更通用的Pacemaker解决方案。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用MongoDB作为Salt Pillar后端存储数据]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/02/23/use-mongodb-to-store-salt-pillar/"/>
    <updated>2015-02-23T15:19:21+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/02/23/use-mongodb-to-store-salt-pillar</id>
    <content type="html"><![CDATA[<p>今天在查找salt中pillar嵌套pillar的方法时，无意之间发现了pillar除了可以直接使用文件(sls)外，也同时支持多种后端的数据存储方式。例如：MySQL, MongoDB, Ldap, json, cobbler甚至是puppet。这无疑为开发中的接口提供了极大的便利。</p>

<!-- more -->


<p>详细的支持列表可见：<a href="http://docs.saltstack.com/en/latest/ref/pillar/all/index.html#all-salt-pillars">http://docs.saltstack.com/en/latest/ref/pillar/all/index.html#all-salt-pillars</a></p>

<p>严格意义上来说，这篇博文并非完全原创，英文原文请参考：<a href="http://www.tmartin.io/articles/2014/salt-pillar-mongodb/">http://www.tmartin.io/articles/2014/salt-pillar-mongodb/</a></p>

<p>下面就来说说详细的配置方式，假定你已经有了一个部署好的salt环境，并且正确配置了salt master和salt minion，并且完成认证，主机名为salt-master.salt.com，这里我们使用Ubuntu 12.04 64bit作为演示环境。</p>

<h2>安装MongoDB和Python MongoDB</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apt-get install mongodb python-pymongo python-pymongo-ext</span></code></pre></td></tr></table></div></figure>


<p>确保你能连接到MongoDB</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mongo</span></code></pre></td></tr></table></div></figure>


<pre><code>MongoDB shell version: 2.2.3
connecting to: test
&gt;
</code></pre>

<h2>创建MongoDB数据库和存放Pillar的Collection</h2>

<ul>
<li>创建数据库pillar</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>use pillar</span></code></pre></td></tr></table></div></figure>


<ul>
<li>在数据库中插入pillar数据</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>db.pillar.insert({
</span><span class='line'>    _id: 'salt-master.salt.com',
</span><span class='line'>    mongo_pillar: {
</span><span class='line'>        key1: "value1",
</span><span class='line'>        key2: "value2",
</span><span class='line'>    }})</span></code></pre></td></tr></table></div></figure>


<p>注意：这里的_id必须要和你的minion节点的主机名一致，并且无法使用通配符，也就是一个节点都有自己一套独立的pillar，这一点和文件中定义pillar有很大的不同。mongo_pillar部分中是定义的是pillar中的内容，也就是我们可以直接引用的部分。</p>

<h3>配置Salt Master</h3>

<ul>
<li><p>下一步就是告诉Salt Master，我们在MongoDB中存放了pillar数据，需要劳您大驾，移步MongoDB读取数据。修改：</p>

<p>  /etc/salt/master</p></li>
</ul>


<p>添加</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mongo.db: "pillar"
</span><span class='line'>mongo.host: "localhost"
</span><span class='line'>ext_pillar:
</span><span class='line'>    - mongo: {}</span></code></pre></td></tr></table></div></figure>


<p>注意：如果需要使用不同于标准安装接口，请使用mongo.port，如果需要配置用户名和密码，请使用mongo.user和mongo.password。其他参数定义，请详见：<a href="http://docs.saltstack.com/en/latest/ref/pillar/all/salt.pillar.mongo.html#module-documentation">http://docs.saltstack.com/en/latest/ref/pillar/all/salt.pillar.mongo.html#module-documentation</a></p>

<ul>
<li>测试</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>salt salt-master.salt.com pillar.item mongo_pillar</span></code></pre></td></tr></table></div></figure>


<p>返回</p>

<pre><code>salt-master.salt.com:
    ----------
    mongo_pillar:
        ----------
        key1:
            value1
        key2:
            value2
</code></pre>

<ul>
<li>如果想在sls中直接使用</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{{ salt['pillar.get']('mongo_pillar:key1') }}</span></code></pre></td></tr></table></div></figure>


<h2>总结</h2>

<p>pillar应该是salt中一个比较灵活的配置选项，个人理解pillar的作用就像puppet中init定义的初始化的参数的默认值，每次部署时，只需要更改pillar的文件就可以啦。但是随着代码的增长(主要用于部署OpenStack)，发现pillar的管理越来越难，pillar本身对如何组织结构并没有严格的限制，而且嵌套(extend)功能暂时还不能很完美的支持(<a href="https://github.com/saltstack/salt/issues/3991">https://github.com/saltstack/salt/issues/3991</a>)，这也给pillar的管理提高了复杂度。</p>
]]></content>
  </entry>
  
</feed>
