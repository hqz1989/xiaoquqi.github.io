<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[RaySun's Blog]]></title>
  <link href="http://xiaoquqi.github.io/atom.xml" rel="self"/>
  <link href="http://xiaoquqi.github.io/"/>
  <updated>2015-05-06T20:19:46+08:00</updated>
  <id>http://xiaoquqi.github.io/</id>
  <author>
    <name><![CDATA[Ray Sun ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OpenStack Kilo版本新功能分析]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/05/04/what-is-new-in-kilo/"/>
    <updated>2015-05-04T10:37:22+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/05/04/what-is-new-in-kilo</id>
    <content type="html"><![CDATA[<p>OpenStack Kilo版本已经于2015年4月30日正式Release，这是OpenStack第11个版本，距离OpenStack项目推出已经整整过去了5年多的时间。在这个阶段OpenStack得到不断的增强，同时OpenStack社区也成为即Linux之后的第二大开源社区，参与的人数、厂商众多，也成就了OpenStack今天盛世的局面。虽然OpenStack在今年经历了Nebula的倒闭，但是随着国内的传统行业用户对OpenStack越来越重视，我们坚信OpenStack明天会更好。</p>

<p>OpenStack Kilo版本的完整翻译版本可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans">https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans</a></p>

<p>OpenStack Kilo版本的翻译工作由我和我的同事裴莹莹(Wendy)共同完成，翻译校对工作由裴莹莹完成。如果翻译有任何问题，请各位多多指正。</p>

<!-- more -->


<h2>社区贡献分析</h2>

<p>我们先来看一下OpenStack在最近的4个稳定版本发布中，每一个项目的贡献情况：</p>

<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/what-is-new-in-kilo-contribution-by-modules.jpg"></p>

<p>我们能够很明显的发现，OpenStack最早的几大核心模块(Nova, Cinder, Glance, Keystone, Horizon, Swift)的代码贡献所占比例呈明显下降趋势，这里强调一下，是比例而不是数量，从数量上来看，版本之间相差并不大，以Nova为例，从Havana版本的24%下降到如今的10%。这从一个侧面反映了OpenStack的核心模块日趋稳定，更多的关注集中到更高层次或者功能优化上。</p>

<p>Neutron模块则一直处于稳中有升的状态,从Havana版本的7%上升到10%，说明Neutron仍然处于需要进一步完善的状态。</p>

<p>对于Ceilometer，Heat，Sahara，Ironic, Trove等新晋的核心模块，都处于稳步增长的阶段。贡献的比例在四个版本中基本保持持平的态势。在Kilo版本中，Sahara和Heat进入了前十名。</p>

<p>从Kilo版本的比例来看，Others的比例过半，Others主要包括了OpenStack测试相关项目，例如Rally；开发相关项目，例如Devstack;以及一些新的模块，例如：Manila，Magnum等众多进入孵化器的项目;还包括所有的Client以及Spec等。可以预见，OpenStack的开发重心逐步从底层的核心模块，逐步向更高层次、提供更丰富功能的方向发展。</p>

<h2>国内社区贡献分析</h2>

<p><img class="center" src="http://xiaoquqi.github.io/images/blogs/what-is-new-in-kilo-contributor.png"></p>

<p>从企业贡献排名来看，几大巨头企业牢牢占据贡献榜的前几名，OpenStack最成功的公司-Mirantis排名紧追Redhat成为第二贡献大户。排名前几位的公司还包括：IBM, Rackspace, Cisco, Suse, VMware, Intel等。</p>

<p>国内方面，华为继续稳定在第13名，但Review的数量从Juno版本的1353提升到2548个，贡献的项目几乎涵盖所有的项目，主要贡献来自Heat，Ceilometer, Horizon，Neutron, Nova等项目。</p>

<p>国内排名第2的贡献企业是九州云，排名达到了21位，看来龚永生的到来为九州云添加了无限活力。九州云的贡献主要来自Horizon和Neutron两个项目，龚永生不愧为Neutron的Core，在网络方面的贡献，九州云的确很给力。</p>

<p>排名第3的企业是海云捷迅，排名为44位，海云是国内比较早的一批OpenStack创业企业，贡献方面以Sahara，Neutron，Nova，oslo.messaging以及Cinder为主，从之前了解的情况来看，海云的项目不少，可能提交的修改是与在实际项目中遇到的问题有关。</p>

<p>排名之后的企业还有Kylin Cloud，UnitedStack，EasyStack等。由于是手工统计，在统计过程中如有遗漏，希望大家多多指正。</p>

<h2>Horizon新功能</h2>

<p>Horizon在K版本除了增强了对新增模块的支持，从UE的角度也为我们带来了很多新功能</p>

<ul>
<li>支持向导式的创建虚拟机，现在还处于beta版本，如果想在Horizon里激活，可以通过设置local_setting.py的配置实现：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>LAUNCH_INSTANCE_NG_ENABLED = True</span></code></pre></td></tr></table></div></figure>


<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/what-is-new-in-kilo-instance-guide1.png"></p>

<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/what-is-new-in-kilo-instance-guide2.png"></p>

<ul>
<li>支持简单的主题，主要通过修改<em>variables.scss和</em>style.scss完成对主题颜色和简单样式的修改，但是格局不能改变，修改local_settings.py</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>CUSTOM_THEME_PATH = 'static/themes/blue'</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>static/themes/blue/_variables.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$gray:                   #2751DB !default;
</span><span class='line'>$gray-darker:            #94A5F2 !default;
</span><span class='line'>$gray-dark:              #0C0CED !default;
</span><span class='line'>$gray-light:             #C7CFF2 !default;
</span><span class='line'>$gray-lighter:           #DCE1F5 !default;
</span><span class='line'>
</span><span class='line'>$brand-primary:         #375A7F !default;
</span><span class='line'>$brand-success:         #00bc8c !default;
</span><span class='line'>$brand-info:            #34DB98 !default;
</span><span class='line'>$brand-warning:         #F39C12 !default;
</span><span class='line'>$brand-danger:          #E74C3C !default;</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// Blue
</span><span class='line'>// ----
</span><span class='line'>
</span><span class='line'>@mixin btn-shadow($color) {
</span><span class='line'>  @include gradient-vertical-three-colors(lighten($color, 3%), $color, 6%, darken($color, 3%));
</span><span class='line'>  filter: none;
</span><span class='line'>  border: 1px solid darken($color, 10%);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>// Buttons ====================================================================
</span><span class='line'>
</span><span class='line'>.btn-default,
</span><span class='line'>.btn-default:hover {
</span><span class='line'>  @include btn-shadow($btn-default-bg);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>.btn-primary,
</span><span class='line'>.btn-primary:hover {
</span><span class='line'>  @include btn-shadow($btn-primary-bg);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/what-is-new-in-kilo-horizon-theme1.png"></p>

<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/what-is-new-in-kilo-horizon-theme2.png"></p>

<h2>Nova新功能</h2>

<h3>Nova Scheduler</h3>

<ul>
<li>标准化了conductor，compute与scheduler的接口，为之后的接口分离做好准备</li>
<li>对Scheduler做了一些优化，例如：Scheduler对于每一个请求都会重新进行Filters/Weighers，为了优化这个问题，将filter/weighter的初始化从handler移到scheduler，这样每次请求的时候都可以重新使用了。</li>
<li>在下一个版本的时候，Scheduler将会分离到Gantt，所以在这个版本中为分离做了一些准备</li>
</ul>


<h3>Libvirt NFV相关功能</h3>

<ul>
<li>NUMA(Non Uniform Memory Architecture)，在这个架构下，每个处理器都会访问“本地”的内存池，从而在CPU和存储之间有更小的延迟和更大的带宽。</li>
<li>在Kilo版本中针对此功能的实现包括：基于NUMA的调度的实现；可以将vCPU绑定在物理CPU上；超大页的支持。以上提到的三点都是通过Flavor的Extra Spces完成定义的。</li>
</ul>


<h3>EC2 API</h3>

<ul>
<li>EC2 API被从Nova中踢出去了</li>
<li>取而代之的是在stackforge的EC2 API转换服务</li>
</ul>


<h3>API Microversioning</h3>

<p>先来解释一下为什么需要API的微版本：主要原因在于现在这种API扩展方式，对于API实现的代码的增加或减少管理非常不方便，容易导致不一致性。引入微版本主要目的就是让开发人员在修改API代码时能够向前兼容，而不是加入一个新的API扩展；用户通过指定API的版本，在请求时也能决定是使用的具体的动作。</p>

<p>包含版本的返回:</p>

<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>GET /
</span><span class='line'>{
</span><span class='line'>     "versions": [
</span><span class='line'>        {
</span><span class='line'>            "id": "v2.1",
</span><span class='line'>            "links": [
</span><span class='line'>                  {
</span><span class='line'>                    "href": "http://localhost:8774/v2/",
</span><span class='line'>                    "rel": "self"
</span><span class='line'>                }
</span><span class='line'>            ],
</span><span class='line'>            "status": "CURRENT",
</span><span class='line'>            "version": "5.2"
</span><span class='line'>            "min_version": "2.1"
</span><span class='line'>        },
</span><span class='line'>   ]
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>客户端的Header信息：</p>

<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>X-OpenStack-Nova-API-Version: 2.114</span></code></pre></td></tr></table></div></figure>


<h3>一个已知的问题：Evacuate</h3>

<p>这个问题的产生主要是因为Evacuate的清理机制，主机名的变化会导致nova-compute重启过程中误删所有虚拟机，所以一个变通的方法是设置</p>

<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>destroy_after_evacuate=False</span></code></pre></td></tr></table></div></figure>


<p>这个问题会在Liberty中得到修复，相关的Spec：<a href="https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst">https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst</a></p>

<h2>Glance新功能</h2>

<ul>
<li>自动进行镜像格式转化，例如，Ceph是使用RAW格式的，假如我们上传的是QCOW2，创建虚拟机时，就会经历一番上传下载的过程，速度异常缓慢。而且RAW格式通常都是原始大小，上传时候非常慢，完全可以通过上传小镜像自动转换为指定格式。</li>
<li>Glance支持多字段排序</li>
</ul>


<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/images?sort_key=status&sort_dir=asc&sort_key=name&sort_dir=asc&sort_key=created_at&sort_dir=desc</span></code></pre></td></tr></table></div></figure>


<ul>
<li>临时将镜像设置为非活跃状态，假如一个镜像里有病毒，管理员就会将该镜像设置为非活跃状态，在清理后重新发布该镜像，在这个过程中，所有非管理员用户都无法使用或者下载这个镜像</li>
<li>免重启动态加载配置文件，配置文件改动后重启服务，现在可以给glance服务发送SIGHUP触发，这样升级就可以零当机时间。</li>
<li>使用多个Swift容器存储镜像，减少大规模部署时的请求瓶颈</li>
</ul>


<h2>Cinder新功能</h2>

<ul>
<li>实现服务逻辑代码与数据库结构之间的解耦，支持Rolling更新</li>
<li>一致性组是指具备公共操作的卷，逻辑上化为一组。在K版本中对增强一致性组的功能：可以添加、删除卷，从已经存在的快照创建新的组，关于一致性组的详细操作可以参考：<a href="http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html">http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html</a></li>
</ul>


<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cinder consisgroup-update
</span><span class='line'>[--name NAME]
</span><span class='line'>[--description DESCRIPTION]
</span><span class='line'>[--add-volumes UUID1,UUID2,......]
</span><span class='line'>[--remove-volumes UUID3,UUID4,......]
</span><span class='line'>CG</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cinder consisgroup-create-from-src
</span><span class='line'>[--cgsnapshot CGSNAPSHOT]
</span><span class='line'>[--name NAME]
</span><span class='line'>[--description DESCRIPTION]</span></code></pre></td></tr></table></div></figure>


<ul>
<li>卷类型的增强功能主要包含两个：为某一项目创建私有的卷类型和为卷类型增加描述信息</li>
</ul>


<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cinder type-create &lt;name&gt; --is-public
</span><span class='line'>cinder type-create &lt;name&gt; &lt;description&gt;</span></code></pre></td></tr></table></div></figure>


<h2>Neutron新功能</h2>

<ul>
<li>DVR支持OVS中的VLANs</li>
<li>新的V2版本的LBaas的API</li>
<li>新的插件的更新，详情请见更新日志中</li>
<li>一些高级服务的分离，例如：L3, ML2, VPNaaS, LBaaS</li>
</ul>


<p>网络方面我不是权威，希望有高人能出来讲讲Kilo中的Neutron新功能。</p>

<h2>Keystone新功能</h2>

<ul>
<li>项目嵌套，创建一个新的Project时候，可以指定parent的Project</li>
</ul>


<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>POST /projects
</span><span class='line'>
</span><span class='line'>{
</span><span class='line'>    "project": {
</span><span class='line'>        "description": "Project space for Test Group",
</span><span class='line'>        "domain_id": "1789d1",
</span><span class='line'>        "enabled": true,
</span><span class='line'>        "name": "Test Group",
</span><span class='line'>        "parent_id": "7fa612"
</span><span class='line'>    }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Keystone与Keystone的联盟，有了这个功能两个或者更多的云服务提供者就可以共享资源，这个功能在J版本引入，在K版本中主要针对该功能的进一步增强，具体的使用方法可参考这篇博文：<a href="http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/">http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/</a></li>
<li>针对新人授权的一些增强功能</li>
<li>keystone的配置中有部分配置发生了变化，例如：keystone.token.backends.memcache被keystone.token.persistence.backends.memcache取代，更多详细内容请参考更新日志</li>
</ul>


<h2>Swift新功能</h2>

<ul>
<li>纠删码的加入应该是这个版本最大的亮点，但是纠删码作为beta版本发布，并不推荐应用于生产环境，关于纠删码的详细介绍可以参考：<a href="http://docs.openstack.org/developer/swift/overview_erasure_code.html">http://docs.openstack.org/developer/swift/overview_erasure_code.html</a></li>
<li>复合型令牌，简而言之就是需要用户加上服务的Token才能对Swfit存放的内容进行操作，如下图所示：</li>
</ul>


<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>client
</span><span class='line'>   \
</span><span class='line'>    \   &lt;request&gt;: &lt;path-specific-to-the-service&gt;
</span><span class='line'>     \  x-auth-token: &lt;user-token&gt;
</span><span class='line'>      \
</span><span class='line'>    SERVICE
</span><span class='line'>       \
</span><span class='line'>        \    PUT: /v1/SERVICE_1234/&lt;container&gt;/&lt;object&gt;
</span><span class='line'>         \   x-auth-token: &lt;user-token&gt;
</span><span class='line'>          \  x-service-token: &lt;service-token&gt;
</span><span class='line'>           \
</span><span class='line'>          Swift</span></code></pre></td></tr></table></div></figure>


<p>具体的设计文档：<a href="http://docs.openstack.org/developer/swift/overview_backing_store.html">http://docs.openstack.org/developer/swift/overview_backing_store.html</a></p>

<ul>
<li>全局性的集群复制优化，大幅提高效率，避免经过广域网传播的数据</li>
</ul>


<h2>Ceilometer新功能</h2>

<ul>
<li>支持Ceph对象存储监控，当对象存储为Ceph而不是Swfit的时候，使用Polling机制，使用Ceph的Rados Gateway的API接口获取数据，具体的设计文档：<a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst</a></li>
<li>Ceilometer API RBAC - 更细粒度的权限控制: <a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst</a></li>
</ul>


<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>    "context_is_admin": [["role:admin"]]
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>更细粒度的控制</p>

<figure class='code'><figcaption><span>static/themes/blue/_style.scss</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>     "context_is_admin": [["role:admin"]],
</span><span class='line'>     "admin_or_cloud_admin": [["rule:context_is_admin"],
</span><span class='line'>              ["rule:admin_and_matching_project_domain_id"]],
</span><span class='line'>     "telemetry:alarm_delete": [["rule:admin_or_cloud_admin"]]
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>接口中的模糊查询，增加了一个新的查询符号=~</li>
<li>支持更多的测量，包括Hyper-V，IPMI相关的</li>
</ul>


<h2>Ironic新功能</h2>

<ul>
<li>iLO的优化</li>
<li>使用Config Drive替代Metadata服务</li>
<li>全盘镜像支持，可以跳过raddisk和kernel，这样就可以部署Windows的镜像了</li>
<li>使用本地盘启动，替代PXE方式，可以通过设置flavor的capabilities:boot_option实现</li>
</ul>


<h2>Oslo</h2>

<p>解决了很多之前遗留的技术债，还有一些命名规范的问题。olso.messaging实现了心跳，olso.log在所有项目中使用，优化了oslo.db的代码。</p>

<h2>OpenStack文档</h2>

<p>优化了docs.openstack.org页面，也可以从中选择相应的语言。有专门的团队负责安装、网络和高可靠的文档。</p>

<h2>其他模块</h2>

<p>对于Sahara, Heat, Trove等模块的更新没有在这里Highlight出来，大家可以参考更新日志里的内容，或者查看specs中的具体描述。</p>

<h2>总结</h2>

<p>通过Kilo的一些更新可以看到，Kilo版本在不断优化代码结构的基础上，增加了一些新功能，也偿还了一些技术债，总体来说是一种稳中有升的态势，但是总体感觉并没有太多的惊喜和出人意料。相信随着更多的孵化项目进入正式版本中，OpenStack一定会向更多元化的方向发展。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Havana)将OpenStack Havana源代码编译为DEB包]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/05/build-openstack-source-code-to-deb-package/"/>
    <updated>2015-03-05T21:57:16+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/05/build-openstack-source-code-to-deb-package</id>
    <content type="html"><![CDATA[<h2>Why</h2>

<p>我想有以下有几个原因促使我写这篇Blog：</p>

<ul>
<li>很多人开始OpenStack之旅是从Ubuntu开始，但是却没有一篇文章系统的介绍如何将修改后的代码重新编译回DEB包。</li>
<li>如果我们采用源代码直接安装的方式对OpenStack模块进行管理，一致性很难保证，难以维护。</li>
<li>Debian类系统的打包看起来比RPM包复杂很多。</li>
</ul>


<h2>Who</h2>

<p>谁需要看这篇文章呢？</p>

<ul>
<li>不了解如何编译DEB包</li>
<li>想把修改过的OpenStack源代码重新发布，供内部使用</li>
<li>希望改变直接维护源代码</li>
</ul>


<p>当然，如果您已经是这方面的高手，欢迎给我指正我Blog中的不足，十分感谢。</p>

<!-- more -->


<h2>Quick Start</h2>

<p>我已经将整个的编译过程集成在Vagrant脚本中，你可以直接安装Vagrant后，下载我的源代码，启动后就能看到整个的编译过程。</p>

<p>Vagrant 版本要求为1.3.5，Virtualbox版本要求为4.1或者4.2均可。</p>

<h2>Let&rsquo;s play some magic</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/xiaoquqi/vagrant-build-openstack-deb
</span><span class='line'>cd vagrant-build-openstack-deb
</span><span class='line'>vagrant up</span></code></pre></td></tr></table></div></figure>


<p>虚拟机启动后，将会自动从github(这里使用的是csdn code的镜像代码)同步最新代码，然后使用编译脚本，执行打包操作。如果不考虑下载的时间，整个过程大概持续5分钟左右的时间，编译好的Deb包将会存放在/root/build目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>vagrant ssh</span></code></pre></td></tr></table></div></figure>


<p>即可登陆到虚拟机，切换到root目录就可以查看到所有打包好的DEB的情况了，当然你也可以直接使用dpkg -i命令进行安装。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo -s
</span><span class='line'>cd /root/build
</span><span class='line'>ls -lrt *.deb
</span><span class='line'>dpkg -i python-glance_2013.2.2.dev1.g5cd7a22~precise-0ubuntu1_all.deb</span></code></pre></td></tr></table></div></figure>


<h2>Step by Step</h2>

<p>看过了整个的编译过程，下面来介绍一点点细节。</p>

<p>全部的编译部分代码都在这个文件中：<a href="https://github.com/xiaoquqi/vagrant-build-openstack-deb/blob/master/scripts/build.sh">https://github.com/xiaoquqi/vagrant-build-openstack-deb/blob/master/scripts/build.sh</a></p>

<p>下面让我们来仔细分析一下整个编译过程。</p>

<ul>
<li>添加必要的源</li>
</ul>


<p>这里面我们用的源包含sohu的Ubuntu 12.04源以及Ubuntu的Havana源</p>

<pre><code>deb http://mirrors.sohu.com/ubuntu/ precise main restricted
deb http://mirrors.sohu.com/ubuntu/ precise-updates main restricted
deb http://mirrors.sohu.com/ubuntu/ precise universe
deb http://mirrors.sohu.com/ubuntu/ precise-updates universe
deb http://mirrors.sohu.com/ubuntu/ precise multiverse
deb http://mirrors.sohu.com/ubuntu/ precise-updates multiverse
deb http://mirrors.sohu.com/ubuntu/ precise-backports main restricted universe multiverse
deb http://mirrors.sohu.com/ubuntu/ precise-security main restricted
deb http://mirrors.sohu.com/ubuntu/ precise-security universe
deb http://mirrors.sohu.com/ubuntu/ precise-security multiverse
deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/havana main
</code></pre>

<ul>
<li>安装必要的编译软件</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apt-get install -y debootstrap equivs schroot
</span><span class='line'>apt-get install -y devscripts
</span><span class='line'>apt-get install -y build-essential checkinstall sbuild
</span><span class='line'>apt-get install -y dh-make
</span><span class='line'>apt-get install -y bzr bzr-builddeb
</span><span class='line'>apt-get install -y git
</span><span class='line'>apt-get install -y python-setuptools</span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译脚本的源代码仓库</li>
</ul>


<p>Ubuntu维护源代码编译脚本是使用叫做bzr的工具，常使用Launchpad的朋友应该比较熟悉，这是一套类似于git的分布式管理工具，不同的是这是一套完全用python语言实现的管理工具，不仅具有代码版本控制功能并且与Launchpad高度整合，作为Ubuntu维护不可缺少的重要工具之一。例如，这里面用到的glance编译脚本就可以在这里找到：</p>

<p><a href="https://code.launchpad.net/~ubuntu-server-dev/glance/havana">https://code.launchpad.net/~ubuntu-server-dev/glance/havana</a></p>

<p>页面上方有下载代码的方式：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bzr branch lp:~ubuntu-server-dev/glance/havana
</span><span class='line'>git clone https://code.csdn.net/openstack/glance.git --branch "stable/havana" glance_source</span></code></pre></td></tr></table></div></figure>


<ul>
<li>准备环境</li>
</ul>


<p>在Vagrant启动一台新虚拟机之后，并没有pip，如果不安装pip，则会在python setup.py sdist过程中，把pip安装到源代码目录中，引起Build失败。将//vagrant/pip/pip-1.4.1.tar.gz解压缩并安装，之后安装pbr：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf pip-1.4.1.tar.gz
</span><span class='line'>cd pip-1.4.1
</span><span class='line'>sudo python setup.py install
</span><span class='line'>sudo pip install pbr</span></code></pre></td></tr></table></div></figure>


<ul>
<li>生成source文件</li>
</ul>


<p>进入glance_source目录，执行</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>python setup.py sdist</span></code></pre></td></tr></table></div></figure>


<p>生成的tar.gz文件会在glance_source/dist下，注意此时该文件的名称为：</p>

<pre><code>glance-2013.2.2.dev1.g5cd7a22.tar.gz
</code></pre>

<p>接下来我们需要将该文件重命名为：</p>

<pre><code>glance_2013.2.2.dev1.g5cd7a22~precise.orig.tar.gz
</code></pre>

<p>特别注意：glance后面已经变为下划线！！！</p>

<p>把文件移动到与glance和glance_source同一级别的目录，这样在编译的时候，才能找到source文件。此时的目录结构为：</p>

<pre><code>├── glance
│   ├── debian
├── glance_source
├── glance_2013.2.2.dev1.g5cd7a22~precise.orig.tar.gz
</code></pre>

<ul>
<li>安装依赖包</li>
</ul>


<p>为了保证顺利的完成编译，我们需要安装要编译包的所有依赖包，简单来说就是glance/debian/control文件中定义的Depends部分的内容。当然在编译的时候我们也可以完全忽略依赖，但是并不推荐。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mk-build-deps -i -t 'apt-get -y' debian/control</span></code></pre></td></tr></table></div></figure>


<p>这样系统就会自动安装所有依赖包，并且生成一个glance-build-deps_1.0_all.deb文件。</p>

<ul>
<li>生成日志信息</li>
</ul>


<p>开始编译前，我们还需要告诉编译器我们要编译的版本，还记得刚才生成的dist包吗，把那个版本拿出来作为我们commit的版本。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd glance
</span><span class='line'>dch -b -D precise --newversion "1:2013.2.2.dev1.g5cd7a22~precise-0ubuntu1" 'This is a build test.'
</span><span class='line'>debcommit</span></code></pre></td></tr></table></div></figure>


<p>这样在glance/debian/changelog中就会增加一条新的日志。</p>

<ul>
<li>开始编译</li>
</ul>


<p>万事俱备，只欠东风。我们利用bzr提供的builddeb开始编译，这里我们忽略签名问题。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd glance
</span><span class='line'>bzr builddeb -- -sa -us -uc</span></code></pre></td></tr></table></div></figure>


<p>大功告成啦。快去/root/build/glance下看看你的deb包吧。</p>

<h2>总结</h2>

<p>Debian包的编译的确涉及很多知识点，而且可使用的编译工具很多，关系很复杂。这篇博文，只为了帮助大家对DEB包的编译有一个快速的认识，如果想了解更多关于编译的知识，请关注后续的博文。</p>

<p>最后，我们仍然希望有更多的热爱OpenStack的朋友们加入我们公司，如果有意向的请与我联系</p>

<ul>
<li>邮箱：<a href="&#x6d;&#x61;&#105;&#x6c;&#116;&#111;&#x3a;&#120;&#105;&#97;&#111;&#x71;&#x75;&#x71;&#105;&#64;&#x67;&#x6d;&#x61;&#x69;&#108;&#46;&#x63;&#x6f;&#x6d;">&#120;&#105;&#97;&#x6f;&#113;&#117;&#113;&#105;&#64;&#x67;&#109;&#97;&#105;&#108;&#x2e;&#99;&#x6f;&#109;</a></li>
<li>新浪微博：@RaySun(<a href="http://weibo.com/xiaoquqi">http://weibo.com/xiaoquqi</a>)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Juno)OpenStack Neutron L3高可靠]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/05/openstack-neutron-l3-high-availability/"/>
    <updated>2015-03-05T20:53:57+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/05/openstack-neutron-l3-high-availability</id>
    <content type="html"><![CDATA[<h2>L3层Agent的低可靠解决方案</h2>

<p>当前，你可以通过多网络节点的方式解决负载分担，但是这并非高可靠和冗余的解决方案。假设你有三个网络节点，当你创建新的路由时，会自动的将新路由规划和分布在这三个网络节点中的一个上。但是，如果一个节点坏掉，上面运行的所有路由将无法提供服务，路由转发也无法正常进行。在Neutron的IceHouse版本中，没有提供任何内置的解决方案。</p>

<!-- more -->


<h2>DHCP Agent的高可靠的变通之道</h2>

<p>DHCP的Agent是一个另类——DHCP协议本身就支持在同一个资源池内同时使用多个DHCP提供服务。</p>

<p>在neutron.conf中仅仅需要改变：</p>

<pre><code>dhcp_agents_per_network = X
</code></pre>

<p>这样DHCP的调度程序会为同一网络启动X个DHCP Agents。所以，对于三个网络节点，并设置dhcp_agents_per_network = 2，每个Neutron网络会在三个节点中启动两个DHCP Agents。这个是如何工作的呢？</p>

<p><img class="right" src="http://xiaoquqi.github.io/images/blogs/neutron-l3-ha-dhcp-ha.png" width="200" height="300"></p>

<p>首先，让我们来看一下物理层面的实现。当一台主机连接到子网10.0.0.0/24，会发出DHCP Discover广播包。两个DHCP服务进程dnsmasq1和dnsmasq2(或者其他的DHCP服务)收到广播包，并回复10.0.0.2。假设第一个DHCP服务响应了服务器请求，并将10.0.0.2的请求广播出去，并且指明提供IP的是dnsmasq1-10.0.0.253。所有服务都会接收到广播，但是只有dnsmasq1会回复ACK。由于所有DHCP通讯都是基于广播，第二个DHCP服务也会收到ACK，于是将10.0.0.2标记已经被AA:BB:CC:11:22:33获取，而不会提供给其他的主机。总结一下，所有客户端与服务端的通讯都是基于广播，因此状态(IP地址什么时候被分配，被分配给谁)可以被所有分布的节点正确获知。</p>

<p>在Neutron中，分配MAC地址与IP地址的关系，是在每个dnsmasq服务之前完成的，也就是当Neutron创建Port时。因此，在DHCP请求广播之前，所有两个dnsmasq服务已经在leases文件中获知了，AA:BB:CC:11:22:33应该分配10.0.0.2的映射关系。</p>

<h2>回到L3 Agent的低可用</h2>

<p>L3 Agent，没有(至少现在没有)提供任何DHCP所能提供的高可靠解决方案，但是用户的确很需要高可靠。怎么办呢？</p>

<p>Pacemaker/Corosync - 使用外部的集群管理技术，为Active节点指定一个Standby的网络节点。Standby节点在正常情况下等呀、等呀等，一旦Active节点发生故障，L3 Agent立即在Standby节点启动。这两个节点配置相同的主机名，当Standby的Agent出台并和服务之前开始同步后，它自己的ID不会改变，因此就像管理同一个router一样。</p>

<p>另外一个方案是采用定时同步的方式(cron job)。用Python SDK开发一段脚本，使用API获取所有已经故去的Agent们，之后获取所有上面承载的路由，并且把他们重新分配给其他的Agent。</p>

<p>在Juno开发过程中，查看Kevin Benton的这个Patch，让Neutron自己具备重新分配路由的功能: <a href="https://review.openstack.org/#/c/110893/">https://review.openstack.org/#/c/110893/</a></p>

<h2>重新分配路由——路漫漫兮</h2>

<p>以上列出的解决方案，实质上都有从失败到恢复的时间，如果在简单的应用场景下，恢复一定数量的路由到新节点并不算慢。但是想象一下，如果有上千个路由就需要话费数个小时完成，重新分配和配置的过程。人们非常需要快速的故障恢复！</p>

<h2>分布式虚拟路由(Distributed Virtual Router)</h2>

<p>这里有一些文档描述DVR是如何工作的：</p>

<ul>
<li><a href="http://specs.openstack.org/openstack/neutron-specs/specs/juno/neutron-ovs-dvr.html">http://specs.openstack.org/openstack/neutron-specs/specs/juno/neutron-ovs-dvr.html</a></li>
<li><a href="https://docs.google.com/document/d/1jCmraZGirmXq5V1MtRqhjdZCbUfiwBhRkUjDXGt5QUQ/">https://docs.google.com/document/d/1jCmraZGirmXq5V1MtRqhjdZCbUfiwBhRkUjDXGt5QUQ/</a></li>
<li><a href="https://docs.google.com/document/d/1depasJSnGZPOnRLxEC_PYsVLcGVFXZLqP52RFTe21BE/">https://docs.google.com/document/d/1depasJSnGZPOnRLxEC_PYsVLcGVFXZLqP52RFTe21BE/</a></li>
</ul>


<p>这里的要点是将路由放到计算节点(compute nodes)，这让网络节点的L3 Agent变得没用了。是不是这样呢？</p>

<ul>
<li>DVR主要处理Floating IPs，把SNAT留给网络节点的L3 Agent</li>
<li>不和VLANs一起工作，仅仅支持tunnes和L2pop开启</li>
<li>每个计算节点需要连接外网</li>
<li>L3 HA是对部署的一种简化，这个基于Havana和Icehouse版本部署的云平台所不具备的</li>
</ul>


<p>理想情况下，你想把DVR和L3 HA一起使用。Floating IP的流量会从你的计算节点直接路由出去，而SNAT的流量还是会从你的计算节点HA集群的L3 Agent进行转发。</p>

<h2>3层高可靠</h2>

<p>Juno版本的L3的HA解决方案应用了Linux上流行的keepalived工具，在内部使用了VRRP。首先，我们先来讨论一下VRRP。</p>

<h2>什么是VRRP，如何在真实世界里工作</h2>

<p>虚拟路由冗余协议(Virtual Router Redundancy Protocol)是一个第一条冗余协议——目的是为了提供一个网络默认网关的高可靠，或者是路由的下一跳的高可靠。那它解决了什么问题呢？在一个网络拓扑中，有两个路由提供网络连接，你可以将网络的默认路由配置为第一个路由地址，另外一个配置成第二个。</p>

<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/neutron-l3-ha-router_ha-topology-before-vrrp.png" width="200" height="300"></p>

<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/neutron-l3-ha-switch-moves-mac.png" width="200" height="300"></p>

<p>这样将提供负载分担，但是如果其中一个路由失去了连接，会发生什么呢？这里一个想法就是虚拟IP地址，或者一个浮动的地址，配置为网络默认的网管地址。当发生错误时，Standby的路由并不会收到从Master节点发出的VRRP Hello信息，并且这将触发选举程序，获胜的成为Active网管，另外一个仍然作为Standby。Active路由配置虚拟IP地址(简写VIP)，内部的局域网接口，回复ARP请求时会附加虚拟的MAC地址。由于在该网络内的计算机已经拥有了ARP缓存(VIP+虚拟机MAC地址)，也就没有必要重新发送ARP请求了。依据选举机制，有效的Standby路由变为Active，并且发送一个非必要的ARP请求——来向网络中声明当前的VIP+MAC对已经属于它了。这个切换，包含了网络中将虚拟机MAC地址从旧的迁移到新的。</p>

<p>为了实现这一点，指向默认网关的流量会从当前的(新的)Active路由经过。注意这个解决方案中并没有实现负载分担，这种情况下，所有的流量都是从Active路由转发的。(注意：在Neutron的用户使用场景中，负载分担并没有在单独的路由级别完成，而是在节点(Node)级别，也就是要有一定数量的路由)。那么如何在路由层面实现负载分担呢？VRRP组：VRRP的投中包含虚拟机路由识别码(Virtual Router Identifier)，也就是VRID。网络中一半的主机配置为第一个VIP，另外一个使用第二个。在失败的场景下，VIP会从失败的路由转移到另外一个。</p>

<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/neutron-l3-ha-router-ha-two-vrrp-groups.png" width="200" height="300"></p>

<p><img class="left" src="http://xiaoquqi.github.io/images/blogs/neutron-l3-ha-router-ha-external-trap.png" width="200" height="300"></p>

<p>善于观察的读者已经发现了一个明显的问题——如果一个Active路由失去了与Internet的连接怎么办？那它还会作为Active路由，但是不能转发？VRRP增加了监控外部连接的能力，并且当发生失败后交出Active路由的地位。</p>

<p>注意：一旦地址发生改变，可能会出发两种模式：</p>

<ul>
<li>每一个路由得到一个新的IP地址，不管VRRP的状态。Master路由将VIP配置为附加的或第二地址。</li>
<li>仅仅VIP配置了。例如：Master路由配置了VIP，同时Slave上没有IP配置。</li>
</ul>


<h2>VRRP —— 一些事实</h2>

<ul>
<li>直接在IP协议中封装</li>
<li>Active实例使用多播地址224.0.0.18, MAC 01-00-5E-00-00-12来给Standby路由发送hello消息</li>
<li>虚拟MAC地址格式：00-00-5E-00-01-{VRID}，因此只有256个不同的VRIDs(0到255)在一个广播域中</li>
<li>选举机制使用用户配置优先级，从1到255，越高优先级越高</li>
<li>优先选举策略(Preemptive elections)，和其他网络协议一样，意味着一个Standby被配置为较高优先级，或者从连接中断回复后(之前是Active实例)，它始终会恢复为Active路由</li>
<li>非优先选举策略(Non-preemptive elections)意外着当Active路由回复后，仍然作为Standby角色</li>
<li>发送Hello间隔是可以设置的(假定为每T秒)，如果Standby路由在3T秒后仍然没有收到master的hello消息，就会触发选举机制</li>
</ul>


<h2>回到Neutron的领地</h2>

<p>L3 HA在每一个路由空间上启动一个keepalived实例。不同路由实例间的通讯，通过制定的HA网络，每一个tenant一个。这个网络是由一个空白的(blank) tenant下创建的，并且无法通过CLI或者GUI操作。这个HA网络也是一个Neutron tenant网络，和所有其他的一样，也是用了默认的分区技术。keepalived流量被转发到HA设备(在keepalived.conf中指定，在路由的命名空间中keepalived实例会使用)。这是路由中命名空间&#8217;ip address&#8217;的输出：</p>

<pre><code>[stack@vpn-6-88 ~]$ sudo ip netns exec qrouter-b30064f9-414e-4c98-ab42-646197c74020 ip address
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    ...
2794: &lt;/span&gt;&lt;span style="color:#ff0000;"&gt;&lt;strong&gt;ha-45249562-ec&lt;/strong&gt;&lt;/span&gt;&lt;span style="color:#333333;"&gt;: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default
    link/ether 12:34:56:78:2b:5d brd ff:ff:ff:ff:ff:ff
    inet 169.254.0.2/24 brd 169.254.0.255 scope global ha-54b92d86-4f
      valid_lft forever preferred_lft forever
    inet6 fe80::1034:56ff:fe78:2b5d/64 scope link
      valid_lft forever preferred_lft forever
2795: qr-dc9d93c6-e2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default
    link/ether ca:fe:de:ad:be:ef brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.1/24 scope global qr-0d51eced-0f
      valid_lft forever preferred_lft forever
    inet6 fe80::c8fe:deff:fead:beef/64 scope link
      valid_lft forever preferred_lft forever
2796: qg-843de7e6-8f: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default
    link/ether ca:fe:de:ad:be:ef brd ff:ff:ff:ff:ff:ff
    inet 19.4.4.4/24 scope global qg-75688938-8d
      valid_lft forever preferred_lft forever
    inet6 fe80::c8fe:deff:fead:beef/64 scope link
      valid_lft forever preferred_lft forever&lt;/span&gt;
</code></pre>

<p>这是在master实例中的输出。在另外一个节点的同一个路由上，在ha, hr或者qg设备上没有IP地址。也没有Floating Ip或者是路由记录。这些是被配置在keepalived.conf中的，当keepalived检测到Master实例的失败后，这些地址(或者:VIPs)会被keepalived在适当的设备中重新配置。这是对于同一个路由的keepalived.conf的实例：</p>

<pre><code>vrrp_sync_group VG_1 {
    group {
        VR_1
    }
    notify_backup "/path/to/notify_backup.sh"
    notify_master "/path/to/notify_master.sh"
    notify_fault "/path/to/notify_fault.sh"
}
vrrp_instance VR_1 {
    state BACKUP
    interface ha-45249562-ec
    virtual_router_id 1
    priority 50
    nopreempt
    advert_int 2
    track_interface {
        ha-45249562-ec
    }
    virtual_ipaddress {
        19.4.4.4/24 dev qg-843de7e6-8f
    }
    virtual_ipaddress_excluded {
        10.0.0.1/24 dev qr-dc9d93c6-e2
    }
    virtual_routes {
        0.0.0.0/0 via 19.4.4.1 dev qg-843de7e6-8f
    }
}
</code></pre>

<p>这些notify脚本是干虾米用的呢？这些脚本是被keepalived执行，转换为Master，备份或者失败。这些Master脚本内容：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/usr/bin/env bash</span>
</span><span class='line'>neutron-ns-metadata-proxy --pid_file<span class="o">=</span>/tmp/tmpp_6Lcx/tmpllLzNs/external/pids/b30064f9-414e-4c98-ab42-646197c74020/pid --metadata_proxy_socket<span class="o">=</span>/tmp/tmpp_6Lcx/tmpllLzNs/metadata_proxy --router_id<span class="o">=</span>b30064f9-414e-4c98-ab42-646197c74020 --state_path<span class="o">=</span>/opt/openstack/neutron --metadata_port<span class="o">=</span><span class="m">9697</span> --debug --verbose
</span><span class='line'><span class="nb">echo</span> -n master &gt; /tmp/tmpp_6Lcx/tmpllLzNs/ha_confs/b30064f9-414e-4c98-ab42-646197c74020/state
</span></code></pre></td></tr></table></div></figure>


<p>这个Master脚本简单的打开了metadata代理，将状态写入文件，状态文件之后会被L3 Agent读取。备份和出错脚本杀死代理服务并且将状态写入刚才的提到的状态文件。这就意味着metadata服务职能在master路由节点存在。</p>

<h2>我们是不是忘了Metadata Agent呢？</h2>

<p>在每个网络节点启动metadata agent就好啦。</p>

<h2>未来的工作和局限性</h2>

<ul>
<li>TCP连接跟踪——在当前的实现中，TCP连接的session在失败恢复后中断。一种解决方案是使用conntrackd复制HA路由间的session状态，这样当故障恢复后，TCP的session会恢复到失败前的状态。</li>
<li>Master节点在哪？当前，还没有办法让管理员知道哪个网络节点是HA路由的Master实例。计划由Agent提供这些信息，并可以通过API进行查询。</li>
<li>Agent大逃亡——理想状态下，将一个节点变为维护模式后，应该触发所有HA路由所在节点回收他们的Master状态，加速恢复速度。</li>
<li>通知L2pop VIP的改变——考虑在一个tenant网络中路由IP/MAC，只有Master配置真正的有IP地址，但是同一个Neutron Port和同样的MAC会在相关的网络出现。这可能对配置了L2pop驱动产生不利，因为它只希望在一个网络中MAC地址只存在一处。解决的计划是一旦检测到VRRP状态改变，从Agent发送一个RPC消息，这样当路由变为Master，控制节点被通知到了，这样就能改变L2pop的状态了。</li>
<li>内置的防火墙、VPN和负载均衡即服务。在DVR和L3 HA与这些服务整合时还有问题，可能会在kilo中解决。</li>
<li>每一个Tenant一个HA网络。这就意味着每个Tenant只能有255个HA路由，因为每个路由需要一个VRID，根据VRRP协议每个广播域只允许255个不同的VRID值。</li>
</ul>


<h2>使用和配置</h2>

<p>neutron.conf</p>

<pre><code>l3_ha = True
max_l3_agents_per_router = 2
min_l3_agents_per_router = 2
</code></pre>

<ul>
<li>l3_ha 表示所有的路由默认使用HA模式(与之前不同)。默认是关闭的。</li>
<li>你可以根据网络节点的数量设置最大最小值。如果你部署了4个网络节点，但是设置最大值为2，只有两个L3 Agent会被用于HA路由(一个是Master，一个是Slave)。</li>
<li>min被用来稳健性(sanity)的检查：如果你有两个网络节点，其中一个坏掉了，任何新建的路由在这段时间都会失败，因为你至少需要min个L3 Agent启动来建立HA路由。</li>
</ul>


<p>l3_ha控制默认的开关，当管理员(仅管理员)通过CLI方式可以覆盖这个选项，为每个路由创建单独的配置：</p>

<pre><code>neutron router-create --ha=&lt;True | False&gt; router1
</code></pre>

<h2>参考文档</h2>

<ul>
<li><a href="https://blueprints.launchpad.net/neutron/+spec/l3-high-availability">Blueprint</a></li>
<li><a href="http://specs.openstack.org/openstack/neutron-specs/specs/juno/l3-high-availability.html">Spec</a></li>
<li><a href="https://docs.google.com/document/d/1P2OnlKAGMeSZTbGENNAKOse6B2TRXJ8keUMVvtUCUSM/">How to test</a></li>
<li><a href="https://review.openstack.org/#/q/topic:bp/l3-high-availability,n,z">Code</a></li>
<li><a href="https://wiki.openstack.org/wiki/Neutron/L3_High_Availability_VRRP">Dedicated wiki page</a></li>
<li><a href="https://wiki.openstack.org/wiki/Meetings/Neutron-L3-Subteam#Blueprint:_l3-high-availability_.28safchain.2C_amuller.29">Section in Neutron L3 sub team wiki (Including overview of patch dependencies and future work)</a></li>
<li><a href="http://assafmuller.com/2014/08/16/layer-3-high-availability/">http://assafmuller.com/2014/08/16/layer-3-high-availability/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph可靠性的量化分析]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability/"/>
    <updated>2015-03-04T08:36:06+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/04/ceph-reliability</id>
    <content type="html"><![CDATA[<p>在开始正文之前，首先要感谢UnitedStack工程师朱荣泽对这篇博文的大力帮助和悉心指教。本文主要针对UnitedStack公司在巴黎峰会上对Ceph可靠性的计算方法(<a href="https://www.ustack.com/blog/build-block-storage-service/">https://www.ustack.com/blog/build-block-storage-service/</a>)做了一个更明确的分析和阐述，供对此话题感兴趣的朋友一起来讨论、研究，文章中如果有不当之处，还请各位高人指教。</p>

<!-- more -->


<h2>什么情况下数据会丢失？</h2>

<p>这个话题的另外一种提法就是存储的可靠性，所谓存储的可靠性最基本的一点就是数据不要丢失，也就是我们俗称的“找不回来了”。所以，要分析Ceph的可靠性我们只需要搞清楚，到底在什么情况下我们的数据会丢失，并且再也无法恢复，基于此我们便可以创建我们的计算模型。</p>

<p>我们先来假定一套简单的Ceph环境，3个OSD节点，每个OSD节点对应一块物理硬盘，副本数为3。那么我们排除MON的因素影响Ceph集群的运行的问题，显而易见，当三个OSD对应的物理硬盘全部损坏时，数据必然无法恢复。所以此时集群的可靠性是与硬盘本身的可靠性直接相关。</p>

<p>我们再来假定一套更大的Ceph环境，30个OSD节点，分3个机架摆放，每一个机架有10个OSD节点，每个OSD节点仍然对应一块物理硬盘，副本数为3，并且通过CRUSH MAP，将每一份副本均匀分布在三个机架上，不会出现两份副本同时出现在一个机架的情况。此时，什么时候会出现数据丢失的情况呢？当三个机架上都有一块硬盘损坏，而恰恰这三块硬盘又保存了同一个Object的全部副本，此时数据就会出现丢失的情况。</p>

<p>所以根据以上的分析，我们认为，Ceph的可靠性的计算是与OSD的数量(N)、副本数(R)、每一个服务节点的OSD数量(S)、硬盘的年失败概率(AFR)。这里我们使用UnitedStack相关参数进行计算。</p>

<p>具体取值如下图所示：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-formula.jpg" width="640" height="480"></p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-constant.jpg" width="640" height="480"></p>

<h2>硬盘年失败概率</h2>

<p>根据维基百科的计算方法(<a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">http://en.wikipedia.org/wiki/Annualized_failure_rate</a>)，AFR的计算方法如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-afr.jpg" width="640" height="480"></p>

<p>例如，计算Seagate某企业级硬盘的AFR，根据文档查到MTBF为1,200,000小时，则AFR为0.73%，计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-afr-example.jpg" width="640" height="480"></p>

<p>但是，根据Google的相关计算，在一个大规模集群环境下，往往AFR的值并没有硬盘厂商那样乐观，下面的统计告诉了我们在真实环境下AFR变化的情况：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-google-afr.jpg" width="640" height="480"></p>

<p>所以我们可以看到实际的AFR的变化范围随着年份而变化，取值范围在1.7%-8%左右，所以本文中AFR为1.7%。</p>

<h2>硬盘在一年之内损坏的概率</h2>

<p>有了AFR，我们就可以尝试计算硬盘在一年中出现故障的概率，根据相关研究，硬盘在一定时间内的失败概率符合Possion分布(已经把知识还给老师的同学请移步：<a href="http://en.wikipedia.org/wiki/Poisson_distribution">http://en.wikipedia.org/wiki/Poisson_distribution</a>)。具体的计算公式为：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-Pn.jpg" width="640" height="480"></p>

<p>当我最初拿到这个计算公式时，一下子懵了，到底该如何确定数学期望值lamda呢？</p>

<h2>lamda的计算过程</h2>

<p>根据相关的研究资料，单块的硬盘损坏的期望值(Failures in Time)是指每10亿小时硬盘的失败率(Failure Rate λ)，计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-fit.jpg" width="640" height="480"></p>

<p>这里的Af(Acceleration Factor)是由测试时间乘以阿伦尼乌斯方程的值得出来的结果，好吧，我承认，我也是现学现卖，这个方程式是化学反应的速率常数与温度之间的关系式，适用于基元反应和非基元反应，甚至某些非均相反应。不过可以看出Failure Rate的计算过程实质主要是计算环境因素引起的物理变化，最终导致失败的数学期望值。所以根据相关研究，最终FIT的计算方法为：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-fit-afr.jpg" width="640" height="480"></p>

<p>有了这些参数后，我们就可以开始正式计算Ceph集群中，不同机架上有三块硬盘同时出现损坏的概率啦。</p>

<h2>任意一个OSD出现损坏的概率P1(any)</h2>

<p>我们不太容易直接去计算任意一个OSD出现损坏的概率，但是我们很容易计算没有OSD出现问题的概率，方法如下，用一减去无OSD节点出现问题的概率，得到P1(any)。</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-osd1-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第二个节点出现故障的概率P2(any)</h2>

<p>我们知道当Ceph发现一个有问题的OSD节点时，会自动的将节点OUT出去，这个时间大约为10min，同时Ceph的自我修复机制会自动平衡数据，将故障节点的数据重新分配在其他的OSD节点上。</p>

<p>我们假设我们单盘的容量为1000 GB，使用率为75%，也就是此时将有750 GB的数据需要同步。我们的数据只在本机架平衡，节点写入速度为50 MB/s，计算方法如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-recovery-time.jpg" width="640" height="480"></p>

<p>注意：由于每个节点有三个OSD，所以要求每台物理机所承受的节点带宽至少要大于150 MB/s。并且在这个计算模型下，并没有计算元数据、请求数据、IP包头等额外的信息的大小。</p>

<p>有了Recovery Time，我们就可以计算我们第二个节点在Recovery Time内失败的概率，具体的计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-osd2-failure.jpg" width="640" height="480"></p>

<h2>在恢复时间内第三个节点出现故障的概率P3(any)</h2>

<p>计算方法同上，计算过程如下：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-osd3-failure.jpg" width="640" height="480"></p>

<h2>一年内任意副本数&reg;个OSD出现故障的概率</h2>

<p>所以将上述概率相乘即可得到一年内任意副本数&reg;个OSD出现故障的概率。</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-arbitrary-osd-failure.jpg" width="640" height="480"></p>

<h2>Copy Sets(M)</h2>

<p>在这个计算模型中，因为任意R个OSD节点的损坏并不意外着副本的完全丢失，因为损坏的R个OSD未必保存着一个Object的全部副本信息，所以未必造成数据不可恢复，所以这里引入了Copy Sets的概念。简单来说，Copyset就是存放所有拷贝的一个集合，具体的定义和计算方法可以查看参考链接。那么这里的场景下，Copy Sets为三个机架OSD数量相乘，即M=24<em>24</em>24。当然如果是两个副本的情况下，M应该为24<em>24+24</em>24+24*24。</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-copysets.jpg" width="640" height="480"></p>

<h2>CEPH的可靠性</h2>

<p>所以最终归纳出CEPH可靠性的算法为：</p>

<p><img src="http://xiaoquqi.github.io/images/blogs/ceph-reliability-copysets-failure.jpg" width="640" height="480"></p>

<p>可以看出Ceph三副本的可靠性大约为9个9，由于Recovery Time和AFR取值的问题，所以计算结果和UnitedStack上略有出入。</p>

<h2>参考链接</h2>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Annualized_failure_rate">Annualized Failure Rate</a></li>
<li><a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a></li>
<li><a href="http://www.microsemi.com/document-portal/doc_view/124041-calculating-reliability-using-fit-mttf-arrhenius-htol-model">Calculating Reliability using FIT &amp; MTTF: Arrhenius HTOL Model</a></li>
<li><a href="http://storagemojo.com/2007/02/19/googles-disk-failure-experience/">Google’s Disk Failure Experience</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/disk_failures.pdf">Failure Trends in a Large Disk Drive Population</a></li>
<li><a href="http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/11727-atc13-cidon.pdf">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack虚拟机的高可靠]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/03/03/openstack-instance-ha-high-availability/"/>
    <updated>2015-03-03T20:58:19+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/03/03/openstack-instance-ha-high-availability</id>
    <content type="html"><![CDATA[<p>译者注：OpenStack虚拟机级别的HA是在企业私有云中必须提及的话题，换句话说如果没有此机制，很多企业级用户根本不会考虑使用OpenStack+开源云平台的解决方案。这一点也得益于VMware等大公司孜孜不倦、持之以恒、长年累月的对用户的洗脑，这样的洗脑让用户觉得一些VMware的功能成为了“云”的标准(HA/FT/DRS/DPM等)。个人认为，OpenStack这部分功能的缺失也直接的阻碍了OpenStack进入中国行业用户的步伐，记得在夏天的时候，华为曾经为社区提供了一个HA的解决方案(用C语言实现)，打算贡献给社区，但是经过社区激烈的讨论，终于不了了之。</p>

<p>在我所经历的私有云项目中，也尝试了一些虚拟机级别的HA解决方案，我的个人观点是，如果将OpenStack作为一个项目，可以不提供HA，但是作为产品，必须要提供HA的解决方案(私有云)。这篇文章中提到HA实现的过程，也恰恰是我们在实际中遇到的状况，下面让我们来看一看OpenStack社区是如何设想实现该问题的。感谢 @陈沙克 微博提供的资料。</p>

<!-- more -->


<p>原文地址：<a href="http://blog.russellbryant.net/2014/10/15/openstack-instance-ha-proposal/">http://blog.russellbryant.net/2014/10/15/openstack-instance-ha-proposal/</a></p>

<p>在一个完美世界中(不是那个游戏公司)，OpenStack承载的云主机上运行的应用都应当具备天然的横向扩展能力和容灾能力。但是现实世界却并不是这样。我们一直看到在OpenStack运行传统负载的需求积极伴随而来的HA的需求。</p>

<p>传统应用运行在OpenStack在大多数情况下是没有问题的。一些需要可靠性要求的应用部署在OpenStack上是没有自动提供这种能力的。如果虚拟化软件挂掉了，没人会去拯救运行在上面的虚拟机。OpenStack中有手动拯救的能力，但是这需要云平台的运维人员或者外部的工具完成。</p>

<p>这篇文章主要讨论如何在虚拟化程序失败后自动侦测并拯救运行在上面的虚拟机于水火之中。对于不同的虚拟机化软件有不同的解决方案。我这里主要关注libivrt/kvm，下面的部分也主要围绕着它进行。除非特别提及libvirt，我想所有提及的一切也同样适用于xenserver驱动。</p>

<p>这是OpenStack社区的定期讨论的话题。已经有反对将这个功能放到OpenStack。无论哪个功能组件被使用，我想我们应该提供一个针对问题的解决方案。我想使用现有的软件来解决这个问题。</p>

<h3>范围</h3>

<p>我们主要针对基础架构层的故障。也有其他的一些因素会影响应用的可靠性。客户机的操作系统或者软件自身出现故障。对于这种故障的恢复，我们主要留给应用的开发者或部署者。</p>

<p>值得注意的是，OpenStack中的libvirt/kvm驱动并不包含于客户机操作系统故障相关的功能。在Icehouse版本中的Nova实现了一个libvirt-watchdog的blueprint。这个功能允许你设置hw_watchdog_action属性在image或是flavor中。合法的值包含：poweroff，rest，pause和none。当选项被打开时，libvirt会为客户机开启i7300esb watchdog驱动，并且当watchdog被出发时发送动作。这可能是对你客户机故障恢复策略很有帮助的部件。</p>

<h3>架构</h3>

<p>解决这个问题需要一些关键的部件：</p>

<ul>
<li>监控(Monitoring) - 系统检测到虚拟化层的故障</li>
<li>隔离(或是围栏，Fencing) - 系统隔离故障计算节点</li>
<li>恢复(Recovery) - 从故障的虚拟化上恢复虚拟机</li>
</ul>


<h3>监控</h3>

<p>针对这种解决方案的监控部件有两个主要需求：</p>

<ul>
<li>检测主机是否已经失败</li>
<li>针对错误出发自动的相应(隔离和恢复)</li>
</ul>


<p>通常建议这个解决方案应该是OpenStack的一部分。很多人建议这个功能应该在Nova中实现。将这个部分放在Nova中主要是因为，Nova已经能过获知运行环境下的基础架构的健康状况。servicegroup API能够提供基础的组的成员信息。特别是他持续跟踪计算节点的活跃状态。然而，这只能提供你nova-compute服务本身的状态。对于客户的虚拟机而言(即使nova-compute不再运行)，他们也能运行良好。将基础架构层的状态信息放入Nova之中，有违Nova的层级。无论如何，这将对Nova有一个重大的(管理)范围的扩大，所以我也不指望Nova团队会同意这么做。</p>

<p>还有一种建议是将功能做进Heat里。最重要的功能问题是，我们不该让云平台用户去使用Heat重启他们出现故障的虚拟机。另外一种建议是用其他的(可能是全新的)OpenStack模块来做这件事情。像我不喜欢这些放在Nova的理由一样，我也不认为他们该放在新的模块一样( I don’t like that for many of the same reasons I don’t think it should be in Nova.)。我觉得这件事情应该是基础架构层需要支持的，而不是OpenStack自己。</p>

<p>放弃将这部分功能放入OpenStack的想法，我认为应当从基础架构的角度入手支持OpenStack部署。许多OpenStack部署中已经使用了Pacemaker提供部署中的高可靠。从历史的角度来看，由于集群中横向限制，Pacemaker并不会用在计算节点上，因为他们太多了。这个限制其实在Corosync而不是Pacemaker本身。最近，Pacemaker提供了一个新功能叫做pacemaker_remote，允许将一台主机作为Pacemaker集群的一部分，而不需要作为Corosync集群的一部分。这样看起来可以被作为OpenStack计算节点的一个合适的解决方案。</p>

<p>许多OpenStack部署中也使用监控工具像Nagios来监控他们的计算节点。这也很合理。</p>

<h3>隔离(Fencing)</h3>

<p>概括一下，隔离就是将故障的计算节点完全隔离(isolates)。举个例子，这个可能由IPMI保证失败的节点已经关机了。隔离非常重要，有以下几点原有。很多原因都会造成节点故障，我们在将同样的虚拟机恢复之前，完全确认它确实不在(completely gone)啦。我们不想让我们的虚拟机跑两份。用户也肯定不想看到。更糟糕的是，处理自动疏散(evacuate)时，OpenStack的部署可能是基于共享存储的，跑两个一样的虚拟机可能会引起数据损坏，因为两个虚拟机尝试使用同一块磁盘。另外一个问题就是，在网络上产生两个相同的IP。</p>

<p>用Pacemaker最大的好处就是他内建隔离，因为这正是HA解决方案中的一个核心组件。如果你使用Nagios，隔离的集成需要你自己实现。</p>

<h3>恢复</h3>

<p>一旦故障被检测到并且计算节点被隔离，需要触发疏散(evacuate)。概括一下，所谓疏散就是将一台故障节点的虚拟机实例在另外一个节点上启动起来。Nova提供了API去疏散一个实例。这个功能正常工作的前提是需要虚拟机磁盘文件在共享存储上。或者是从卷启动的。有意思的是，即使疏散中没有以上两个条件，API仍然可以运行。结果就是用基础镜像重新启动一个新的实例而没有任何数据。这样的唯一好处就是你得到一个和你之前虚拟机相同UUID的实例。</p>

<p>一个通用的疏散用例是“从一个指定的Host上疏散所有虚拟机”。因为这个太通用了，所以这个功能在novalcient库中实现了(译者：Juno更新日志提到了这一点)。所以监控工具可以通过novaclient触发这个功能。</p>

<p>如果这个功能在你的OpenStack的部署上用于所有虚拟机，那么我们的解决方案还是挺好的。许多人有了额外的需求：用户应该可以自行对每一个虚拟机做(HA)的设定。这个的确很合理，但是却带来了一个额外的问题。我们在OpenStack中该如何让用户指定哪台虚拟机需要被自动恢复？</p>

<p>通常的方式就是用镜像的属性或者规格的extra-specs。这样当然可以工作，但是对于我好像并不太灵活。我并不认为用户应该创建一个新的镜像叫做“让这个虚拟机一直运行”。Flavor的extra-specs还行，如果你觉得为你所有虚拟机使用特殊的flavor或者是flavor类。在任何一种情况下，都需要修改novaclient的&#8221;疏散一个Host&#8221;来支持他。</p>

<p>另外一种潜在的解决方案是使用一个用户自定义的特殊标记。已经有一个正在review的功能提供一个API来给虚拟机打标签(tagging)。对于我们的讨论，我们假设标签是“自动回复”。我们也需要更新novaclient来支持“将所有带指定标记的虚拟机从Host疏散”。监控工具也会触发这个功能，让novalcient将带有“自动恢复“标记的所有虚拟机从Host疏散。</p>

<h3>结论和下一步计划</h3>

<p>虚拟机的HA显然是许多部署需要提供的功能。我相信可以通过在部署中将现有软件进行集成方式实现，特别是Pacemaker。下一步就是提供具体的信息，如何建立已经如何测试。</p>

<p>我希望有人可能说”但是我已经使用系统Foo(Nagios或其他的什么)来监控我的计算节点“。你也可以按照这条路尝试。我不确认如何将Nagios之类的监控软件和隔离部分进行整合。如果在这个解决方案中跳过隔离，你要在失败时保持和平(keep the pieces when it breaks，译者：就是上面提到的fencing出现的问题)。除此之外，你的监控系统能够像Pacemaker一样出发novalcient的疏散功能。</p>

<p>未来非常好的开发方向可能是将这个功能集成到OpenStack管理界面。我希望通过部署的控制面板告诉我哪些失败了，出发了哪些响应动作。这个需要pcsd提供REST API(WIP)来导出这些信息。</p>

<p>最后，值得思考一下TripleO在这个问题的内容。如果你使用OpenStack部署OpenStack，解决方案是不是不同呢？在那个世界里，你所有的裸金属节点都是通过OpenStack Ironic管理的资源。Ceilometer可以被用于监控这些资源。如果是那样，OpenStack本身就有足够的信息来支持基础设施完成这个功能。再次强调，为了避免对OpenStack的改造，我们在这种条件小也应该使用更通用的Pacemaker解决方案。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用MongoDB作为Salt Pillar后端存储数据]]></title>
    <link href="http://xiaoquqi.github.io/blog/2015/02/23/use-mongodb-to-store-salt-pillar/"/>
    <updated>2015-02-23T15:19:21+08:00</updated>
    <id>http://xiaoquqi.github.io/blog/2015/02/23/use-mongodb-to-store-salt-pillar</id>
    <content type="html"><![CDATA[<p>今天在查找salt中pillar嵌套pillar的方法时，无意之间发现了pillar除了可以直接使用文件(sls)外，也同时支持多种后端的数据存储方式。例如：MySQL, MongoDB, Ldap, json, cobbler甚至是puppet。这无疑为开发中的接口提供了极大的便利。</p>

<!-- more -->


<p>详细的支持列表可见：<a href="http://docs.saltstack.com/en/latest/ref/pillar/all/index.html#all-salt-pillars">http://docs.saltstack.com/en/latest/ref/pillar/all/index.html#all-salt-pillars</a></p>

<p>严格意义上来说，这篇博文并非完全原创，英文原文请参考：<a href="http://www.tmartin.io/articles/2014/salt-pillar-mongodb/">http://www.tmartin.io/articles/2014/salt-pillar-mongodb/</a></p>

<p>下面就来说说详细的配置方式，假定你已经有了一个部署好的salt环境，并且正确配置了salt master和salt minion，并且完成认证，主机名为salt-master.salt.com，这里我们使用Ubuntu 12.04 64bit作为演示环境。</p>

<h2>安装MongoDB和Python MongoDB</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apt-get install mongodb python-pymongo python-pymongo-ext</span></code></pre></td></tr></table></div></figure>


<p>确保你能连接到MongoDB</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mongo</span></code></pre></td></tr></table></div></figure>


<pre><code>MongoDB shell version: 2.2.3
connecting to: test
&gt;
</code></pre>

<h2>创建MongoDB数据库和存放Pillar的Collection</h2>

<ul>
<li>创建数据库pillar</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>use pillar</span></code></pre></td></tr></table></div></figure>


<ul>
<li>在数据库中插入pillar数据</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>db.pillar.insert({
</span><span class='line'>    _id: 'salt-master.salt.com',
</span><span class='line'>    mongo_pillar: {
</span><span class='line'>        key1: "value1",
</span><span class='line'>        key2: "value2",
</span><span class='line'>    }})</span></code></pre></td></tr></table></div></figure>


<p>注意：这里的_id必须要和你的minion节点的主机名一致，并且无法使用通配符，也就是一个节点都有自己一套独立的pillar，这一点和文件中定义pillar有很大的不同。mongo_pillar部分中是定义的是pillar中的内容，也就是我们可以直接引用的部分。</p>

<h3>配置Salt Master</h3>

<ul>
<li><p>下一步就是告诉Salt Master，我们在MongoDB中存放了pillar数据，需要劳您大驾，移步MongoDB读取数据。修改：</p>

<p>  /etc/salt/master</p></li>
</ul>


<p>添加</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mongo.db: "pillar"
</span><span class='line'>mongo.host: "localhost"
</span><span class='line'>ext_pillar:
</span><span class='line'>    - mongo: {}</span></code></pre></td></tr></table></div></figure>


<p>注意：如果需要使用不同于标准安装接口，请使用mongo.port，如果需要配置用户名和密码，请使用mongo.user和mongo.password。其他参数定义，请详见：<a href="http://docs.saltstack.com/en/latest/ref/pillar/all/salt.pillar.mongo.html#module-documentation">http://docs.saltstack.com/en/latest/ref/pillar/all/salt.pillar.mongo.html#module-documentation</a></p>

<ul>
<li>测试</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>salt salt-master.salt.com pillar.item mongo_pillar</span></code></pre></td></tr></table></div></figure>


<p>返回</p>

<pre><code>salt-master.salt.com:
    ----------
    mongo_pillar:
        ----------
        key1:
            value1
        key2:
            value2
</code></pre>

<ul>
<li>如果想在sls中直接使用</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{{ salt['pillar.get']('mongo_pillar:key1') }}</span></code></pre></td></tr></table></div></figure>


<h2>总结</h2>

<p>pillar应该是salt中一个比较灵活的配置选项，个人理解pillar的作用就像puppet中init定义的初始化的参数的默认值，每次部署时，只需要更改pillar的文件就可以啦。但是随着代码的增长(主要用于部署OpenStack)，发现pillar的管理越来越难，pillar本身对如何组织结构并没有严格的限制，而且嵌套(extend)功能暂时还不能很完美的支持(<a href="https://github.com/saltstack/salt/issues/3991">https://github.com/saltstack/salt/issues/3991</a>)，这也给pillar的管理提高了复杂度。</p>
]]></content>
  </entry>
  
</feed>
